%My questions.
%2014/5/16 
%(1)There is no explanation about PCD. I could not find the word "PCD".
 
%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
% \begin{filecontents*}{example.eps}
% gsave
% newpath
%   20 20 moveto
%   20 220 lineto
%   220 220 lineto
%   220 20 lineto
% closepath
% 2 setlinewidth
% gsave
%   .4 setgray fill
% grestore
% stroke
% grestore
% \end{filecontents*}
%
%\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
%\documentclass[smallextended]{svjour3}       % onecolumn (second format)
\documentclass[twocolumn]{svjour3}
\usepackage{multirow}
\usepackage{epsfig}
\usepackage{here}
\usepackage{slashbox}        % twocolumn
\def\mbf#1{\mbox{\boldmath $#1$}} 
\def\bm#1{\mbox{\boldmath $#1$}} 
\def\rup#1{{^#1}\hspace{-0.5mm}} 
%\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
\usepackage{ascmac}
\usepackage{mathtools}
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journalname{myjournal}
%
\begin{document}

\title{\Large\bf Molded Article Picking Robot Using Image Processing Technique and Pixel-Based Visual Feedback Control
%\thanks{Grants or other notes
%about the article that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}
}
%\titlerunning{Short form of title}        % if too long for running head
 \author{
   Kohei Miki \and Fusaomi Nagata \and Takeshi Ikeda \and \\
   Keigo Watanabe \and Maki K. Habib%etc.
  }
 \authorrunning{Short form of author list} % if too long for running head
  \institute{Kohei Miki \and Fusaomi Nagata \and Takeshi Ikeda
   \at Graduate School of Engineering, Sanyo-Onoda City University, 1-1-1 Daigaku-Dori, Sanyo-Onoda 756-0884, Japan\\
   \email nagata@rs.socu.ac.jp \\
   Keigo Watanabe, Graduate School of Natural Science and Technology, Okayama University, Japan \\
   Maki K. Habib, Mechanical Engineering Department, School of Sciences and Engineering, American University in Cairo, Egypt
}
 %\institute{}

\date{Received: date / Accepted: date}

\maketitle

\begin{abstract}
This paper aims to develop a robotic system that is able to find and remove unwanted molded articles, which fell in a narrow metallic mold space. Currently, this task is being supported by skilled workers. The proposed robotic system has the ability to estimate the orientation of articles by using transfer learning-based convolutional neural networks (CNNs). The orientation information is essential and indispensable to realize stable robot picking operations. In addition, pixel-based visual feedback (PBVF) controller is introduced by referring to the center of gravity (COG) position of articles computed by image processing techniques. Hence, it is possible to eliminate the complex calibration between the camera and the robot coordinate systems. The implementation and effectiveness of the pick and place robot are demonstrated, where the conventional calibration of such task is not required.
\keywords{Convolutional Neural Network (CNN) \and Transfer Learning \and Pixel-Based Visual Feedback \and Pick and Place}
% \PACS{PACS code1 \and PACS code2 \and more}
%\subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
A wide range of robots is applied, organized, and controlled for the purpose to achieve the desire automation of production lines in a factory. During robotic manipulation, it is required to conduct pick and place tasks, and hence it is essential for the robot to estimate the position and orientation of target objects. To cope with this need, many research development activities have been devoted to develop the necessary technologies to abstract visual features from acquired images and control robots autonomously in real-world environments~\cite{Visual-Survo}. For example, Tarydi et al. proposed an image processing algorithm to estimate the 3D position and orientation of an object from images taken by calibrated stereo cameras, which allowed a 6 DOF robot arm with a gripper to place it at an arbitrary position in the robot workspace \cite{3Dpose-estimation}.

Until around 2012, the research field of object recognition had been dominated by the use of human-designed features such as Fisher and SIFT features. However, the SuperVision team using convolutional neural networks (CNNs) successfully become the winner at the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012 \cite{ILSVRC,AlexNet}. After that, the use of CNN in robot control become attractive because visual features can be extracted through training. For example, Haochen et al. designed a CNN to estimate the category, position, and orientation of PBC object, and showed that it could be processed at high speed and accuracy \cite{PBC}.

However, it is also recognized at the present stage that when a robot working on a production line fails to pick and place an article, the article is quickly removed by a worker to avoid damaging the production line. Such type of task is exhausting the worker as it is usually required to assign one person to follow up the operation of several robots during actual production.

In this paper, for the purpose to solve this problem, a basic pick and place robotic system is proposed. The system has an ability to estimate the orientation of articles with a resolution of 5 degrees by using transfer learning-based CNNs \cite{Robomech2020,AROB2021}. The orientation information of molded articles is essential to realize stable robotic picking operations.

In addition, a pixel-based visual feedback (PBVF) controller is designed by referring to the articles' center of gravity (COG) obtained by image processing techniques as the control quantity \cite{AROB2021}. In the proposed PBVF controller, the end-effector's position is regulated in order to get the COG of an article overlapped with the center of the image frame. Consequently, the normally used complicated calibration task between the camera and the robot coordinate systems can be eliminated. The effectiveness and promise of the proposed robotic system are demonstrated through pick and place experiments.
.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{CNNs for Orientation Detection}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This section describes the design of transfer learning-based CNNs to estimate objects' orientations. CNNs such as AlexNet~\cite{AlexNet}, VGG16, VGG19 \cite{VGGNet} or GoogLeNet\cite{GoogLeNet} are used as powerful base CNN models to support wide range of development. Besides, conditions of additional training ( i.e., fine tuning of weights) in the transfer learning are also presented. Then, the eight types of transfer learning-based CNNs are compared quantitatively. In order to determine the best among the four CNN models under consideration, the generalization performance of each CNN is evaluated using approximately 2,000 test images that are not used during the additional training processes.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 2.1.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Design of eight types of CNNs using transfer learning}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
As for the transfer learning, the conceptual idea is focusing on making use of the knowledge obtained in one domain in order to improve the learning of a prediction function in another domain~\cite{Transfer-2010}. It is known that there are two advantages of the usage of transfer learning when designing a new CNN. The first advantage is the shortening the learning time compared to normal learning process from scratch. The other advantage is the improvement of recognition rate in spite of less training data. As for the actual design and training of a transfer learning-based CNN for a task, the output layer of the original CNN model has to be replaced with a new one that has the desired number of outputs needed by the task. Additional training of the CNN facilitated by the error back propagation method is mainly applied to the fully connected layers. 

In this paper, the designs of eight types of transfer learning-based CNNs are considered and the CNNs are trained to estimate the orientation of objects within the desired resolution of 5 degrees. The CNNs can output 36 kinds of labels such as 0$^\circ$, 5$^\circ$,..., 175$^\circ$ as shown in Fig. \ref{fig:Replaced}.

%%%%%%%%%%%%%%%%%
%----------------
% 構造の置き換え
%----------------
%%%%%%%%%%%%%%%%%
\begin{figure}[t]
  \begin{center}
    \includegraphics[width=85mm ,clip]{Figure/FullyConected.eps}
    \caption{Design example of transfer learning-based CNN for orientation detection,
in which the desired resolution of orientation is set to 5 degree.}
    \label{fig:Replaced}
  \end{center}
\end{figure}

In the training processes, the weight parameters of four CNN models are fine-tuned in two different training conditions. In the first training condition, the weights are trained only in the fully connected layers. On the other hand, in the second training condition, all weights in the convolution layers and the fully connected ones are trained. The superiority or inferiority of each training condition is evaluated by checking the effects of the fine-tuning. The criterion of the effect is conducted based on recognition accuracy.

The powerful four CNN models that are effectively used as the bases of transfer learning are AlexNet, VGG16, VGG19 and GoogLeNet. AlexNet, VGG16 and VGG19 have orthodox network structures called the series type. On the other hand, GoogLeNet has a different structure called DAG (Directed Acyclic Graph) type network. These are originally trained using the ImageNet dataset~\cite{ILSVRC}. In the design based on transfer learning, the original output layer structure for 1,000 categorizations is replaced with a redesigned one that has 36 outputs, i.e., labels from 0$^\circ$ to 175$^\circ$ , as shown in Fig.~\ref{fig:Replaced}.

As for the parameters for the additional training, the max epoch, mini batch size, desired accuracy, loss and L2 regularization coefficient are set to 500, 30, 0.999, 0.0001 and 0.004, respectively. The initial learning rate is set to 0.004 and then it is gradually decreased by multiplying 0.1 for every 2 epochs. The dataset used for additional training, i.e., fine tuning, consists of 15,264 (424$\times$36) images designed by authors as shown in Fig. \ref{fig:Training}, in which 424 images are equally alloted to each of 36 classes. All input images are automatically resized to fit the resolutions of the input layer of the transferred CNN models. The classifiers' performances are evaluated using different features included in 2,232 test images. The training and test processes of CNNs are conducted using the authors' developed CNN and support vector machine (SVM) design application implemented on MATLAB \cite{CNN&SVM-tool}.

%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
%----------------
% Figure Training
%----------------
%%%%%%%%%%%%%%%%%
\begin{figure}[t]
  \begin{center}
    \includegraphics[width=75mm ,clip]{Figure/training_img.eps}
    \caption{Examples of training images for 0$^\circ$, 50$^\circ$, 105$^\circ$ and 160$^\circ$. Note that a little bit augmented images are included in the training data.}
    \label{fig:Training}
  \end{center}
\end{figure}
%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
%----------------
% Figure Augmentation
%----------------
%%%%%%%%%%%%%%%%%
\begin{figure}[t]
  \begin{center}
    \includegraphics[width=82mm ,clip]{Figure/Augmentation.eps}
    \caption{Examples of several augmentation methods by changing brightness, resolution and orientation, which are applied to the original training images to increase the number.}
    \label{fig:Augmentation}
  \end{center}
\end{figure}
%%%%%%%%%%%%%%%%%

The training images are firstly created by rotating the twelve kinds of objects from 0$^\circ$ to 175$^\circ$ with 5$^\circ$ increments. Figure \ref{fig:Training} shows examples of training images in case of 0$^\circ$, 50$^\circ$, 105$^\circ$ and 160$^\circ$. Note that a little bit augmented images are included in the training data. As for the augmentation of training images, the following image processing methods are applied.\\
(1) Rotating the original images every 0.1$^\circ$ within the range of $\pm$1$^\circ$. \\
(2) Changing the brightness of original images, i.e., RGB pixel values, every 1 within the range of $\pm 10$.\\
(3) Changing the size of images every $1\%$ within the range of $\pm 10 \%$.

Figure \ref{fig:Augmentation} shows examples of the augmentation obtained by the above processes. The eight kinds of CNNs transfered from AlexNet, VGG16, VGG19 and GoogLeNet were additionally and finely trained using the same 15,264 images. Numerical evaluations are compared in the following subsection.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 2.2.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Evaluation of the transfer learning-based CNNs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
After the fine training process, generalization abilities of the transfer learning-based CNNs were checked using 2,232 test images as shown in Fig. \ref{fig:Test}. These test images were not included in the training data set. The classification results are shown in Table \ref{table:Classification results}, in which the accuracy, precision and recall are used as criteria~\cite{Classification-Assesment}. It is observed from the comparison result of the CNN models that the GoogLeNet-based CNN model obtained through all layers fine tuning process could perform the best. Besides, It is also observed from the comparison between VGG16-based and VGG19-based CNNs, VGG16-based CNN was superior with respect to the accuracy, precision and recall. Actually, the lengths of the transferred network structures of AlexNet-based, VGG16-based and VGG19-based CNNs, which are series type CNNs, are 25, 41 and 47, respectively. These results suggest that the number of training images, i.e. 15,264, may have been not sufficient to additionally train the VGG19-based CNN with the deepest structure within the three CNNs. On the other hand, although GoogLeNet-based CNN has a deeper network structure with 144 layers, the better numerical result is obtained as shown in Table \ref{table:Classification results}. It seems that the DAG type network has a superiority compared with the series type ones to be efficiently trained with less number of training images.

Consequently, the authors had selected the GoogLeNet-based CNN as the best one for estimating the orientation of target objects. In the subsequent section, picking experiments using a small articulated robot are demonstrated.
%%%%%%%%%%%%%%%%%
%----------------
% Figure Test
%----------------
%%%%%%%%%%%%%%%%%
\begin{figure}[t]
  \begin{center}
    \includegraphics[width=83mm ,clip]{Figure/Test_img.eps}
    \caption{Examples of test images for 10$^\circ$, 70$^\circ$, 130$^\circ$ and 175$^\circ$.}
    \label{fig:Test}
  \end{center}
\end{figure}
%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
%----------------
% 評価結果
%----------------
%%%%%%%%%%%%%%%%%
\begin{table*}[t]
  \caption{Classification results of eight kinds of transfer learning-based CNNs \cite{Classification-Assesment}.}
  \label{table:Classification results}
  \centering
  \begin{tabular}{|c|c|r|r|r|}
    \hline
    Base models & Fine-tuned layers & Accuracy & Precision & Recall \\ \hline \hline
    AlexNet     & ALL               & 0.392    & 0.402     & 0.392  \\ \hline
    AlexNet     & FC-Layers         & 0.187    & 0.185     & 0.235  \\ \hline
    VGG16       & ALL               & 0.683    & 0.713     & 0.683  \\ \hline
    VGG16       & FC-Layers         & 0.196    & 0.220     & 0.196  \\ \hline
    VGG19       & ALL               & 0.671    & 0.694     & 0.671  \\ \hline
    VGG19       & FC-Layers         & 0.202    & 0.217     & 0.202  \\ \hline
    GoogLeNet   & ALL               & 0.724    & 0.734     & 0.724  \\ \hline
    GoogLeNet   & FC-Layers         & 0.078    & 0.080     & 0.078  \\ \hline
  \end{tabular}
\end{table*}
%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 3.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Application of Transfer Learning Based CNN to a Picking Robot}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section, the obtained transfer learning-based CNN using GoogLeNet is applied to a picking robot as shown in Fig. \ref{fig:Picking-Robot}. A Web camera is used when the PBVF controller is not applied, on the other hand, an endoscope camera is used when the PBVF controller is applied. The CNN is used to estimate objects' orientation on a working table.

%%%%%%%%%%%%%%%%%
% Figure Picking-Robot
%%%%%%%%%%%%%%%%%
\begin{figure}
  \begin{center}
    \includegraphics[width=60mm ,clip]{Figure/Dobot.eps}
    \caption{Picking robot system that can move the end-effector to the position just above the COG position of a workpice by PBVF control and then recognize the orientation by CNN.}
    \label{fig:Picking-Robot}
  \end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 3.1.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Without the pixel-based visual feedback (PBVF) controller}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The robotic pick and place without PBVF controller can be applied through a process consisting of five steps. Firstly, a snapshot with the resolution of $1200 \times 960$ is captured by the Web-camera. Secondly, the image is binarized to black and white. Thirdly, the COG position $[I_x \ I_y]^T$ of an object is computed using Eqs.~(\ref{COG_of_Image_x}) and (\ref{COG_of_Image_y}), assuming that the largest connected component in the image is the target object.
%%%%%%%%%%%%%%%%%
%----------------
% COG險育ｮ・
%----------------
%%%%%%%%%%%%%%%%%
\begin{eqnarray}
  \label{COG_of_Image_x}
  I_x & = & \frac{\sum_{x=1}^{1200} \sum_{y=1}^{960} xB(x, y)}{S} \\
  \label{COG_of_Image_y}
  I_y & = & \frac{\sum_{x=1}^{1200} \sum_{y=1}^{960} yB(x, y)}{S}
\end{eqnarray}
%%%%%%%%%%%%%%%%%
where $x$ and $y$ are the position $(x,y)$ variables representing columns and rows in the images, i.e., camera coordinate system $(1 \leq x \leq 1200, 1 \leq y \leq 960)$. Also, $B(x,y)$ is the pixel value in the binary image, i.e., 1 or 0, at position~$(x,y)$. Figure \ref{fig:binarization} shows an example of the first, second and third steps, i.e., binarization of a captured image and the calculation of COG position. Consequently, the desired position $(x_d, y_d)$ in robot coordinate system that aims to move the robot's gripper just above a COG position can be obtained by
%%%%%%%%%%%%%%%%%
% Figure binarization
%%%%%%%%%%%%%%%%%
\begin{figure}[t]
  \begin{center}
    \includegraphics[width=80mm ,clip]{Figure/binarization.eps}
    \caption{Example of capturing an image, binarization, and calculation of COG position.}
    \label{fig:binarization}
  \end{center}
\end{figure}

%%%%%%%%%%%%%%%%%
%----------------
% Figure CNN-Input
%----------------
%%%%%%%%%%%%%%%%%
\begin{figure}[t]
  \begin{center}
    \includegraphics[width=85mm ,clip]{Figure/Estimate_Ori.eps}
    \caption{Procedure to extract the orientation of a workpiece from a captured image.}
    \label{fig:CNN-Input}
  \end{center}
\end{figure}
%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
%----------------
% Pick & Place Flow
%----------------
%%%%%%%%%%%%%%%%%
\begin{figure}[h]
  \begin{center}
    \includegraphics[width=65mm ,clip]{Figure/FlowChart.eps}
    \caption{Flowchart of the robotic pick and place operation, in which the trained CNN and proposed PBVF controller using an endoscope camera are incorporated.}
    \label{fig:FlowCheart}
  \end{center}
\end{figure}
%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
%----------------
% 繧ｰ繝ｪ繝・ヱ縺ｮ遘ｻ蜍募ｺｧ讓呵ｨ育ｮ・
%----------------
%%%%%%%%%%%%%%%%%
\begin{eqnarray}
  \label{eq:x_d}
  x_d &=& X_1 + I_x\frac{X_2-X_1}{1200}\\
  \label{eq:y_d}
  y_d &=& Y_1 + I_y\frac{Y_2-Y_1}{960}
\end{eqnarray}
%%%%%%%%%%%%%%%%%
where $(X_1 , Y_1)$ and $(X_2 , Y_2)$ are the positions of left upper and right bottom of the snapshot described in robot coordinate system as shown in Fig.~\ref{fig:CNN-Input}, i.e., they correspond to image coordinates of $(1,1)$ and $(1200,960)$, respectively. Fourthly, the part of the connected component is further cropped centering the COG from the original snapshot as shown in Fig.~\ref{fig:CNN-Input}. The cropped image is resized according to the resolution of the CNN's input layer, and then it is given to the input layer. Finally, the CNN estimates the desired yaw angle $\theta$ of the object with a major axis shape, so that the robot can try to grasp the object  by using the estimated $\theta$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 3.2.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Pixel-based visual feedback (PBVF) controller}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Figure \ref{fig:FlowCheart} shows the flowchart of the robotic pick and place operation, in which the proposed CNN and PBVF controller are implemented. It is expected that complicated camera configuration is no more required due to the contribution of the developed PBVF controller. This has the advantage that is different from the system using a Web camera introduced in the previous subsection. A lightweight endoscope camera is attached close to the gripper as shown in Fig. \ref{fig:Picking-Robot} so that real-time snapshot images viewed from the gripper can be obtained. Manipulated variable $\bm{v}(k)=[v_x(k) \ v_y(k)]^T$ for visual feedback is generated by a simple PI-action given by
%%%%%%%%%%%%%%%%%
%----------------
% VF縺ｮ騾溷ｺｦ險育ｮ・
%----------------
%%%%%%%%%%%%%%%%%
\begin{equation}
  \label{eq:v}
  \bm{v}(k) = K_p \bm{e}(k) + K_i \sum_{n=1}^k \bm{e}(n)
\end{equation}
%%%%%%%%%%%%%%%%%
where $k$ is the discrete time. $K_p$ and $K_i$ are the gains of proportional and integral actions, respectively.
$\bm{e}(k) = [e_x(k) \ e_y(k)]^T$ is the error vector in image coordinate system measured by
%%%%%%%%%%%%%%%%%
%----------------
% error繝吶け繝医Ν縺ｮ險育ｮ・
%----------------
%%%%%%%%%%%%%%%%%
\begin{equation}
  \label{eq:error}
  \bm{e}(k) = \bm{X}_d - \bm{I}(k)
\end{equation}
%%%%%%%%%%%%%%%%%
where $\bm{X}_d = (600 , 480)$ and $\bm{I}(k) = \left[ I_x(k) , I_y(k) \right]$ are the desired position (center of image) and the measured object's COG position in image coordinate system, respectively. The PBVF controller controls the gripper position so that the object's COG position can overlap with the center of captured image as shown in the left photo in Fig. \ref{fig:CNN-Input}. Also, the PBVF controller allows the robot to execute the pick and place task without using Eqs. (\ref{eq:x_d}) and~(\ref{eq:y_d}), so that the measuring of $(X_1 , Y_1)$ and $(X_2 , Y_2)$ is not needed.
%%%%%%%%%%%%%%%%%
%----------------
% Pick and Place螳滄ｨ・
%----------------
%%%%%%%%%%%%%%%%%
\begin{figure}[t]
  \begin{center}
    \includegraphics[width=80mm ,clip]{Figure/Pick_and_Place.eps}
    \caption{Pick and place experiments using the proposed CNN and PBVF controller.}
    \label{fig:Pick_and_Place}
  \end{center}
\end{figure}
%%%%%%%%%%%%%%%%%

Finally, several pick and place experiments were conducted in order to confirm the effectiveness of the proposed pick and place robot. Figure \ref{fig:Pick_and_Place} shows the experiment scenes, in which four kinds of different shapes of workpieces are successfully picked and placed to the desired position while skillfully gripping the just center of the long axis. Another important advantage of the PBVF controller is that the recalibration between camera and robot coordinate systems is not required for the recovery even in the trouble case such that they become misaligned without noticing.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 4.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this paper, transfer learning-based CNN models were developed and implemented to estimate the orientation of objects with a resolution of 5 degrees on a working table for stable picking operation. Actually, the transfer learning-based CNNs for orientation estimation were produced by finely tuning four existing CNN architectures named AlexNet, VGG16, VGG19, and GoogLeNet. It was confirmed from the classification experiments using test images, the GoogLeNet-based CNN model obtained through all layers' fine-tuning could have the highest recognition accuracy. Also, in order to reduce the calibration load between the camera and robot coordinate systems, a simple pixel-based visual feedback controller with PI actions was successfully implemented. The robot could have a higher ability to pick and place operation due to the proposed CNN model and the PBVF controller. 
%%%%%%%%%%%%%%%%%, 
% 蜿り・枚迪ｮ
%%%%%%%%%%%%%%%%%
\begin{thebibliography}{99}
\bibitem{Visual-Survo}
Kragic D, Christensen HI, ``Survey on visual servoing for manipulation,'' {\it Computational Vision and Active Perception Laboratory Technical Report}, 59 pages, 2002.

\bibitem{3Dpose-estimation}
Taryudi, Wang MS, ``3D object pose estimation using stereo vision for object manipulation system,'' {\it Procs. of 2017 International Conference on Applied System Innovation (ICASI)}, pp. ~1532--1535, 2017.

\bibitem{ILSVRC}
Russakovsky O., Deng J, Su H, Krause J, Satheesh S, Ma S, Huang Z, Karpathy A, Khosla A, Bernstein M, Berg AC, Fei-Fei L, ``ImageNet Large Scale Visual Recognition Challenge,'' {\it International Journal of Computer Vision (IJCV)}, Vol. 115, pp.~211--252, 2015.

\bibitem{AlexNet}
Krizhevsky A, Sutskever I, Hinton GE, ``Imagenet classification with deep convolutionalneural networks.'' {\it Procs. of Advances in neural information processing systems}, pp. 1097--1105, 2012.

\bibitem{PBC}
Haochen L, Bin Z, Xiaoyong S, Yongting Z, ``CNN-based model for pose detection of industrial PCB,'' {\it Procs. of International Conference on Intelligent Computation Technology and Automation (ICICTA)}, vol. 1, pp. 390--393, 2017.

\bibitem{Robomech2020}
Miki K, Nagata F,  Watanabe K, ``Defective article picking robot in narrow metal mold space using image processing technique,'' {\it Procs. of the 2020 JSME Conference on Robotics and Mechatronics (ROBOMECH2020)}, 2P2-B03, 4 pages, 2020 (in Japanese).

\bibitem{AROB2021}
Miki K, Nagata F,  Watanabe K, Habib MK, ``Picking robot of defective molded articles using image processing technique and visual feedback control, {\it Procs. of 26th International Symposium on ARTIFICIAL LIFE AND ROBOTICS (AROB 26th 2021)}, pp. 498--502, 2021.
 
\bibitem{VGGNet}
Simonyan K, Zisserman A, ``Very deep convolutional networks for large-scale image recognition,'' {\it Procs. of International Conference on Learning Representations 2015 (ICLR2015)}, 14 pages, 2015.

\bibitem{GoogLeNet}
Szegedy C, Liu W, Jia Y, Sermanet P, Reed S, Anguelov D, Erhan D, Vanhoucke V, Rabinovich A, ``Going deeper with convolutions,'' {\it Procs. of Conference on Computer Vision and Pattern Recognition (CVPR)}, pp. 1--9, 2015.

  %\bibitem{Fine-Turing}
  %Ross Girshick et al. , ``Rich feature hierarchies for accurate object detection and semantic segmentation,'' {\it Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.~580--587, 2014.

  \bibitem{Transfer-2010}
  Pan SJ, Yang Q, ``A survey on transfer learning,'' {\it Procs. of IEEE Transactions on Knowledge and Data Engineering}, vol. 22, no. 10, pp. 1345--1359, 2010.

  \bibitem{CNN&SVM-tool}
  Nagata F, Miki K, Otuka A, Yoshida K, Watanabe K, Habib MK, ``Pick and place robot using visual feedback control and transfer learning-based CNN,'' {\it Procs. of IEEE International Conference on Mechatronics and Automation (ICMA)}, pp. 850--855, 2020.

  \bibitem{Classification-Assesment}
  Tharwat A, ``Classification assessment methods,'' {\it Applied Computing and Informatics}, vol.~16, no. 1/2, 25 pages, 2020.

  %\bibitem{Fine-Turn}
  %Pulkit Agrawal, Ross Girshick, and Jitendra Malik, ``Analyzing the Performance of Multilayer NeuralNetworks for Object Recognition,'' {\it European Conference on Computer Vision(ECCV)}, pp. ~329--344, 2014.

\end{thebibliography}
\end{document}
% end of file template.tex


# Molded Article Picking Robot Using Image Processing Technique and Pixel-Based Visual Feedback Control

# Abstract

本稿では，狭い金型空間に落下した不要な成形品を発見し，除去することができるロボットシステムの開発を目指している。現在、この作業は熟練した作業者によってサポートされている。提案するロボットシステムは、転移学習に基づく畳み込みニューラルネットワーク(CNN)を用いて成形品の向きを推定する機能を有している。向きの情報は、ロボットの安定したピッキング作業を実現するために必要不可欠であり、欠かせないものです。さらに、画像処理技術を用いて算出した物品の重心位置を参照することで、ピクセルベースのビジュアルフィードバック（PVF）制御を導入しています。これにより、カメラとロボットの座標系の間の複雑なキャリブレーションを不要にすることができます。本研究では、従来のキャリブレーションが不要なピック＆プレースロボットを実現し、その有効性を示した。

# 1. はじめに

工場の生産ラインの自動化を実現するために、さまざまなロボットが応用され、組織化され、制御されています。ロボットを操作する際には、ピックアンドプレースの作業が必要となりますが、そのためには対象物の位置や向きを推定することが不可欠です。このようなニーズに対応するために、取得した画像から視覚的特徴を抽出し、実環境下でロボットを自律的に制御するための技術を開発するために、多くの研究開発が行われています。例えば、Tarydiらは、キャリブレーションされたステレオカメラで撮影された画像から物体の3次元位置・姿勢を推定する画像処理アルゴリズムを提案し、グリッパーを備えた6自由度のロボットアームが、ロボットの作業空間内の任意の位置に物体を配置することを可能にしました。

2012年頃まで、物体認識の研究分野では、Fisher特徴量やSIFT特徴量といった人間が設計した特徴量を使用することが主流でした。しかし、畳み込みニューラルネットワーク（CNN）を用いたSuperVisionチームが、ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012で優勝することに成功しました。その後、CNNは学習によって視覚的特徴を抽出できるため、ロボット制御への利用が魅力的になった。例えば、Haochenらは、PBCオブジェクトのカテゴリ、位置、向きを推定するためのCNNを設計し、高速かつ高精度に処理できることを示しました[1]。

しかし、現段階では、生産ラインで働くロボットが物品のピッキングや配置に失敗した場合、生産ラインを壊さないように作業者が素早く物品を取り除くことも認識されています。このような作業は、実際の生産時には複数のロボットの動作を1人でフォローする必要があるため、作業者を疲弊させてしまう。

本論文では，この問題を解決するために，基本的なピックアンドプレースロボットシステムを提案する．このシステムでは，転移学習に基づくCNNを用いて，成形品の向きを5度の分解能で推定することができる．ロボットによる安定したピッキング作業を実現するためには、成形品の向きの情報が不可欠です。

また、画像処理技術を用いて得られた成形品の重心（COG）を制御量として参照し、ピクセルベースのビジュアルフィードバック（PVF）コントローラを設計した。提案されたPBVF制御装置では、物品のCOGが画像フレームの中心に重なるようにエンドエフェクタの位置を制御します。その結果、通常行われているカメラとロボットの座標系の間の複雑なキャリブレーションタスクを排除することができる。この提案されたロボットシステムの有効性と将来性を、ピック＆プレース実験によって実証した。

# 2. 姿勢検出のためのCNN

ここでは、物体の向きを推定するための伝達学習ベースのCNNの設計について説明する。AlexNet, VGG16, VGG19, GoogLeNetなどのCNNを、幅広い開発をサポートする強力なベースCNNモデルとして使用する。また、転移学習における追加学習（重みの微調整）の条件も提示しています。そして、8種類の伝達学習ベースのCNNを定量的に比較します。4つのCNNモデルの中から最適なものを選ぶために、各CNNの汎化性能を、追加学習の際に使用されなかった約2,000枚のテスト画像を用いて評価した。

## 2.1. 伝達学習を用いた8種類のCNNの設計

転移学習とは、ある領域で得られた知識を利用して、別の領域での予測関数の学習を向上させることに焦点を当てた概念である. 新しいCNNを設計する際に転移学習を利用することで、2つの利点があることが知られています。1つ目の利点は、通常の学習プロセスに比べて学習時間を短縮できることです。もう1つの利点は、少ない学習データにもかかわらず、認識率が向上することです。タスクのための伝達学習ベースのCNNの実際の設計とトレーニングに関しては、オリジナルのCNNモデルの出力層を、タスクが必要とする数の出力を持つ新しいものと交換しなければならない。エラーバックプロパゲーション法によるCNNの追加学習は、主に完全連結層に適用される。

この論文では、8種類の伝達学習ベースのCNNの設計を検討し、5度の望ましい解像度内で物体の向きを推定するようにCNNを学習した。CNNは図のように、0$^\circ$, 5$^\circ$,..., 175$^\circ$といった36種類のラベルを出力することができる。

学習プロセスでは，4つのCNNモデルの重みパラメータを2つの異なる学習条件で微調整する．最初の学習条件では，完全連結層のみの重みを学習します．一方，第2の学習条件では，畳み込み層と完全連結層のすべての重みを学習する．各学習条件の優劣は，微調整の効果を確認することで評価する。効果の判定は、認識精度に基づいて行われる。

転移学習のベースとして有効なCNNモデルは、AlexNet、VGG16、VGG19、GoogLeNetの4つです。AlexNet、VGG16、VGG19は、系列型と呼ばれるオーソドックスなネットワーク構造です。一方、GoogLeNetはDAG（Directed Acyclic Graph）型と呼ばれる異なる構造のネットワークです。これらは、もともとImageNetデータセットを用いて学習されたものです。Transfer Learning（転移学習）を用いた設計では、図のように、1,000個のカテゴリ分類を行う出力層の構造を、0$^\から175$^ansenまでの36個のラベルを出力する構造に変更しています（図：Replaced）。

追加学習のパラメータとして，最大エポック，ミニバッチサイズ，希望精度，損失，L2正則化係数をそれぞれ500，30，0.999，0.0001，0.004に設定しました．初期の学習率は0.004に設定され，その後2エポックごとに0.1を乗じて徐々に下げていく．追加学習（ファインチューニング）に使用したデータセットは，図のように著者がデザインした15,264枚 (424 $\times$ 36) の画像で，36のクラスに424枚の画像が均等に割り当てられています．すべての入力画像は，転送したCNNモデルの入力層の解像度に合わせて自動的にリサイズされる．分類器の性能は，2,232枚のテスト画像に含まれるさまざまな特徴を用いて評価される．CNNの学習とテストのプロセスは、著者らが開発したMATLAB上のCNNおよびサポートベクターマシン（SVM）設計アプリケーションを用いて行われた。

まず、12種類の物体を0$^\circ$から175$^\circ$まで5$^\circ$刻みで回転させて学習画像を作成します。図は、0$^\circ$, 50$^\circ$, 105$^ansencirc$, 160$^ansencirc$の場合の学習画像の例です。なお，学習データには，少しだけ増量された画像が含まれています。学習画像の補正には，以下のような画像処理を行っています。
(1)原画を0.1$^$circ$ごとに、$pm$1$^$circ$の範囲で回転させる。\\
(2)原画の明るさ（RGBの画素値）を、$\$10$の範囲で1ずつ変化させる。
(3)画像の大きさを$1\%$ごとに変更する。

以上の処理で得られたオーグメンテーションの例を図3に示す。AlexNet、VGG16、VGG19、GoogLeNetから転送された8種類のCNNは、同じ15,264枚の画像を使って追加で細かく学習された。数値的な評価は次のサブセクションで比較します。

## 2.2. Evaluation of the transfer learning-based CNNs

緻密な学習プロセスの後、転送学習ベースのCNNの汎化能力を、図4に示すように2,232枚のテスト画像を使って確認した。これらのテスト画像はトレーニングデータセットには含まれていない。分類結果を表1に示すが，ここでは正確さ，精度，リコールを基準にしている[12]．各CNNモデルの比較結果から、全レイヤーのファインチューニングプロセスを経て得られたGoogLeNetベースのCNNモデルが最も優れた性能を発揮していることがわかります。また、VGG16ベースのCNNとVGG19ベースのCNNとの比較では、VGG16ベースのCNNが精度、正確性、再現性の点で優れていることが確認されています。実際、シリーズ型CNNであるAlexNetベース、VGG16ベース、VGG19ベースのCNNの転送されたネットワーク構造の長さは、それぞれ25、41、47となっている。これらの結果から、3つのCNNの中で最も深い構造を持つVGG19ベースのCNNを追加学習するには、15,264枚という学習画像数では足りなかったのではないかと考えられる。一方、GoogLeNetベースのCNNは144層とより深いネットワーク構造を持っていますが、表1に示すように、より良い数値結果が得られています。このことから、DAG型ネットワークは、シリーズ型ネットワークに比べて、少ない学習画像数で効率よく学習できるという点で優れていると考えられます。

# 3. Application of Transfer Learning Based CNN to a Picking Robot



## 3.1. Without the pixel-based visual feedback (PBVF) controller
PBVFコントローラを使用しないロボットピックアンドプレースは、5つのステップからなるプロセスで適用できます。まず、Webカメラで1200ドルの解像度のスナップショットを撮影します。第二に、この画像を白黒に二値化します。第3に、画像中の最大の連結成分が対象物であると仮定して、式(1)および(2)を用いて、対象物のCOG位置$[I_x \ I_y]^T$を算出します。

```math
\begin{align}
  I_x & = & \frac{\sum_{x=1}^{1200} \sum_{y=1}^{960} xB(x, y)}{S} \\
  I_y & = & \frac{\sum_{x=1}^{1200} \sum_{y=1}^{960} yB(x, y)}{S}
\end{align}
```

ここで，$x$,$y$は画像中の列と行を表す位置変数$(x,y)$，すなわちカメラ座標系$(1 \leq x \leq 1200, 1 \leq y , 960)$とする。また、$B(x,y)$は2値画像における位置$(x,y)$の画素値，つまり1か0である。図に、第1ステップ、第2ステップ、第3ステップ、すなわち撮影画像の2値化とCOG位置の算出の例を示す。その結果、ロボットのグリッパをCOG位置の真上に移動させることを目的としたロボット座標系における所望の位置$(x_d, y_d)$は、次式で求められる。

```math
\begin{align}
  x_d &=& X_1 + I_x\frac{X_2-X_1}{1200}\\
  y_d &=& Y_1 + I_y\frac{Y_2-Y_1}{960}
\end{align}
```

ここで、$(X_1 , Y_1)$, $(X_2 , Y_2)$は、図のようにロボット座標系で記述したスナップショットの左上と右下の位置、すなわち、それぞれ$(1,1)$,$(1200,960)$という画像座標に相当するものである。第四に、図に示すように、元のスナップショットからCOGを中心に連結成分の部分をさらに切り出す。切り出された画像はCNNの入力層の解像度に合わせてリサイズされ、入力層に渡される。最後に、CNNは長軸形状を持つ物体の所望のヨー角$\theta$を推定し、推定した$\theta$を用いてロボットが物体を把持しようとすることができるようにする。


## 3.2. Pixel-based visual feedback (PBVF) controller

図7に、提案するCNNとPBLFコントローラを実装したロボットのピック＆プレース操作のフローチャートを示します。開発したPBLFコントローラの貢献により、複雑なカメラ設定が不要になることが期待されます。これは、前節で紹介したWebカメラを用いたシステムとは異なる利点があります。図のように、軽量の内視鏡カメラをグリッパーの近くに取り付け、グリッパーから見たリアルタイムのスナップショット画像を得ることができます。ビジュアルフィードバックのための操作変数 $\bm{v}(k)=[v_x(k) ˶ 'v_y(k)] ^T$ は、次式で与えられる単純なPIアクションによって生成される。
$$
\bm{v}(k) = K_p \bm{e}(k) + K_i \sum_{n=1}^k \bm{e}(n)
$$
ここで、$k$は離散時間である。$K_p$と$K_i$はそれぞれ比例動作と積分動作のゲインです。
$bm{e}(k) = [e_x(k) \ e_y(k)]^T$ は、画像座標系における誤差ベクトルで、以下の方法で測定します。
$$
\bm{e}(k) = \bm{X}_d - \bm{I}(k)
$$
ここで， $\bm{X}_d = (600 , 480)$ と $\bm{I}(k) = \left[ I_x(k) , I_y(k) \right]$ は，それぞれ画像座標系における希望の位置 (画像の中心) と計測された物体のCOG位置を表す．PBVFコントローラは、図の左の写真に示すように、物体のCOG位置がキャプチャされた画像の中心と重なるようにグリッパーの位置を制御します。また、PVF制御により、式(x_d)と式(y_d)を使わずにピックアンドプレースタスクを実行することができ、$(X_1 , Y_1)$と$(X_2 , Y_2)$の計測が不要になります。

最後に、提案したピックアンドプレースロボットの有効性を確認するために、いくつかのピックアンドプレース実験を行いました。図：Pick_and_Placeに実験の様子を示します。4種類の異なる形状のワークを、長軸のちょうど中心を上手につかみながら、目的の位置にピックアンドプレースすることに成功しています。また、気づかないうちにカメラとロボットの座標系がずれてしまうようなトラブルが発生した場合でも、カメラとロボットの座標系の再校正を必要とせずに復旧できるのもPBLFコントローラの大きなメリットです。

# 4. Conclusions

本論文では，安定したピッキング作業を行うために，作業テーブル上の物体の向きを5度の分解能で推定するために，伝達学習ベースのCNNモデルを開発し，実装した．姿勢推定のための伝達学習ベースのCNNは、AlexNet、VGG16、VGG19、GoogLeNetという4つの既存のCNNアーキテクチャを微調整することで生成されました。テスト画像を用いた分類実験では、すべての層を微調整して得られたGoogLeNetベースのCNNモデルが最も高い認識精度を得られることが確認されました。また、カメラとロボットの座標系間のキャリブレーション負荷を軽減するために、PIアクションを用いたシンプルなピクセルベースのビジュアルフィードバックコントローラの実装に成功しました。提案されたCNNモデルとPAVFコントローラにより、ロボットはより高いピック＆プレース操作能力を持つことができた。
\documentclass[a4paper]{jarticle}
\usepackage{sice-si}
\usepackage{amsmath} 
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{mathrsfs}
\def\mbf#1{\mbox{\boldmath $#1$}}
\def\rup#1{{^#1}\hspace{-0.5mm}}
\def\bm#1{\mbox{\boldmath $#1$}}

\begin{document}
%
% タイトルと著者名
\title{畳み込みニューラルネットワーク（CNN）の転移学習を用いた\\ラップフィルムの不良品検出\\\large{ーInseptionV3の転移学習によるCNNの設計と評価ー}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\name{○有馬　滉宜（山口東京理科大学工学部機械工学科），中島 健斗，三木 康平\\永田 寅臣（山口東京理科大学大学院工学研究科），渡辺 桂吾（岡山大学大学院自然科学研究科）\\} 
\etitle{Detection of Defective Wrap Roll Product Using Transfer Learning of Convolution Neural Networks\\\large{ーDesign and evaluation of CNNs by transfer learning of InseptionV3ー}} % 英文タイトル
\ename{Koki ARIMA, Department of Mechanical Engineering, Sanyo-Onoda City University \\Kento NAKASHIMA, Kohei MIKI, Fusaomi NAGATA (Graduate School of Engineering, Sanyo-Onoda City University)\\Keigo WATANABE (Graduate School of Natural Science and Technology, Okayama University)}	%著者名（英）
%
% アブストラクト
\abst{
The authors are developing an application that make it easy to design convolutional neural networks (CNNs) and support vector machines (SVMs). In this study, the authors try to detect defects that occur in the manufacturing process of wrap roll products by using a transfer learning based CNN model. In advance, template matching is applied to all the images of the wrap roll products to efficiently extract only the film part. InceptionV3 is used as the trained CNN model for transfer learning, in which the fully connected layers are replaced according to the number of classifications, i.e., OK or NG. After the structure of the CNN model is newly designed, additional training is done using a large number of non-defective and defective images increased by image augmentation technique in order to enhance the generalization ability. The CNN model obtained by the transfer learning of InceptionV3 is evaluated through the classification experiment of test images including defective products. 
%and confirm the usefulness of the developed application.
}
% タイトルの出力
\maketitle
%
% 本文
%%%%%%%%%%%%%%%%%
\section{緒言}
%%%%%%%%%%%%%%%%%
様々な工業製品の検査工程においては一部で自動化が進んでいるものの，それぞれの製品の品質管理に精通した検査員の目視検査に頼るところが大きい状況である．最近，深層学習の技術を画像認識に特化させたCNNを製品の欠陥検出に応用しようとする試みがなされている．例えば，寺野らはVGG16アーキテクチャーを用いた転移学習によるコンクリート構造物のひび割れ抽出及び判別方法に
関する研究を行っている\cite{Terano}．また，Perezらは建造物の欠陥・劣化の自動検出と位置特定ができるようにVGG16の転移学習を用いて4カテゴリ分類用に新たなCNNを設計し，分類性能を評価する研究を行っている\cite{Perez}．


著者らは現在，図\ref{fig:wraproll}に示すようなラップフィルム製品の不良品検出の研究を行っている．透明なフィルムは，はみだしや光の反射などにより，市販の画像認識システムを用いても十分な検出結果が得られないという課題がある．これまでにも，オリジナルで設計したCNNを特徴抽出器として用いたSVM\cite{Robomech2019}と，転移学習により新たに設計したCNN\cite{AROB2020}による欠陥部分の検出の基礎研究に取り組んできた．本研究では，より高い分類性能を得るために図\ref{fig:main dialog}に示す開発済みのCNN\&SVM設計ツールを用いて，学習済みのCNNであるInceptionV3 \cite{Szegedy}の転移学習により新たなCNNを設計し，分類性能を評価する．
%%より詳しく%%


%学習済みのCNNの転移学習により新たに設計したCNNを用いて，図\ref{fig:wraproll}のようなラップフィルム製品の不良品検出の研究を行っている．透明なフィルムは，はみだしや光の反射などにより，市販の画像検出器を用いても十分な検出結果が得られない課題がある．そこでオリジナルCNNを特徴抽出器として用いたSVM\cite{Robomech2019}と，転移学習により新たに設計したCNNそれぞれによる欠陥部分の検出の基礎研究に取り組んできた\cite{AROB2020}．%参考文献%
%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
%%%不良品画像%%%
%%%%%%%%%%%%%%%%%
\begin{figure}[t]
 \begin{center}
 %\vspace{1mm}
 \includegraphics[width=80
mm,clip]{./figure1/furyouhinn.eps}
 \caption{Image samples of wrap film products with several kinds of defects after template matching.}
 \label{fig:wraproll}
 \end{center}
 \end{figure}
%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
まず，治具を含むラップフィルム製品の全体画像に対してテンプレートマッチングを適用し，ラップフィルム部分のみを抽出する画像処理を行う．次に学習済みのCNNにはInceptionV3を用い，2クラス分類用に終段にある全結合層部を置き換えた転移学習用の新たなCNNを設計する．このCNNに対して全層を追加学習させたCNN (IncA)と，終段にある全結合層部のみを追加学習させたCNN (IncB)を用いて分類性能を比較する．汎化性を発揮できるように画像拡張（画像オーギュメンテーション）を施した多数の良品と不良品の画像を訓練に用いる．テスト画像の分類実験によって設計したIncAとIncBを比較評価する．
%図\ref{fig:main dialog}に示すMATLAB環境で開発済みのアプリケーションを用いてラップフィルムの製造過程で発生する欠陥の検出を試みる．
%%%%%%%%%%%%%
%%開発したアプリケーション%%
%%%%%%%%%%%%%
 \begin{figure*}[t]
 \begin{center}
 %\vspace{-35mm}
 \includegraphics[width=160mm,clip]{./figure1/application.eps}
  \vspace{-3mm}
  \caption{A part of main dialog developed on MATLAB system to user-friendly design original CNN\cite{Nagata}.}
  \label{fig:main dialog}
  \end{center}
 \end{figure*}
 %%%%%%%%%%%%%%%%
\section{テンプレートマッチングによる訓練画像のダウンサイジング}
%%%%%%%%%%%%%%%%%

様々な課題に対する画像処理で広く利用されているテンプレートマッチングは，撮影されたワークの中で欠陥が含まれやすい領域を抽出するためにも非常に有効である．これから設計するCNNへの入力画像のサイズを大幅に減らすことができ，計算コスト，メモリ占有コストを軽減することができる．このため，開発したメイン，オプション及びオーギュメンテーションのダイアログの中ではテンプレートマッチング機能を利用できるようにしている．%%%%%%%%%%他%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
%%テンプレートマッチング画像%%
%%%%%%%%%%%%%%%%%
\begin{figure}[t]
\begin{center}
\includegraphics[width=80mm ,clip]{./figure1/template_1027.eps}
\vspace{-1mm}
\caption{Configuration among a target image, padding area and template
image with the size of $(M,N)$.}
\label{fig:template}
\end{center}
\end{figure}
%%%%%%%%%%%%%%%%%
%% Template_image画像%%%%
%%%%%%%%%%%%%%%%%
\begin{figure}[t]
\begin{center}
\includegraphics[width=80mm ,clip]{./figure1/templateimage_1027.eps}
\vspace{-2mm}
\caption{An example of extracted image using the template matching
technique.}
\label{fig:templateimage}
\end{center}
\end{figure}

$(M, N)$ のサイズのテンプレートをターゲット画像内でラスタースキャンさせて相関係数の高い位置を検出する場合，周辺領域でもマッチング評価ができるように図\ref{fig:template}
に示すようにパッディング処理を行う．テンプレートとパッディングにより拡張されたターゲット画像内の同面積の領域との相関係数$\alpha(u,v)$は，次式から計算される．
\begin{eqnarray}
\alpha(u,v) = \frac{s_{it}(u,v)}{s_{i}(u,v)s_{t}(u,v)}
\label{eq:10}
\end{eqnarray}
\begin{eqnarray}
s_{it}(u,v)=  \nonumber \\
\sum_{y=v}^{v+N-1}\sum_{x=u}^{u+M-1}\Big\{ f(x,y) - \bar{f}_{u,v} \Big\}
\Big\{ t(x-u,y-v) - \bar{t}\Big\}
\end{eqnarray}
\begin{eqnarray}
s_{i}(u,v)= \sqrt{ \sum_{y=v}^{v+N-1}\sum_{x=u}^{u+M-1}\Big\{ f(x,y) -
\bar{f}_{u,v} \Big\}^2}
\end{eqnarray}
\begin{eqnarray}
s_{t}(u,v)= \sqrt{ \sum_{y=v}^{v+N-1}\sum_{x=u}^{u+M-1}\Big\{ t(x-u,y-v)
- \bar{t}\Big\}^2}
\end{eqnarray}

%%%%%%%%%%%%%%%%%
%%%%%%%%%%%
ここで，$(u,v)$はターゲット画像内におけるテンプレート左上コーナーの座標である．$s_{t}(u,v)$と$s_{i}(u,v)$はそれぞれ，テンプレート内とターゲット内比較領域の標準偏差であり，$s_{it}(u,v)$は共分散である．$f(x,y)$は拡張された画像内の$(x,y)$におけるグレースケール256階調値を正規化した値である．$t(x-u,y-v)$はテンプレート内の$(x-u,y-v)$における同様の値である．$M$と$N$はそれぞれ，テンプレートの幅と高さである．$\bar{t}$と$\bar{f}(u,v)$はそれぞれ，テンプレート内のグレースケール値の平均値と，ターゲット画像内のテンプレート真下の領域のそれである．
式~(\ref{eq:10})で与えられる相関係数$\alpha(u,v)$は，テンプレートをターゲット画像内の左上から右下までラスタースキャンさせることで計算される．ラスタースキャン後，テンプレートと最もマッチする領域，すなわち最も大きな値$\alpha(u,v)$を持つ領域が抽出される．図~\ref{fig:templateimage}には，ラップフィルム品のオリジナル画像からテンプレートで抽出された画像の例を示している．今回の実験では，図\ref{fig:wraproll}のようにテンプレートマッチングにより$640\times480$の解像度を持つラップフィルム製品画像から目標領域を正方形$347\times347$の解像度で切り出し，ダウンサイジングした．
%%%%%%%%%%%%%%%%%
%%IncV3_original%%
%%%%%%%%%%%%%%%%%
 \begin{figure*}[t]
 \begin{center}
 \includegraphics[width=130mm,clip]{./figure1/original_1023.eps}
 \caption{Network architecture of original InceptionV3 .}
 \label{fig:IncV3original}
 \end{center}
 \end{figure*}
%%%%%%%%%%%
%%module Pic%%
%%%%%%%%%%%
 \begin{figure*}[!t]
 \begin{center}
 %\vspace{-35mm}
 \includegraphics[width=130mm,clip]{./figure1/each_module_1015.eps}
  \vspace{-3mm}
  \caption{Each module included in original InceptionV3 network shown in Fig. \ref{fig:IncV3original}.}
  \label{fig:part_of_module}
  \end{center}
 \end{figure*}
%%%%%%%%%%%%%%%%%
%%InceptuionV3の再設計%%
%%%%%%%%%%%%%%%%%
 \begin{figure*}[!t]
 \begin{center}
 \includegraphics[width=130mm,clip]{./figure1/remake_1023.eps}
 \caption{Reconstructed InceptionV3 architecture for binary  classification designed by application shown in Fig. \ref{fig:main dialog}.}
 \label{fig:IncV3}
 \end{center}
 \end{figure*}
%%%%%%%%%%%%%%%%%
\section{InceptionV3による欠陥検出}
%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
\subsection{InceptionV3の転移学習について}
%%%%%%%%%%%%%%%%%%%%%%
本研究では図\ref{fig:main dialog}のアプリケーションを用いてInceptionV3の転移学習により2クラス分類用のCNNを新たに設計する．図\ref{fig:IncV3original}, \ref{fig:part_of_module}にはそれぞれオリジナルのInceptionV3と各モジュールの構造を示している．InceptionV3は，ILSVRC2014で優勝したInceptionV1(GoogLeNet)の畳み込みフィルタのサイズ変更とバッチ正規化を導入したアーキテクチャである\cite{Szegedy}．これはImageNetデータベースの100万枚を超える画像で事前に学習されており，入力画像のサイズは299$\times$299$\times$3である．


%%移学習はある異なるタスクに対して学習済みのCNNのモデルの重みを初期値としてセットし，目的のタスクに応じた学習データセットに合うように全結合層を置き換え，再学習を行う手法である．%%
転移学習の手法の1つとしてFine-tuningがある．Fine-tuningは異なるタスクに対して学習済みのCNNのモデルの重みを初期値とすることで，適用先のデータセットで学習を進める手法である\cite{Nakayama}．%文献を少し書き換える%
図\ref{fig:IncV3}には，この方法に基づき，InceptionV3の終段にある全結合層部を２クラス分類用に新たに設計した構造を示す．KandelらはInceptionV3, VGG16, VGG19それぞれに対して，幾つかのブロックごとにFine-tuningを適用し，組織病理画像をより高い精度で分類できる方法についての研究を行っている~\cite{Kandel}．この研究ではInceptionV3のネットワーク全体をFine-tuningさせることにより，最も良い結果が得られたと報告されている．%また先行研究\cite{Kandel}では，CNNの重みの更新として組織病理画像を用いた研究を行っている．研究結果によると，IncAのようにFine-tuningの工程で全層の重みを更新させた方がよい結果だった．%．

本研究では，全層をFine-tuningさせたものをIncA，終段の全結合層部のみをFine-tuningさせたものをIncBとし，2種類のCNNを設計した．このため，学習後のIncBについては，全結合層部以外の全ての層の重みはInceptionV3のそれと同値を持つことになる．共通の学習条件として，訓練用データセットは良品画像34,482枚と不良品画像2,233枚で構成した．また，最大エポック4, ミニバッチサイズ50, 学習率を0.0001に設定し，訓練を行った．なお，図\ref{fig:main dialog}で紹介したアプリケーション内の右下のチェックボックスで重み更新を行わない層の設定を可能にしている．%Freeze Weightパラメータの切り替えにより，ワンクリックでIncAあるいはIncBの再学習を設定できるようにしている．
%この転移学習を利用して新しい一連のイメージを分類できる転移学習法
%%%%%%%%%%%%%%%%%%%%%%
\subsection{分類実験}
%%%%%%%%%%%%%%%%%%%%%%

訓練後の汎化性能を評価するためにテスト用データセットとして良品画像4,035枚と不良品画像21枚の計4,056枚を用いてIncAとIncBによる分類実験を行った．表\ref{table:sssNetconfusion1}にはIncAの全体的な認識結果を表す混合行列を示す．誤認識された合計枚数は3枚で，その内2枚の不良品画像は良品画像として誤認識され，1枚の良品画像は不良品画像として誤認識されていた．この1枚の画像を観察すると，不良品の画像に類似したグレーな状態であったことが確認された．
%4056枚の画像を用いて分類実験を行った．%
%表\ref{table:sssNetconfusion1}はIncAの全体的な認識結果を表す混合行列を示す．誤認識の合計枚数は3枚で，その内2枚は不良品画像を良品画像として誤認識し，1枚は良品画像を不良品画像として誤認識していた．%%この3枚は良品・不良品にも類似したグレーな状態の画像であったからだと考察する．%%
%%%%%%%%%%%%%%%%%%%%%%
%認識率等結果%
%%%%%%%%%%%%%%%%%%%%%%
表\ref{table:sssNetconfusion1}と同様に表\ref{table:sssNetconfusion2}にはIncBの全体的な認識結果を表す混合行列を示す．誤認識した合計枚数は22枚で，その中にはIncAが誤認識していた画像も含まれていた．
%表\ref{table:sssNetconfusion1}と同様に表\ref{table:sssNetconfusion2}はIncBの全体的な認識結果を表す混合行列を示す．誤認識の合計枚数は22枚で，4枚不良品を良品として誤認識され良品・不良品にも類似したグレーな状態の画像であったからだと考察する．また18枚良品を不良品として誤認識していた画像は欠陥部分の特徴を抽出できていなかったと考察する．

%%%%%%%%%%%%%%%%%
\subsection{比較結果}
%%%%%%%%%%%%%%%%%
次に4つの評価基準である認識率，精度，再現率，及び$F$値について説明する．認識率はテスト画像データセットに対する正解率を，精度は不良品と判断された画像のうち実際に不良品であった割合を，再現率は実際に不良品である画像が正しく不良品と判断された割合を，さらに$F$値は精度と再現率の調和平均の値をそれぞれ表す．これらの指標をもとにIncAとIncBの汎化性能を評価する．全体のデータの中で不良品を不良品として分類したものを$T_P$，良品を良品として分類したものを$T_N$，不良品を良品として誤分類したものを$F_N$，さらに良品を不良品として誤分類したものを$F_P$としたとき認識率$A_c$，精度$P_r$，再現率$R_e$，及び$F$値$F$はそれぞれ，次式で与えられる．
\begin{eqnarray}
A_c = \frac{T_P+T_N}{T_P+T_N+F_P+F_N}  
 \label{eq:Accuracy}
 \end{eqnarray}
\begin{eqnarray}
P_r = \frac{T_P}{T_P+F_P}  
 \label{eq:Presision}
 \end{eqnarray}
\begin{eqnarray}
R_e = \frac{T_P}{T_P+F_N}  
 \label{eq:Recall}
 \end{eqnarray}
\begin{eqnarray}
F = 2\times\frac{P_r \times R_e}{P_r+R_e}  
 \label{eq:F-measure}
 \end{eqnarray}

表\ref{table:Comparison1}には表\ref{table:sssNetconfusion1}，\ref{table:sssNetconfusion2}の結果をもとに算出したIncAとIncBの認識率，精度，再現率，及び$F$値を示している．
%%%%%%%%%%%%%%%%%
%CNN cnfusion InceptionV3%
%%%%%%%%%%%%%%%%%
\begin{table}[t]
\renewcommand{\arraystretch}{1.3}
\small
\caption{Confusion matrix classified by IncA (row: true labels, column: predicted labels).}
\vspace{1mm}
\label{table:sssNetconfusion1}
\centering
\begin{tabular}{c|cccccc}
\hline
 & Normal & Anomaly \\
\hline
Normal & 4034 & 2\\
Anomaly & 1 & 19 \\
\hline
\end{tabular}
\end{table}	
%%%%%%%%%%%%%%%%%
%CNN cnfusion InceptionV3%
%%%%%%%%%%%%%%%%%
\begin{table}[t]
\renewcommand{\arraystretch}{1.3}
\small
\caption{Confusion matrix classified by IncB (row: true labels, column: predicted labels).}
\vspace{1mm}
\label{table:sssNetconfusion2}
\centering
\begin{tabular}{c|cccccc}
\hline
 & Normal & Anomaly \\
\hline
Normal & 4017 & 4\\
Anomaly & 18 & 17 \\
\hline
\end{tabular}
\end{table}	
%%%%%%%%%%%%%%%%%
%%4つの評価基準%%
%%%%%%%%%%%%%%%%%
\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\small
\caption{Statistical evaluation results of IncA and IncB.}
\vspace{1mm}
\label{table:Comparison1}
\centering
\begin{tabular}{c|cccccc} 
\hline
CNN & Accuracy & Precision & Recall & F-measure \\
\hline
IncA & 0.998 & 0.791 & 0.904& 0.844\\
IncB& 0.994& 0.485& 0.809& 0.607\\
\hline
\end{tabular}
\end{table}	
%%%%%%%%%%%%%%%%%
%総評評価%
%%%%%%%%%%%%%%%%%
IncAの認識率と再現率は，それぞれ0.998，0.904であった．再現率が高いということは，取りこぼしなく不良品を検出できているということになる．また，精度は0.791と低い値であったが，これはテスト画像に含まれる良品のサンプル数が4,056であり，不良品の21に対して多かったことに起因していると考えられる．製造ラインにおける不良品の未検出は流通後の商品回収，リコールなど重大な問題に発展する．このため，不良品を良品として誤認識する画像の枚数を減らすために精度よりも再現性の値が重要視される．

表\ref{table:sssNetconfusion1}，\ref{table:sssNetconfusion2}から確認できるようにIncBはIncAより誤認識枚数が多かったため，表\ref{table:Comparison1}のように4つの評価基準となる数値はIncAに比べて低くい結果となった．これらの結果から，全層をFine-tuningさせることでより高い分類性能を得られることが確認できた．
%認識率0.998，精度0.791，再現率0.904，$F$値0.844となった．誤認識枚数が3枚と少なかったため，認識率は高い数値を得た．再現率は，不良品を取りこぼしなく検出できていたため高い数値を得た．精度は，テスト画像に含まれる良品サンプル数が4,035枚が不良品サンプル21枚に対し多かったことに起因している．また，IncBは誤認識枚数が多かったため4つの評価基準となる数値はIncAに比べ全体に低くなっている．
%IncAの認識率と再現率は，それぞれ0.9983，0.9048であった．再現率が高いことから，取りこぼしなく不良品を検出できていた．精度は0.791と低い数値で，これはテスト画像に含まれる良品サンプル数4035枚が，不良品サンプル数が21枚に対し多かったことに起因している．
%%%%%%%%%%%%%%%%%
%IncBの評価%
%%%%%%%%%%%%%%%%%

%また，IncBは誤認識枚数が多かったため4つの評価基準となる数値はIncAに比べ全体に低くなっている．


%認識率0.994，精度0.485，再現率0.809，$F$値0.607となった．誤認識の枚数が多かったため評価%

%IncBの認識率と再現率は，それぞれ0.994，0.809であった．再現率が低いことから，不良品を検出できていなかったことが分かった．それにより精度の値も低くなった．


%%%%%%%%%%%%%%%
\section{結言}
%%%%%%%%%%%%%%%
本研究では，開発済みのCNN\&SVM設計ツールを用いてInseptionV3の転移学習によるCNNを構築し，ラップフィルム製品の製造過程で発生する欠陥検出を試みた．まず，治具を含むラップフィルムの全体画像に対してテンプレートマッチングを適用し，ラップフィルム部分のみを抽出する画像処理を行うことで，良品と不良品のターゲット領域のオリジナル画像を採集した．次に，これらのオリジナル画像と，それらを画像拡張して増やした多数の画像を加えてデータセットを構成し，転移学習の手法の1つとして層の重みを軽微に更新するFine-tuningを用いた．全層をFine-tuningさせたCNNと，終段の全結合層部のみをFine-tuningさせたCNNをそれぞれ作成し，不良品の画像を含むテスト画像のデータセットを用いた分類実験により比較したところ，全層をFine-tuningさせたCNNを用いた場合がより良い分類性能が得られた．これは，Kandelらの組織病理画像を用いた研究と同様の結果となり，ラップフィルム製品の分類でも全層をFine-tuningさせた方が効果的であると確認できた．

今後の展開として，他の製品にも転移学習ベースのCNNによる欠陥検出法を適用し，Fine-tuningの適用方法とともに有効性を評価していきたい．
%アプリケーションの有用性を確認していきたい．

%%%%%%%%%%%%%%%%%	
%参考文献
%%%%%%%%%%%%%%%%%
\begin{thebibliography}{99}
\bibitem{Terano}
寺野 聡恭, 出水 享, 古賀 掲維, 中島 賢哉, 松田 浩, ``深層学習を用いたコンクリート構造物のひび割れ抽出・判別方法に関する研究," 長崎大学大学院工学研究科研究報告, pp. 71--76, 2020.
\bibitem{Perez}
H. Perez, J. H. M. Tah, A. Mosavi,
``Deep Learning for Detecting Building Defects Using Convolutional Neural Networks," {\it Sensors}, Vol. 19, No. 16, pp. 3556--3579, 2019.
\bibitem{Robomech2019}
中島 健斗, 永田 寅臣,  渡辺 桂吾, ``畳み込みニューラルネットワーク(CNN)とサポートベクターマシン(SVM)を用いた微小な欠陥を持つ不良品検出の基礎研究," ロボティクス・メカトロニクス講演会 2019 講演論文集, 2A1-Q05(1--4), 広島国際会議場, 2019.
\bibitem{AROB2020}
K. Nakashima, F. Nagata, H. Ochi, A. Otsuka, T. Ikeda, K. Watanabe, M. K. Habib, ``Detection of Minute Defects Using Transfer Learning-Based CNN Models," {\it  Procs. of 25th International Symposium on Artificial Life and Robotics}, pp. 871--875, 2020.
 \bibitem{Szegedy}
 C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, Z. Wojna ``Rethinking the Inception Architecture for Computer Vision," {\it Procs. of the IEEE Conference on Computer Vision and Pattern Recognition}, pp. 2818--2826, 2016.
\bibitem{Nagata}
 F. Nagata, K. Tokuno, K. Mitarai, A. Otsuka, T. Ikeda, H. Ochi, K. Watanabe, M. K. Habib, ``Defect Detection Method Using Deep Convolutional Neural Network, Support Vector Machine and Template Matching Techniques, {\it Artificial Life and Robotics}," Vol. 24, No. 4, pp. 512--519, 2019. 
\bibitem{Nakayama}
中山 英樹 ``深層畳み込みニューラルネットワークによる画像特徴抽出と転移学習," 信学技報, pp. 55--59,~2015.
\bibitem{Kandel}
I. Kandel and M. Castelli, ``How Deeply to Fine-Tune a Convolutional Neural Network: A Case Study Using a Histopathology Dataset," {\it Applied Science}, Vol. 10, No. 10, pp. 1--20, 2020.
\end{thebibliography}
%
%
%
\end{document}


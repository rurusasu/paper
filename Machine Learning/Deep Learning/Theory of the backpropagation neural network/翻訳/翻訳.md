# Theory of the backpropagation neural network

# 備考

## 著者

Robert Hecht-Nielsen

## 掲載

International 1989 Joint Conference on Neural Networks, Vol. 1, pp. 593--605, 1989.

# Abstract

バックプロパゲーションは、現在最も広く適用されているニューラルネットワークのアーキテクチャである。その情報処理操作は、写像の作用例 $(x_1, y_1), (x_2, y_2), \dots , (x_k. y_k), \dots$ に対する学習により、$n$次元ユークリッド空間の有界部分集合$A$から$m$次元ユークリッド空間の有界部分集合$f[A]$への写像または関数 $f : A \in R^n \rightarrow R^m, y_k = f(x_k)$ を近似するものである。このような例は、固定された確率密度関数$\rho(x)$に従って、$A$から$x_k$ベクトルをランダムに選択することによって生成されると仮定される。本論文では、バックプロパゲーション・ニューラルネットワークの基本理論について、アーキテクチャ設計、性能測定、関数近似能力、学習の各分野についてサーベイを行う。バックプロパゲーション・ニューラルネットワークを有効なニューラルネットワークとするための定式化（過去の定式化は処理の局所性の制約に違反していた）、バックプロパゲーションの平均平方根誤差関数が存在し微分可能であることの証明など、既知の内容に加えていくつかの新しい結果を含んでいる。また、$[0, 1]^n$から$R^m$までの任意の$L_2$関数が3層のバックプロパゲーション・ニューラルネットワークで任意の精度で実装できることを示す定理も含まれている。最後に付録として、バックプロパゲーション・ニューラル・ネットワークが哺乳類の脳でどのように実装され、大脳皮質の近傍領域間の皮質学習が可能になるかを、具体的な脳生理学的モデルで説明する。

# Abst

機械学習アルゴリズムの成功は、一般的にデータ表現に依存しており、我々は、異なる表現によって、データの背後にある変動の異なる説明因子を多かれ少なかれ絡めて隠すことができるからだと仮説を立てている。表現の設計には特定のドメインの知識を用いることができるが、事前分布を用いた学習も可能であり、AIの探求はそのような分布を実装したより強力な表現学習アルゴリズムの設計の動機となっている。本論文では、教師なし特徴学習と深層学習の共同学習の分野における最近の研究をレビューし、確率モデル、オートエンコーダー、多様体学習、深層アーキテクチャの進歩をカバーします。これは、良い表現を学習するための適切な目的、表現を計算するための目的（すなわち、推論）、および表現学習、密度推定、多様体学習の間の幾何学的な接続についてのより長期的な未回答の質問を動機付けます。

# 1. Intro

機械学習手法の性能は、適用するデータ表現（または特徴）の選択に大きく依存します。そのため、機械学習アルゴリズムを導入する際には、効果的な機械学習を可能にするデータ表現を実現するための前処理パイプラインやデータ変換の設計に多くの労力が費やされます。**このような特徴量工学は重要ですが、手間がかかり、現在の学習アルゴリズムの弱点である、データから識別情報を抽出・整理できないという点が浮き彫りになります。特徴量工学は、その弱点を補うために、人間の創意工夫や事前の知識を活用する方法です。**機械学習の適用範囲や適用のしやすさを拡大するためには、学習アルゴリズムを特徴量工学に依存しないものにして、新規のアプリケーションをより早く構築することが強く望まれますし、何よりも人工知能（AI）への進展が期待されます。人工知能は、私たちを取り巻く世界を根本的に理解しなければなりませんが、それは、観察された低レベルの感覚データの環境に隠された根本的な説明要因を識別し、分離することを学習して初めて達成されると私たちは主張します。

この論文では、特徴量学習、または表現学習、すなわち、**分類器やその他の予測器を構築する際に有用な情報を抽出しやすくするためのデータの変換を学習することについて述べています。**確率モデルの場合、良い表現とは、観測された入力に対する基本的な説明因子の事後分布を捕らえるものであることが多い。

表現を学習する様々な方法の中でも、本稿では深層学習法に焦点を当てます。**深層学習法とは、より抽象的な、そして最終的にはより有用な表現を得ることを目的とした、データの複数の非線形変換の組み合わせによって形成される手法**です。ここでは、急速に発展しているこの分野について、特に最近の進展を中心に概観します。また、この分野の研究を推進してきた基本的な問題についても考察する。具体的には、ある表現が他の表現よりも優れている理由は何か？ある例が与えられたとき、その表現をどのように計算すればよいのか、つまり特徴抽出をどのように行えばよいのか。また、良い表現を学ぶための適切な目標は何か？これらの問題を解決するために、この分野で最も人気のあるモデルのいくつかをレビューし、この分野全体の文脈の中でそれらを位置づけます。

# 2. なぜ学習表現にこだわるのか？

表現学習は、機械学習の分野では、それ自体が一つの分野となっており、NIPSやICMLなどの主要な会議では、深層学習や特徴学習というヘッダーで定期的にワークショップが開催されている。深さは重要な部分であるが、他の多くのプリオールも興味深く、次節で述べるように、学習問題を表現の学習の1つとして考えると、学習者は便利に取り込むことができる。表現学習に関する科学的活動の急速な増加は、学術界と産業界の両方での顕著な経験的成功を伴っており、（好循環の中で）養われてきた。このセクションでは、これらの成功例のいくつかを簡単に紹介する。

## 音声認識・信号処理

音声認識は、ニューラルネットワーク、特に畳み込みニューラルネットワーク（または時間遅延ニューラルネットワーク）の初期の応用例の一つでした。近年、ニューラルネットワーク、深層学習、表現学習への関心が高まっており、音声認識の分野にも強い影響を与えています。いくつかの学術機関では画期的な成果 (Dahl ら., 2010; Seide ら., 2011; Mohamed ら., 2012; Dahl ら., 2012) が得られているほか、産業界の研究所の研究者がこれらのアルゴリズムを大規模化して製品化する作業を引き継いでいます。例えば、Microsoft社は2012年に、深層学習に基づいたMAVIS (Microsoft Audio Video Indexing Service) 音声システムの新バージョンをリリースしました (Seide ら.、2011)。彼らは、音響モデリングにガウス混合を用い、同量のデータ（309時間分の音声）で学習した最先端のモデルと比較して、4つの主要なベンチマークにおける単語の誤り率を約30％削減することに成功しました（例：RT03Sで27.4％から18.5％）。Dahlら（2012）は，より少ない大語彙の音声認識ベンチマーク（Bingモバイルビジネス検索データセット，40時間の音声）で得たエラーレートの相対的な改善率は16%から23%であった．

表現学習アルゴリズム（リカレントニューラルネットワークに基づく）は、音楽にも適用されており、4つの異なるデータセットからなる標準的なベンチマークにおいて、相対誤差が5％から30％改善され、ポリフォニックトランスクリプションにおける最先端の技術を大幅に上回った（Boulanger-Lewandowski ら, 2012）。

## 物体分類

2006年の深層学習の始まりは、MNISTの数字画像分類問題に焦点を当てたもので（Hintonら.2006a; Bengio ら.2007）、このデータセットでSVMの優位性（誤差1.4%）を打ち破った。最新の記録は、依然としてディープネットワークが保持しています。Ciresanら（2012）は，制約のないバージョンのタスク（畳み込みアーキテクチャを使用するなど）では，0.81%の誤差で最新のタイトルを獲得しており，Rifaiら（2011c）は，MNISTのフリーバージョンで0.81%の誤差で最新のタイトルを獲得しています．

ここ数年、深層学習は自然画像の物体認識に大きな影響を与えており、ImageNetでは、最先端のエラーレートを15.3%にまで下げることに成功しています(Krizhevsky ら., 2012)。

## 自然言語処理

音声認識以外にも、表現学習アルゴリズムの自然言語処理への応用は数多くあります。記号データの分散表現のアイデアは、Hinton (1986)によって紹介され、統計的言語モデリングの文脈では、Bengio ら. これらのアルゴリズムはすべて、各単語の分散表現を学習することに基づいており、単語埋め込みとも呼ばれます。このアイデアと畳み込みアーキテクチャを組み合わせて、Collobertら（2011）は、言語モデリング、品詞タグ付け、チャンキング、名前付き実体認識、意味的役割のラベル付け、および構文解析のタスク間で表現を共有するSENNAシステムを開発しました。SENNAは、これらのタスクで最先端に近づき、あるいは凌駕していますが、従来の予測器よりもはるかに高速で、予測を実行するために必要なCコードの行数はわずか3500行です。

ニューラルネットの言語モデルも、隠れ層に再帰性を加えることで改良されました（Mikolov ら, 2011）、perplexity（正しい次の単語を予測する平均負の対数尤度の指数で、140から102に下がった）だけでなく、音声認識における単語誤り率（言語モデルは音声認識システムの重要な構成要素であるため）の点でも最先端（平滑化されたn-gramモデル）を打ち負かすことができ、Wall Street Journalのベンチマークタスクで17.2%（KN5ベースライン）または16.9%（識別的言語モデル）から14.4%に減少しました。同様のモデルは、統計的機械翻訳にも適用されており（Schwenk ら.、2012）、BLEUスコアが2ポイント近く向上しています。また，再帰オートエンコーダー（リカレントネットワークを一般化したもの）を用いて，全文パラフレーズ検出のF1スコアをほぼ2倍にすることに成功した(Socher ら., 2011a)．表現学習は語義曖昧性解消にも用いられ(Bordes ら., 2012)，Senseval-3のサブセット(主語-動詞-目的語の文)に適用したところ，67.8%から70.2%の精度を得た．また、感情分析の分野でも最先端の技術を上回る成果を上げています(Glorot ら., 2011b; Socher ら., 2011b)。

## マルチタスク・伝達学習、ドメイン適応

転移学習とは、学習アルゴリズムが、異なる学習タスク間の共通点を利用して、統計的な強みを共有し、タスク間で知識を移転する能力のことである。後述するように、表現学習アルゴリズムは、図1に示すように、それぞれのタスクに関連する基礎的な要因を捉える表現を学習するため、このようなタスクに有利であるという仮説を立てた。この仮説は、表現学習アルゴリズムが伝達学習のシナリオで強みを発揮することを示す多くの経験的な結果によって確認されているようだ。

最も印象的なのは、2011年に開催された2つの転移学習チャレンジで、表現学習アルゴリズムが優勝したことです。まず、Transfer Learning Challengeは、ICML 2011の同名のワークショップで発表され、教師なしのレイヤーワイズの事前学習を用いて優勝しました(Bengio, 2011; Mesnil ら., 2011)。同年に開催された第2回Transfer Learning Challengeでは，Goodfellowらが優勝しました（2011）．この結果は、NIPS 2011のChallenges in Learning Hierarchical Models Workshopで発表されました。他にも、表現学習を転移学習に関連する分野に適用して成功した例として、対象は同じだが入力分布が変化するドメイン適応がある（Glorot ら 2011b; Chen ら 2012）。もちろん、多くのタスクやクラスの出力を共同で予測する場合、つまりマルチタスク学習を行う場合も、表現学習アルゴリズムの優位性を高めることができる (Krizhevsky ら. (2012); Collobert ら. (2011)).

# 3. 表現の良さとは？
## 3.1. AIにおける表現学習のためのプリオール

Bengio and LeCun (2007)では、私たちの一人が、現在の機械学習アルゴリズムにとって困難な、複雑かつ高度に構造化された依存関係を含むAIタスクという概念を紹介しました。表象を明示的に扱うことが興味深い理由の一つは、身の回りの世界に関する多くの一般的な分布、つまりタスクに特化していないが、学習機械がAIタスクを解決するために有用であろうと思われる分布を表現するのに便利だからです。そのような汎用分布の例として、以下のようなものがあります。

* **Smoothness:**
関数 $f s.t. x ≒ y$ を学習したい場合、一般的には $f(x)≒f(y)$ となります。これは最も基本的な優先順位であり、ほとんどの機械学習に存在するものですが「次元の呪い」を回避するには不十分です。セクション3.2で説明します。

* **複数の説明因子:**
データを生成する分布は、異なる基礎的な要因によって生成されており、ほとんどの場合、ある要因について学んだことは、他の要因の多くの構成で一般化されます。これらの基本的な変動要因を回復する、あるいは少なくとも分離するという目的については、3.5節で説明します。この仮定は、以下のセクション3.3で説明する分散表現のアイデアの背景にあります。

* ** 説明因子の階層的な構成**
私たちを取り巻く世界を説明するのに役立つ概念は、他の概念の観点から、階層的に定義することができ、抽象度の高い概念は、抽象度の低い概念の観点から定義されます。これが、深い表現を持つことで得られる前提です。以下のセクション3.4で詳しく説明します。

* **半教師付き学習**
予測したい入力変数 $X$ と目標変数 $Y$ がある場合、$X$ の分布を説明する要因の一部は、$X$ が与えられたYの多くを説明します。したがって、$P(X)$ に有用な表現は、$P(Y |X)$ を学習する際にも有用である傾向があり、セクション4で議論するように、教師なし学習と教師あり学習の間で統計的な強さを共有することができます。

*  **タスク間で共通する要素**
多くの $Y's$ や多くの学習タスクがある状況では、タスク（例えば、対応する $P(Y |X,task)$ ）は、他のタスクと共有される要因で説明され、前節（マルチタスクと転移学習、ドメイン適応）で議論したように、タスク間で統計的な強みを共有することができます。

* **マニホールド**
確率質量は、データが存在する元の空間よりもはるかに小さな次元の領域の近くに集中します。このことは、7.2節および8節で説明する自動符号化アルゴリズムおよびその他の多様体に着想を得たアルゴリズムの一部で明示的に利用されています。

* **自然なクラスター化。**
オブジェクトクラスのようなカテゴリー変数の異なる値は、別々の多様体に関連付けられます。より正確には，多様体上の局所的な変化はカテゴリーの値を保持する傾向があり，一般に異なるクラスの例間の線形補間は低密度領域を通過することになる．すなわち，異なる $i$ に対する $P(X|Y = i)$ はよく分離され，あまり重ならない傾向がある．例えば、これは8.3節で述べたManifold Tangent Classifierで利用されています。この仮説は、人間がカテゴリーやクラスという名前をつけたのは、そのような統計的構造（脳が発見し、文化が伝播したもの）があったからだという考えと一致しており、機械学習のタスクでは、そのようなカテゴリー変数を予測することが多い。

* **時間的・空間的なコヒーレンス**
これはクラスターの仮定に似ていますが、オブザベーションのシーケンスに関するものです。連続した、または空間的に近いオブザベーションは、関連するカテゴリー概念の同じ値に関連する傾向があり、または高密度多様体の表面上で小さな動きをもたらします。より一般的には、様々な要因が異なる時間的・空間的スケールで変化し、関心のある多くのカテゴリー概念はゆっくりと変化します。このようなカテゴリー変数を捉えようとする場合、関連する表現をゆっくりと変化させる、つまり時間や空間に伴う値の変化にペナルティを課すことで、この事前処理を実施することができる。この優先順位はBecker and Hinton (1992)で紹介され、セクション11.3で議論されています。
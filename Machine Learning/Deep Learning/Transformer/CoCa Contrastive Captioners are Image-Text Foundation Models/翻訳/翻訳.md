# 1. Introduction

ディープラーニングは最近、BERT [2], T5 [3], GPT-3 [4] などの基礎言語モデル [1] の台頭を目の当たりにしました。個々のモデルに特化した場合と比較して、大規模な下流タスクのための基礎モデルの事前学習は、学習コストを償却し、人間レベルの知能のためのモデル規模の限界 [5] を押し上げる機会を提供することが可能です。

視覚と視覚言語問題については、いくつかの基礎モデル候補が検討されてきた。 (1) 先駆的な研究 [6, 7, 8] は、ImageNet [9] などの画像分類データセットに対してクロスエントロピ損失で事前学習したシングルエンコーダーモデルの有効性を示している。画像エンコーダは、画像やビデオの理解を含む様々な下流タスクに適応可能な汎用的な視覚表現を提供します[10, 11]。しかし、これらのモデルは、ラベル付きベクトルとしての画像注釈に大きく依存しており、人間の自由形式の自然言語の知識を取り込まないため、視覚と言語の両方の様式を含む下流のタスクへの応用を妨げている。(2) 最近、ウェブスケールのノイズの多い画像-テキストペアに対して、対照的な損失を持つ2つの並列エンコーダの事前学習を行うことで、画像-テキスト基礎モデル候補の実現可能性が示された研究 [12, 13, 14]がある。視覚のみのタスクのための視覚的埋め込みに加え、得られたデュアルエンコーダーモデルはさらに、同じ潜在空間へのテキスト的埋め込みを符号化することができ、ゼロショット画像分類や画像-テキスト検索といった新しいクロスモーダルアライメント能力を可能にします。しかし、これらのモデルは、画像とテキストを融合した表現を学習する共同成分がないため、視覚的質問応答（VQA）のような視覚と言語の共同理解タスクには直接適用できない。(3) 別の研究[15, 16, 17]では、一般的な視覚・マルチモーダル表現を学習するためのエンコーダ・デコーダモデルによる生成的事前学習について研究している。前学習では、エンコーダ側で画像を取り込み、デコーダ出力に対して言語モデリング（LM）ロス（またはPrefixLM[3, 16]）を適用する。下流のタスクでは、デコーダ出力はマルチモーダル理解タスクのための共同表現として使用することができます。事前学習されたエンコーダ・デコーダモデルにより、優れた視覚言語結果[16]が得られているが、画像埋め込みと整合したテキストのみの表現を生成しないため、クロスモーダルな整合タスクの実現性と効率性は低い。

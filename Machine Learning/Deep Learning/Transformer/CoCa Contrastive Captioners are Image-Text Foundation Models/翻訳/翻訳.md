# 1. Introduction

ディープラーニングは最近、BERT [2], T5 [3], GPT-3 [4] などの基礎言語モデル [1] の台頭を目の当たりにしました。個々のモデルに特化した場合と比較して、大規模な下流タスクのための基礎モデルの事前学習は、学習コストを償却し、人間レベルの知能のためのモデル規模の限界 [5] を押し上げる機会を提供することが可能です。

視覚と視覚言語問題については、いくつかの基礎モデル候補が検討されてきた。 (1) 先駆的な研究 [6, 7, 8] は、ImageNet [9] などの画像分類データセットに対してクロスエントロピ損失で事前学習したシングルエンコーダーモデルの有効性を示している。画像エンコーダは、画像やビデオの理解を含む様々な下流タスクに適応可能な汎用的な視覚表現を提供します[10, 11]。しかし、これらのモデルは、ラベル付きベクトルとしての画像注釈に大きく依存しており、人間の自由形式の自然言語の知識を取り込まないため、視覚と言語の両方の様式を含む下流のタスクへの応用を妨げている。(2) 最近、ウェブスケールのノイズの多い画像-テキストペアに対して、対照的な損失を持つ2つの並列エンコーダの事前学習を行うことで、画像-テキスト基礎モデル候補の実現可能性が示された研究[12, 13, 14]がある。視覚のみのタスクのための視覚的埋め込みに加え、得られたデュアルエンコーダーモデルはさらに、同じ潜在空間へのテキスト的埋め込みを符号化することができ、ゼロショット画像分類や画像-テキスト検索といった新しいクロスモーダルアライメント能力を可能にします。しかし、これらのモデルは、画像とテキストを融合した表現を学習する共同成分がないため、視覚的質問応答（VQA）のような視覚と言語の共同理解タスクには直接適用できない。(3) 別の研究[15, 16, 17]では、一般的な視覚・マルチモーダル表現を学習するためのエンコーダ・デコーダモデルによる生成的事前学習について研究している。前学習では、エンコーダ側で画像を取り込み、デコーダ出力に対して言語モデリング（LM）ロス（またはPrefixLM[3, 16]）を適用する。下流のタスクでは、デコーダ出力はマルチモーダル理解タスクのための共同表現として使用することができます。事前学習されたエンコーダ・デコーダモデルにより、優れた視覚言語結果[16]が得られているが、画像埋め込みと整合したテキストのみの表現を生成しないため、クロスモーダルな整合タスクの実現性と効率性は低い。

**本研究では、シングルエンコーダー、デュアルエンコーダー、エンコーダーデコーダーのパラダイムを統一し、3つのアプローチの能力を包含する一つの画像・テキスト基盤モデルを学習する。** 我々は、コントラスト損失とキャプション（生成）損失の両方を用いて訓練された修正エンコーダ・デコーダ・アーキテクチャを持つコントラストキャプション（CoCa）と名付けたシンプルなモデルファミリーを提案する。図1に示すように、我々はデコーダ変換器をユニモーダルデコーダとマルチモーダルデコーダの2つの部分に分離している。ユニモーダルデコーダ層ではクロスアテンションを省略し、テキストのみの表現をエンコードし、マルチモーダルデコーダ層は画像エンコーダ出力にクロスアテンションして、マルチモーダル画像-テキスト表現を学習するようにカスケード接続する。画像エンコーダとユニモーダルテキストデコーダの出力には対比目的、マルチモーダルデコーダの出力にはキャプション目的の両方を適用している。さらに、CoCaは画像アノテーションデータとノイズの多い画像テキストデータの両方に対して、全てのラベルを単にテキストとして扱うことにより学習を行う。画像注釈テキストに対する生成損失は、シングルエンコーダのクロスエントロピー損失アプローチと同様のきめ細かい学習信号を提供し、3つのプリトレーニングパラダイムを効果的に単一の統一手法に統合しています。

CoCaのデザインは、グローバルな表現を学習するコントラスト学習と、細かい領域レベルの特徴を学習するキャプションを活用することで、図1に示す3つのカテゴリすべてのタスクに恩恵をもたらします。CoCaは、ゼロショット転送や最小限のタスク特異的適応を用いることで、単一の事前学習済みモデルが多くの特殊なモデルを凌駕できることを示している。例えば、CoCaはImageNetで86.3%のゼロショット精度、MSCOCOとFlickr30kでより優れたゼロショットクロスモーダル検索を獲得しています。凍結エンコーダの場合、CoCaはImageNetの分類で90.6%、Kinetics-40/600/700で88.0%、88.5%、81.1%、Moments-in-Timeで47.4%を達成しました。さらに、軽量化により、ImageNetで91.0%、VQAで82.3%、NoCapsで120.6%のCIDErスコアを達成した。

# 2. RELATED WORK

ビジョンの事前学習ImageNet [6, 7, 8]、Instagram [20]、JFT [21]などの大規模アノテーションデータに対するConvNets [18] やTransformers [19] の事前学習は、分類、位置特定、分割、ビデオ認識、追跡、その他多くの問題を含む視覚認識問題の解決に向けて人気の戦略になっています。近年では、自己教師付き事前学習アプローチも検討されている。BEiT [22]は自然言語処理におけるBERT [2]に準じたマスク画像モデリングタスクを提案し、予測対象として量子化された視覚的トークンIDを用いている。MAE [23]とSimMIM [24]は画像トークナイザーを必要とせず、軽量なデコーダや投影層を直接使用して画素値を回帰させる。しかし、これらの方法は視覚モダリティのモデルのみを学習するため、画像とテキスト入力の両方に対する共同推論を必要とするタスクには適用できない。

**視覚-言語プリトレーニング:** 近年、視覚と言語を融合させることを目的とした視覚-言語プリトレーニング（VLP）が急速に発展している。この方向での初期の研究（例えば、LXMERT [25], UNITER [26], VinVL [27]）は、視覚表現を抽出するためにFast(er) R-CNN [28] などの事前に学習した物体検出モジュールに依存している。その後、ViLT [29]やVLMo [30]などは、視覚と言語の変換器を統合し、マルチモーダル変換器を一から学習させるものである。

**画像テキスト基盤モデル:** 最近の研究では、視覚と視覚言語の両方の事前学習を含むことができる画像-テキスト基盤モデルを提案しています。CLIP [12]とALIGN [13]は、ノイズの多い画像とテキストのペアに対比的な目標を設定して事前学習したデュアルエンコーダモデルが、クロスモーダルな位置合わせタスクとゼロショット画像分類のために強力な画像とテキスト表現を学習できることを実証しています。Florence [14]はこの方法をさらに発展させ、広範囲の視覚と画像-テキストベンチマークに適応できる基礎モデルを学習する。ゼロショット画像分類の精度をさらに向上させるために、LiT [31] と BASIC [32] は、まず大規模な画像アノテーションデータセットでモデルをクロスエントロピで事前学習し、さらにノイズの多いオルトテキスト画像データセットでコントラストロスで細かく学習する。また、別の研究[16, 17, 33]では、生成損失を用いて学習したエンコーダ・デコーダモデルを提案し、視覚言語ベンチマークにおいて強い結果を示しています。この研究では、これらのアプローチを統一するために、単一の事前学習ステージでゼロから画像-テキスト基礎モデルを学習することに焦点を当てる。FLAVA [34]とBLIP [35]も画像-テキスト統合を研究していますが、これらはユニモーダルおよびマルチモーダルモジュールの複数の事前学習段階を必要とします。

# 3. APPROACH

まず、自然言語監視を利用する3つの基礎モデルファミリ（シングルエンコーダ分類プリトレーニング、デュアルエンコーダ対照学習、エンコーダ・デコーダ画像キャプション）のレビューから始める。次に、コントラスト学習と画像からキャプション生成の両方のメリットをシンプルなアーキテクチャで共有するコントラストキャプショナ（CoCa）を紹介する。さらに、CoCaモデルが、ゼロショット転送や最小限のタスク適応で、下流タスクに迅速に移行する方法について議論する。

## 3.1. 自然言語監視

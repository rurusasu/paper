# Swin Transformer: Hierarchical Vision Transformer using Shifted Windows

# 1. INTRODUCTION

コンピュータビジョンのモデリングは、長い間畳み込みニューラルネットワーク（CNN）によって支配されてきた。AlexNet [38]とImageNet画像分類課題におけるその革命的な性能に始まり、CNNアーキテクチャは、より大きなスケール [29、73]、より広範な接続 [33]、およびより洗練された畳み込みの形 [67, 17, 81]によってますます強力になって進化してきた。CNNは様々な視覚タスクの基幹ネットワークとして機能するため、これらのアーキテクチャの進歩は、この分野全体を広く引き上げる性能向上につながった。

一方、自然言語処理（NLP）におけるネットワークアーキテクチャの進化は異なる道を歩み、今日普及しているアーキテクチャは、代わりにTransformer[61]である。Transformerはシーケンスのモデリングとトランスダクションタスクのために設計され、データ中の長距離依存性をモデル化するためにアテンションを使用することが特徴である。言語領域におけるその大きな成功により、研究者はコンピュータビジョンへの適応を調査するようになり、最近特定のタスク、特に画像分類［19］とビジョンと言語のジョイントモデリング［46］において有望な結果を実証している。

本論文では、NLPやビジョンにおけるCNNのように、コンピュータビジョンのための汎用的なバックボーンとしてTransformerの適用性を拡張することを試みる。我々は、言語領域での高い性能を視覚領域に移行させる際の重要な課題が、2つのモダリティの違いによって説明できることを観察している。この違いの1つは、スケールである。言語トランスフォーマーにおける処理の基本要素となる単語トークンとは異なり、視覚的要素はスケールにおいて大幅に変化することができ、この問題はオブジェクト検出などのタスクにおいて注目されている[41, 52, 53]。既存のTransformerに基づくモデル[61, 19]では、トークンは全て固定スケールであり、このような視覚の用途には適さない性質である。もう一つの違いは、テキスト中の単語と比較して、画像中のピクセルの解像度がはるかに高いことである。セマンティックセグメンテーションのような、ピクセルレベルでの高密度な予測を必要とするビジョンタスクは多く存在するが、高解像度画像におけるTransformerの自己注意の計算量は画像サイズの2次式となるため、これは実用的でないだろう。これらの課題を克服するために、階層的な特徴マップを構築し、画像サイズに対して線形な計算量を持つSwin Transformerと呼ばれる汎用的なTransformerのバックボーンを提案する。図1(a)に示すように、Swin Transformerは小さいサイズのパッチ（グレーで表示）から始まり、より深いTransformer層で徐々に隣接するパッチを結合して、階層的な表現を構築する。これらの階層的な特徴マップにより、Swin Transformerモデルは、特徴ピラミッドネットワーク（FPN）[41]やU-Net[50]などの高密度予測のための先進技術を便利に利用することができます。線形計算量は、画像を分割する非重複ウィンドウ（赤で囲んだ部分）内で局所的に自己注意を計算することで達成される。各ウィンドウに含まれるパッチの数は固定であるため、計算量は画像サイズに対して線形になる。これらの利点により、Swin Transformerは、単一の解像度の特徴マップを生成し、二次的な複雑さを持つ以前のTransformerベースのアーキテクチャ[19]とは対照的に、様々なビジョンタスクの汎用バックボーンとして適しています。

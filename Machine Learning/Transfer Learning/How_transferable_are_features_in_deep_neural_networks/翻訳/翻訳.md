# How transferable are features in deep neural networks?

# 備考

図2から言えること。
1. 1000クラス分類と500クラス分類の問題に対して同じネットワークを訓練したところ、500クラス分類の方が精度が高かった。これは、単純にクラス数が半分になったため誤差が小さくなったと考えられる。
2. n層まで知識を転移し、追加学習時にパラメタを固定し場合、n=3~5にかけて性能の低下が観察された。これは、この部分付近に脆弱な共適応の特徴が連続する層に含まれていたためと考えられる。よって、実験結果からネットワークの下部および上部に比べて中央部では共適応の特徴が多分に含まれており、この部分でネットワークを分断して再学習することはその後の性能低下に繋がると推察される。
3. 重みを微調整することで、共適応の特徴に起因すると考えられる性能の低下を防ぐことができる。
4. 1000クラス分類から500クラス分類の知識の転移ではn=4～7において顕著に性能を低下させる。この原因として、次の2つが考えられる。
   * 共適応の特徴が破壊されたことによる性能の低下（n=3, 4, 5で支配的）
   * 一般的な特徴が少なくなったことによる低下（n=6, 7で支配的）
5. この論文が発表される前までの転移学習は小さなデータセットで過学習を発生させないための手段であったが、データセットが大きくても汎化性能が向上することが示された。

## 著者

Jason Yosinski, Jeff Clune, Yoshua Bengio, Hod Lipson

## 掲載

``How transferable are features in deep neural networks?.'' Procs. Advances in neural information processing systems 27 (NIPS 2014), pp. 3320–3328, 2014.

# Abstract

自然画像上で訓練された多くのディープニューラルネットワークには、共通して不思議な現象が見られます：第1層では、ガボールフィルタやカラーブロブに似た特徴を学習します。 このような第1層の特徴は、特定のデータセットやタスクに特化したものではなく、多くのデータセットやタスクに適用できるという点で一般的なもののように見えます。このような特徴は、最終的にはネットワークの最終層で一般的なものから特定のものへと移行しなければならないが、この移行についてはこれまで広く研究されていなかった。この論文では、深層畳み込みニューラルネットワークの各層のニューロンの一般性と特異性を実験的に定量化し、いくつかの驚くべき結果を報告する。伝達可能性は、2つの異なる問題によってネガティブな影響を受ける。 (1)目標タスクの性能を犠牲にして、高次の層のニューロンが元のタスクに特化してしまうこと、(2)共同適応したニューロン間のネットワーク分割に関連した最適化の難しさ、である。本研究では、ImageNet上で学習したネットワークの例を用いて、ネットワークの下部、中部、上部のいずれから特徴を伝達するかによって、これら2つの問題のいずれかが支配的になることを実証した。 また、ベースタスクとターゲットタスクの距離が長くなるほど特徴の伝達性は低下するが、遠くのタスクからでもランダムな特徴を使うよりは、特徴を伝達する方が優れていることを示した。 最後の驚くべき結果は、ほとんど何層からでも転送された特徴を用いてネットワークを初期化することで、ターゲットデータセットを微調整した後でも、一般化を促進することができるということである。

# 1. Introduction

最近のディープニューラルネットワークには不思議な現象が見られます：画像上で学習すると、Gaborフィルターやカラーブロブに似た第一層の特徴を学習する傾向があります。これらのフィルタの出現は非常に一般的で、自然な画像データセットでそれ以外のものを取得すると、ハイパーパラメタの選択の誤りやソフトウェアのバグが疑われます。 この現象は，異なるデータセットだけでなく，教師付き画像分類（Krizhevskyet al. 2012），教師なし密度学習（Leeet al. 2009），疎な表現の教師なし学習（Leet al. 2011）など，非常に異なる学習目的でも発生する．

これらの標準的な特徴は、正確なコスト関数や自然画像のデータセットに関係なく、第1層の特徴を見つけることができるように思われるので、これらの **第1層の特徴を一般的なものと呼ぶことにする。** 一方、学習されたネットワークの最後の層で計算される特徴は、選択されたデータセットとタスクに大きく依存することがわかっています。例えば、教師付き分類の目的に向かってうまく訓練されたN次元ソフトマックス出力層を持つネットワークでは、各出力ユニットは特定のクラスに固有のものになります。このように、**最後の層の特徴を特定のもの** と呼んでいます。これらの一般的な概念と特異的な概念は直感的なものであり、以下ではより厳密な定義を提供します。  第一層の特徴が一般的であり、第二層の特徴が特異的であるならば、ネットワークのどこかで一般的なものから特異的なものへの移行があるに違いありません。この観察は、いくつかの疑問を投げかけています。

- 特定の層が一般的なのか、特定の層が特定的なのか、その程度を定量化できるか。
- 遷移は単一のレイヤーで突然起こるのか、それとも複数のレイヤーに分散して起こるのか？
- この遷移はどこで起こるのか：ネットワークの最初の層、中間層、最後の層の近くか？

なぜなら、ネットワーク内の特徴が一般的なものであれば、転送学習に利用できるからである(Caruana, 1995; Bengio et al., 2011; Bengio, 2011)。伝達学習では、まずベースとなるデータセットとタスクでネットワークを学習し、学習した特徴を第2のターゲット・ネットワークに再利用し、ターゲット・データセットとタスクで学習する。 このプロセスは、特徴が一般的なもの、つまりベースタスクに特化したものではなく、ベースタスクとターゲットタスクの両方に適したものであれば、うまくいく傾向があります。

ターゲット・データセットがベース・データセットよりも著しく小さい場合、転移学習は、オーバーフィッティングなしで大規模なターゲット・ネットワークを訓練することを可能にする強力なツールとなり得る；最近の研究では、この事実を利用して、より高い層から転移する際に最先端の結果を得ている(Donahueet al., 2013a; Zeiler and Fergus, 2013; Sermanetet al., 2014)が、これらの層のニューラル・ネットワークが実際にかなり一般的な特徴を計算していることをまとめて示唆している。 これらの結果は、この一般性の正確な性質と範囲を研究することの重要性をさらに強調している。

通常の転移学習アプローチは、ベースネットワークを訓練し、その最初の $n$ 層をターゲットネットワークの最初の $n$ 層にコピーします。次に、ターゲット・ネットワークの残りの層はランダムに初期化され、ターゲット・タスクに向かって訓練される。  新しいタスクからの誤差をベース（コピーした）特徴量にバックプロパゲーションして**新しいタスクに合わせて微調整する**か、または**転送された特徴量の層を凍結したままにしておく（つまり、新しいタスクでの訓練中は変化しない）** こともできる。ターゲットネットワークの**最初の層を微調整するかどうかの選択は、ターゲットデータセットのサイズと最初の層に含まれるパラメータの数に依存する。** ターゲットデータセットのサイズが小さく，パラメータの数が多い場合，微調整を行うとオーバーフィッティングになる可能性があり，特徴量は凍結されたままになることが多い．一方、ターゲットデータセットが大きく、パラメータ数が少ないため、オーバーフィットが問題にならない場合は、ベースとなる特徴量を新しいタスクに合わせて微調整することで、パフォーマンスを向上させることができる。 もちろん、ターゲットデータセットが非常に大きい場合は、下位レベルのフィルタをターゲットデータセット上でスクラッチから学習するだけなので、移行の必要性はほとんどありません。 以下のセクションでは、微調整された特徴量と凍結された特徴量の2つの技術のそれぞれの結果を比較する。

この論文では、いくつかの貢献をしている。

1. 我々は、特定の層が一般的か特定のものかを定量化する方法、すなわち、その層の特徴があるタスクから別のタスクにどれだけうまく転移するかを定義する（第2節）。次に、ImageNetデータセット上で畳み込みニューラルネットワークのペアを訓練し、一般的な層から特定の層への層ごとの移行を特徴づける（第4節）。

2. 本研究では、微調整を行わずに転送された特徴量を用いた場合の性能低下の原因として、以下の2つの問題があることを実験的に示した。
(i)特徴量自体の特異性。
(ii) 隣り合う層の共適応ニューロン間でベースネットワークを分割することによる最適化の難しさ。
我々は、これら2つの効果のそれぞれが、ネットワークの異なる層でどのように支配的になるかを示す。(第4.1節)

3. ベースタスクとターゲットタスクが異なればなるほど、特徴を転移することによる性能上のメリットがどのように減少するかを定量化する (第4.2節)
4. 比較的大規模なImageNetデータセットでは，訓練された重みと比較してランダムな下層重みから計算された特徴量を使用した場合，以前に報告された小規模なデータセット（Jarrettet al., 2009）よりも性能が低下することがわかった．我々は，ランダム重みと転移された重み（凍結と微調整の両方）を比較し，転移された重みの方が性能が良いことを発見した（第4.3節）．
5. 最後に、我々は、ネットワークを初期化する際に、ほとんど何層からでも特徴量を転移して初期化すると、新しいデータセットに微調整した後に、一般化性能が向上することを発見した。これは特に驚くべきことで、最初のデータセットを見たことによる効果は、大規模な微調整を行った後でも持続するからです。(第4.1節)

# 2. Gnerality vs. Specificity Measured as Transfer Performance

自然画像上で学習されたニューラルネットワークの第1層には、ガボールフィルタやカラーブロブが現れるという不思議な傾向があることを指摘してきた。この研究では、タスクAで学習した特徴のセットの一般性の程度を、その特徴が別のタスクBで使用できる程度と定義しています。これらのサブセットは、互いに類似しているか、または異なるように選択することができる。

タスクAとタスクBを作成するために、1000個のImageNetクラスをランダムに2つのグループに分割し、それぞれ500個のクラスとデータの約半分、約645,000個の例を含むグループを作成しました。これらのネットワークをベースAとベースBと呼び、図1の上2行に示します。次に、{1, 2, ...., 7}の中から層nを選択し、いくつかの新しいネットワークを訓練する。以下の説明および図１では、選択された層の例として、層ｎ＝３を用いる。まず、以下の２つのネットワークを定義し、訓練する。

* セルファーネットワークB3B：最初の3層はbaseBからコピーして凍結。 上位5層（4〜8層）はランダムに初期化され、データセットBで学習される。このネットワークは、次の転送ネットワークのコントロールとなる。(図1、3行目)
* 転送ネットワークA3B：最初の3層はベースAからコピーされ、凍結される。 直感的には、データセットAで学習されたネットワークから最初の3層をコピーし、その上に上位層の特徴を学習して新しいターゲットのデータセットBを分類する。A3BがベースBと同等の性能を発揮すれば、第3層の特徴は少なくともBに関しては一般的なものであることが証明される。

# 4. Results and Discussion

3組の実験を行った。 主な実験はランダムなA/B分割を行い、4.1節で議論する。 4.2節では、人工/自然分割を用いた実験を示す。 第4.3節では、ランダムな重みを用いた実験について述べる。

![図2](https://raw.githubusercontent.com/rurusasu/paper/master/AI%E6%8A%80%E8%A1%93/%E8%BB%A2%E7%A7%BB%E5%AD%A6%E7%BF%92/How_transferable_are_features_in_deep_neural_networks/%E7%94%BB%E5%83%8F/%E5%9B%B32.png)

本論文の主な実験結果。上：図中の各マーカーは、訓練されたネットワークの検証セットに対する平均的な精度を表している。 n=0より上の白丸は、ベースBの精度を表しています。  それぞれの紺色の点はBnBネットワークを表しています。  水色の点は、BnB+ネットワーク、またはBnBの微調整版を表しています。 濃い赤のダイヤはAnBネットワークを表し、薄い赤のダイヤは微調整されたAnB+バージョンを表しています。 視覚的にわかりやすくするために、ポイントはわずかに左右にずれています。下：各処理の手段を結ぶ線。各行の上の番号付きの記述は、4.1節のどの解釈が適用されるかを示しています。


## 4.1. Similar Datasets: Random A/B splits

ランダムに分割された（つまり類似した）データセットに対するすべてのA/B移転学習実験の結果を図2に示します。この結果から、多くの異なる結論が導き出されている。以下のそれぞれの解釈では、ベースケース（図2の白丸と点線）と性能を比較しています。

1. 白ベースのBの円は、500クラスのランダムなサブセットを分類するように訓練されたネットワークが、トップ1の精度0.625、つまり37.5%の誤差を達成していることを示しています。この誤差は、1000クラスのネットワークで達成された42.5%のトップ1エラーよりも低くなっています。ネットワークはデータの半分しか訓練されていないため、オーバーフィットが多くなる可能性があるため、誤差が高くなっているかもしれませんが、正味の結果は、クラス数が500しかないため誤差が少なく、間違いを犯す方法の数が半分しかないということです。

2. 紺色のBnBポイントは不思議な挙動を示しています。 予想通り、第1層の性能はベースとなるB点と同じである。つまり、8 層の特徴を学習し、学習した Gabor 特徴とカラーブロブの最初の層を保存し、ネットワーク全体を再初期化し、同じタスクに向けて再学習すると、同じように動作します。 この結果は第2層についても同様である。 しかし、レイヤ3、4、5、6、特に4と5では性能が低下しています。 **このパフォーマンスの低下は、元のネットワークには脆弱な共適応の特徴が連続する層に含まれていたことの証拠である。** 勾配降下法は最初に良い解を見つけることができましたが、これはレイヤーが共同で訓練されていたからこそ可能だったのです。レイヤー6までには、レイヤー7と同様にパフォーマンスはほぼベースレベルに戻っています。 最終的な500種類のソフトマックス出力のレイヤー8に近づくにつれ、再学習することは少なくなり、これらの1、2層を再学習するだけで勾配降下法は十分に簡単に良い解を見つけることができるようになります。  あるいは、第6層と第7層の間、および第7層と第8層の間の特徴の共適応は、前の層の間よりも少ないと言えるかもしれません。私たちの知る限りでは、**このような最適化の難しさは、ネットワークの中央部では、下部や上部付近よりも悪くなる可能性があるということ**は、これまでの文献では観測されていませんでした。

3. 水色のBnB+点は、コピーされた下位層の特徴量がターゲットデータセット（ここではベースデータセットと同じ）でも学習している場合、ベースデータセットと同様の性能が得られることを示しています。このように微調整を行うことで、BnBネットワークで観測された性能の低下を防ぐことができます。

4. 濃い赤色のAnBのダイヤモンドは、我々が最初に測定しようとした効果を示しています：各層でのネットワークから別のネットワークへの特徴の伝達可能性です。1層目と2層目はAからBへほぼ完璧に転送され、少なくともこの2つのタスクでは、1層目のGaborとカラーブロブの特徴が一般的であるだけでなく、2層目の特徴も一般的であるという証拠を与えています。 第3層ではわずかに低下し、第4層から第7層ではより顕著な性能低下を示しています。 BnB ポイントのおかげで、この低下は 2つの別々の効果の組み合わせによるものであることがわかります：**失われた共適応による低下**と、**一般的な特徴が少なくなったことによる低下**です。3、4、5 層では、最初の効果が支配的ですが、6、7 層では最初の効果が減少し、表現の特異性がパフォーマンスの低下を支配しています。
特徴の伝達が成功した例は文献でも報告されているが(Girshicket al., 2013; Donahueet al., 2013b)、我々の知る限り、これらの結果は、与えられたレイヤーからの伝達が、厳密にターゲットタスクで訓練する代替案よりもはるかに優れていることに気づくこと、すなわち、あるレイヤーのAnBポイントがゼロからすべてのレイヤーを訓練するよりもはるかに優れていることに気づくことに限定されていた。 私たちは、(1)どの程度の転送が成功しているかをレイヤーごとに注意深く定量化し、(2)これら2つの別々の効果が分離され、それぞれの効果がレジームの一部で支配的であることを示したのは、これが初めてであると考えています。

5. 薄赤色のAnB+のダイアモンドは特に驚くべき効果を示している。これまでは、学習した特徴量を転送する理由は、小さなターゲットデータセットでオーバーフィットせずに学習できるようにするためでしたが、今回の新しい結果は、**ターゲットデータセットが大きくても、特徴量を転送することで汎化性能が向上することを示唆しています。** この効果は、総訓練時間が長くなったことに起因するものではない（AnB+では基本反復回数が450k回＋微調整反復回数が450k回 vs. 基本Bでは450k回）。このように、450k回の微調整（完全にランダムなトップ層から始める）を繰り返した後でも、**基本データセットを見た効果が残っていて、一般化性能が向上している**というのが、もっともらしい説明です。 これだけ多くの再訓練を行ってもこの効果が持続するのは驚くべきことです。この一般化性能の向上は、第二のネットワークを初期化するために第一のネットワークをどれだけ維持しているかにはあまり依存していないようです。 レイヤ 1 から 7 までの平均ブーストはベースケースと比較して 1.6%で、少なくとも 5 つのレイヤを維持した場合の平均は 2.1%です。性能向上の度合いを表1に示します。

![表1](https://raw.githubusercontent.com/rurusasu/paper/master/AI%E6%8A%80%E8%A1%93/%E8%BB%A2%E7%A7%BB%E5%AD%A6%E7%BF%92/How_transferable_are_features_in_deep_neural_networks/%E7%94%BB%E5%83%8F/%E8%A1%A81.png)

表1 AnB+のパフォーマンスをコントロールよりも向上させ、さまざまなレイヤーの範囲で平均化しました。




# 5. Conclusions

我々は、ニューラルネットワークの各層の特徴の伝達性を定量化する方法を実証した。すなわち、脆弱な共適応層の途中でネットワークを分割することによる最適化の難しさと、目標タスクの性能を犠牲にして高次の層の特徴を元のタスクに特化させることである。我々は、ネットワークの下層、中層、上層のいずれから特徴を転送するかによって、これら2つの問題のいずれかが支配的になることを観察した。また、タスク間の距離が長くなるにつれて、特に上位層の特徴量を転送する場合に、転送可能性のギャップがどのように大きくなるかを定量化したが、遠くのタスクから転送された特徴量であっても、ランダム重みよりも優れていることがわかった。最後に、転送された特徴で初期化を行うと、新しいタスクで大幅な微調整を行った後でも一般化性能を向上させることができることを発見した。


# サポート

# A: Training Details

Krizhevskyら(2012)がImageNet 2012で優勝して以来，当然ながら大規模な畳み込みモデルのハイパーパラメータを微調整することに多くの関心と作業が寄せられてきました． 例えば，Zeiler and Fergus(2013)は，第1層のフィルタサイズを11×11から7×7に小さくし，ストライドを4ではなく2にした方が良いことを発見した． しかし，本研究は絶対性能の最大化ではなく，一般的に研究されているアーキテクチャを使用することを目的としているため，Caffe(Jiaet al., 2014)が提供している参照実装を使用した．本研究では、Donahueら(2013)に倣い、Krizhevskyら(2012)からいくつかのマイナーな変更を行った。 元の論文では1%しか改善されていなかった、ピクセルRGB値の主成分のランダムな倍数を追加するというデータ増強のトリックをスキップし、アスペクト比を維持するためにスケーリングしてからトリミングする代わりに、画像を256×256にワープしました。また、局所応答正規化レイヤーをプーリングレイヤーの前ではなく、プーリングレイヤーの直後に配置しました。Krizhevskyら(2012)を含む先行研究と同様に、softmax出力レイヤー以外の完全に接続されたレイヤーにドロップアウト(Hintonら(2012))を使用した。



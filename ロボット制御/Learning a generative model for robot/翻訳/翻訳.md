# Learning a generative model for robotcontrol using visual feedback

# 備考

## 著者
Nishad Gothoskar, Miguel Lazaro-Gredilla, Abhishek Agarwal, Yasemin Bekiroglu, Dileep George


# Abstract

ロボットの制御に視覚フィードバックを組み込むための新しい定式化を紹介する。本研究では、エンドエフェクタ上の特徴量の画像観測から行動を生成するモデルを定義し、その中で推論を行うことで、特徴量の目標位置に対応するロボットの状態を推定する。このモデルの推論により、特徴量の目標位置に対応するロボットの状態を推論することができる。これにより、ロボットの動きを誘導し、最先端のビジュアルサーボ法に比べて大幅に少ないステップで特徴の目標位置を一致させることが可能になります。我々のモデルの訓練手順は、運動学、特徴構造、カメラパラメータを同時に効率的に学習することを可能にします。これは、ロボット、構造、およびそれを観測するカメラに関する事前情報を持たずに行うことができます。学習はサンプル効率よく行われ、テストデータへの強力な一般化を示す。我々の定式化はモジュール化されているので、カメラやオブジェクトのようなセットアップのコンポーネントを変更して、オンラインで素早く再学習することができます。我々の手法は、観測された状態のノイズと、我々が対話するコントローラのノイズを扱うことができる。不正確なコントローラを持つロボットに対して、把持やぴったりとした挿入を実行することで、本手法の有効性を実証している。

# Introduction

ロボットの開発の主な目的は，ロボットがタスクを計画して実行できるようにすることです． 多くの場合，ロボットに実行してもらいたいタスクは，把持したり，挿入したりするなどして，物体と対話することになります．インテリジェントなロボットシステムは、様々な感覚モダリティからの情報を統合することで、ノイズや不確実性のある実世界の環境でも柔軟に動作できるようにしなければなりません。人間の感覚運動ループは、様々な感覚源からの入力を受け取り、運動命令を生成します。 このループの重要な感覚源は視覚であり、周囲の環境に関する豊富な情報を提供します。視覚入力は、人間が周囲の世界と相互作用するために重要であり、脳の多くの領域にわたって視覚運動のカップリングが行われていることが実験的に証明されています（Attingerら2017）。この結合により、視覚的特徴に移動して追跡し、手足やツールを使用してオブジェクトを操作できます。

ロボット工学では，視覚的なフィードバックを利用してロボットを制御する技術をビジュアルサーボと呼んでいます．ビジュアルサーボには大きく分けて2つの方法があります。 画像ベースのビジュアルサーボ（IBVS）と位置ベースのビジュアルサーボ（PBVS）です。IBVSは視覚的特徴量を抽出し、制御則により画像内の現在の特徴量位置と所望の特徴量位置との誤差を最小化します。PBVS は、抽出された視覚特徴を用いて観察対象物のポーズを推論し、制御則は現在のポーズと目的のポーズの間の誤差を最小化します。一般に、これらの方法の目的は、画像から抽出された情報を用いて、追跡された物体の所望の位置を達成することである。

ビジュアルサーボの標準的な方法では、ロボットの運動学とカメラのキャリブレーションが必要です。  キネマティックキャリブレーションでは、エンドエフェクタの姿勢の正確な測定値を提供するために、精密に製造された装置やレーザーベースのデプスセンサが必要になることがよくあります。カメラのキャリブレーションでは、多くの場合、カメラから見えるさまざまな場所に市松模様を配置して、内部パラメータと外部パラメータをキャリブレーションするためのルーチンが必要になります。 これらの手順は、ロボットの構造やカメラの位置が変更されるたびに繰り返す必要があります。

これは、人間が制御できる柔軟性とは対照的です。例えば、視覚的なサーボリングアルゴリズムの代わりに、人間にジョイスティックを操作させてロボットの関節を制御させるとします。 ジョイスティックに反応してロボットがどのように動くかを観察することで、人間はすぐにロボットを制御できるようになります。ジョイスティックは、視点の変化やロボットの構造の変更にもロバストであると考えられます。この能力をアルゴリズムで再現するためには、知覚と制御の問題を同時に考慮する必要があり、変化に応じて視覚モデルや制御方針を適応させることができます。

本論文では，視覚フィードバックからロボットを制御するための学習手法を提案する．本研究では，画像中のピクセル座標を特徴づけるためのアクションから生成モデルを学習する． このモデルは、ロボット、特徴量、カメラなどの事前情報がなくても学習できる。 このモデルの推論により、これらの視覚的特徴を用いてロボットのエンドエフェクタを正確に位置決めすることができます。これにより、明示的な運動学やカメラのキャリブレーション手順が不要になります。さらに、不正確なキャリブレーションによって生じる誤差の影響を受けないようにすることができます。 少ないサンプル数でこのモデルのパラメータを学習し、オンラインで学習を続けることができるため、システムを迅速に展開でき、セットアップの変更にもロバストです。


# Related work

イメージベースのビジュアルサーボングのほとんどの方法は、イメージのヤコビアンを推定しようとします。画像のヤコビアンは、ロボットの状態（関節状態や直交ポーズ）の変化が画像内の特徴のピクセル座標にどのように変化をもたらすかを規定しています。そして、このヤコビアンと現在および希望する特徴のピクセル座標が与えられると、ピクセル空間の誤差をロボットの状態空間の誤差に逆投影することができます。 この状態空間の誤差は、ロボットの状態を修正するためのコマンドを導出するために使用されます。Kragic and Christensen (2002)は、操作タスクのコンテキストでのビジュアルサーボのための様々な方法の詳細な調査を提供しています。

一般的に使用されているソフトウェアパッケージ VISP (Marchand et al. 2005) は、目から手へのセットアップを含む、視覚サーボのための様々な方法を実装しています。彼らが使用している制御則は以下の通りです。

$$
\dot{\bm{q}} = \gamma\left(\hat{\bm{L_e}}^c\bm{V_e} ^\bm{e}\bm{J_e}(\bm{q}) \right)^+ \bm{e}
$$

$^\bm{e}\bm{J_e}(\bm{q})$ はエンドエフェクタフレームで表現されるロボットのヤコビアンであり，関節状態$q$の関数である．$^c\bm{V_e}$ はエンドエフェクタフレームとカメラフレーム間の変換を定義します。$\hat{\bm{L_e}}$は、特徴量の3D位置の変化がピクセル位置の変化とどのように関係しているかをキャプチャする相互作用行列です。現在の座標と希望する座標の間の誤差は$e$であり、座標にはピクセル座標$x$と$y$の両方とカメラ原点$z$からの距離が含まれます。この制御則を用いて、関節に送る速度指令である$\dot{q}$を計算します。$\gamma$は、この指令のステップサイズを制御し、誤差の指数関数的収束を可能にするゲイン項です。

$^\bm{e}\bm{J_e}(\bm{q})$を求めるためには、ロボット運動モデルと関節状態へのアクセスが必要である。$^c\bm{V_e}$はカメラの外部キャリブレーションの出力です。相互作用行列$\hat{\bm{L_e}}$はオンラインで近似されています。我々のモデルは構造的にはVISPと似ていますが、VISPでは上記の値を提供する必要がありますが、我々の手法ではデータから学習することができます。

## 独立したカメラとキネマティクスのキャリブレーション

これまでの研究では、キネマティクスとカメラのパラメータをキャリブレーションする方法が提案されています。
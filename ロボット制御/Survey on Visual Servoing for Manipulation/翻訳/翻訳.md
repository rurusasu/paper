# Survey on Visual Servoing for Manipulation

# 備考

## 掲載
 Technical report, ISRN KTH/NA/P-02/01-SE, Computational Vision and Active Perception Laboratory, Royal Institute of Technology, Stockholm, Sweden .

# Abstract

視覚誘導ロボティクスは、30年以上前から主要な研究課題の一つとなっています。最近の技術開発により、この分野の発展が促進され、既製のハードウェアを使用した多くのシステムが成功し、商業的なシステムにまで発展してきました。視覚誘導システムの用途は、インテリジェントホームから自動車産業まで多岐にわたっています。しかし、この分野では、経験や研究アイデアの交換が必要であることが、オープンで一般的に言われている問題の1つです。そのための良い出発点は、成功事例を宣伝し、調査論文の形で共通の用語を提案することだと私たちは考えています。この論文では、画像ベース、位置ベース、2 1/2D ビジュアルサーボの異なるタイプのビジュアルサーボに焦点を当てています。ハードウェアとソフトウェアの両方の要件に関するさまざまな問題を検討し、最も顕著な貢献をレビューします。提案された用語は、若い研究者を紹介するために使用され、ビジョンガイドロボットの30年の歴史的な分野を通して、この分野の専門家を導くために使用されています。また、概念的な枠組みを提供するだけでなく、本論文で取り上げられている問題のほとんどを説明するために、私たち自身の研究から多くの実世界の例を紹介しています。

# Introduction

ロボットを制御するために視覚的なフィードバックを使用することは、一般的にビジュアルサーボと呼ばれています(Hutchinson et al. 1996)。点、線、領域などの視覚的な（画像ベースの）特徴を使用して、例えば、マニピュレータ/グリッピング機構のオブジェクトへのアラインメントを可能にします。したがって、視覚は環境の状態をフィードバックする制御システムの一部となります。ビジュアルサーボは、単純なピックアンドプレース作業から今日のリアルタイムで高度なオブジェクトの操作に至るまで、30年以上にわたって様々な形で研究されてきました。マニピュレーションという点では、制御ループにビジョンを組み込む主な動機の一つは、ロボットシステムの柔軟性を高めることが求められていました。

この地域のオープンで共通した問題点として、経験や研究アイデアの交換が必要であることが挙げられます。

図1:左) オープンループのロボット制御。初期化は制御シーケンス（ロボットの動作シーケンス）を直接生成するための特徴量の抽出であり、ロボットと環境とのオンラインインタラクションは存在しない。初期化( 視覚サーボシーケンスを初期化 )、トラッキング( ロボット制御に使用する特徴量の位置を連続的に追跡 ロボット/オブジェクトの動作中に更新される )、ロボット制御( 感覚的に入力されると、制御シーケンスが生成されます)。

ロボットシステムのクローズドループ制御では、基礎となるセンサとしてビジョンが使用されますが、通常はトラッキングと制御の 2 つのプロセスが連動したもので構成されます。トラッキングでは、ロボット/オブジェクトの運動中に特徴量の連続的な推定と更新を行います。この感覚入力に基づいて、制御シーケンスが生成されます。さらに、システムはまた、一般的に図-地面セグメンテーションおよび物体認識を含む自動初期化を必要とする場合がある。

前述したように、ビジュアルサーボは 30 年以上前から研究されてきました。最近では、計算機資源の増大によりリアルタイムでの展開が可能になったことから、この分野に大きな注目が集まっています。同時に、実世界のシナリオに対応したロバストな手法により、複雑さの点で現実的な問題にも徐々に対応できるようになってきています。ビジュアルサーボへの貢献が増えてきたことで明らかになってきた問題点は、そのアプロー チの用語や分類法が不足していることである。ビジュアルサーボは、自動車の車線追跡、モバイルプラットフォームのナビゲーション、オブジェクトの操作など、多種多様なアプリケーションで使用されています。これらのアプリケーションの中で最も一般的なのは物体の操作であり、物体の検出、認識、認識、サーボ、アライメント、把持を必要とします。多くのアプローチは、これらのすべてに対応しているわけではありませんが
これらの側面を考慮するためのグローバルなフレームワークを提供します。サーボに関する多様な研究の成果を紹介します。

文献には、チュートリアルの形でビジュアルサーボの優れた紹介があります(Hutchinson et al. 1996)。しかし、このチュートリアルは5年前のものであり、それ以降、重要な作業が報告されています。また、膨大な文献の調査が不足しています。そのため、本論文では、文献の包括的な調査を行います。その基礎として、ビジュアルサーボへのアプローチの分類法を定義し、ビジュアルサーボに影響を与える主要な要因を明らかにしました。ビジュアルサーボへの様々なアプローチは、主に私たち自身の研究から得た例で説明されています。調査の残りの部分は、第 3 章でビジュアルサーボの紹介から始まります。この後、セクション 4 でサーボに関わる主要なステップを紹介します。i) 第 5 節では視覚モータ推定へのアプローチ、ii) 第 6 節では特徴/状態推定への戦略、iii) 第 7 節では制御生成のための方法を説明します。これら 3 つのステップは、ビジュアルサーボのための基本的なタクソノミを提供します。最後に文献をレビューし、提案されたタクソノミ８との関連で、反発されているアプローチを分類し、議論します。 最後のセクションでは、ビジュアルサーボの分野における未解決の問題とトレンドについて議論します。

# 5. Visual-Motor Model Estimation

視覚運動モデルの推定に基づいてシステムを分類するために，図4と図5に示すような分類法を採用しています．ロボットの前進運動学や逆運動学がわかっている場合は，ロボットのジャコビアンを用いて関節と直交空間の微分変化を計算し，ロボットの運動モデルが事前にわかっているシステムを分類します．これらのシステムは、視覚運動モデルが事前に分かっているシステムに分類される。カメラとロボットの間のフィードバック表現モードとキャリブレーションのレベルに応じて フレームの中では、ビジュアルサーボシステムは、位置ベース、画像ベース、2 1/2 Dシステム。

初期のビジュアルサーボシステムのほとんどは、システムの正確なキャリブレーションに依存しており、位置に基づいたアプローチを使用してタスクを実行しました。キャリブレーションのプロセスは面倒でエラーが発生しやすく、実行できない場合もあったため、キャリブレーションのステップを回避するか、キャリブレーションの知識があれば十分なアプローチが魅力的になってきました。そのため、位置ベースのシステムよりも画像ベースのサーボシステムの方が、正確なキャリブレーションを行わずに作業を行うことができるため、通常は画像ベースのサーボシステムの方が好まれています。しかし，センサとロボットフレーム間の変換に関する知識が必要となります．

一方、キャリブレーションステップを完全に省略し、視覚運動モデルをオンラインまたはオフラインで推定するシステムもあります。視覚運動モデルは、a) 解析的（非線形最小二乗最適化）に推定されてもよいし、b) 学習または訓練によって推定されてもよい。さらに、図５に示されるように、システムは、画像ヤコビアンを推定し、既知のロボットモデルを使用してもよいし、結合されたロボット画像ヤコビアンを推定してもよい。

# 5.1. アプリオリ既知モデル (校正済みモデル)

すでに述べたように、我々は、フィードバック表現モードに基づいて視覚サーボのアプローチを分類しています。これらは、i) 位置ベース、ii) 画像ベース、iii) 2 1/2 D ビジュアルサーボシステムである。ここでは、基本的な考え方を示し、それぞれの特徴を説明する。

## 5.1.1. Position based control (ポジションベースの制御)

位置ベースのビジュアルサーボは、カメラや共通のワールドフレームに対するターゲットのポーズを決定するために画像測定が使用されるため、通常は3Dサーボ制御と呼ばれています。目標の現在のポーズと希望のポーズの間の誤差は、ロボットのタスク（直交）空間で定義されます。したがって、誤差はポーズパラメータ $e(\bm{X})$ の関数となります。

位置ベースのサーボの２つの例を図６に示す。左側の図は、カメラを現在のポーズ $^CX_O$ から制御して、対象物に対して所望のポーズ $^C{X^{\ast}}_O$ になるようにする例を示しています。この例では、カメラはマニピュレータの最後のリンクに取り付けられており、静的または移動するターゲットを観測し、オブジェクトのモデルを使用してそのポーズを推定しています。右図は、静止したカメラと移動する対象物の例です。ここでは、対象物をマニピュレータで保持し、マニピュレータを制御して、対象物とカメラとの間で所望のポーズが得られるようにしていると仮定しています。対象物のポーズはカメラに対して相対的に推定されているため、マニピュレータの動きを生成するためには、ロボットとカメラの間の変換を知っておく必要があります。

これらの例は、位置ベースのビジュアルサーボングが通常サーボ作業に採用されない２つの主な理由を示している：
(ｉ) 目標のポーズの推定を必要とするか、または何らかの形態のモデルを必要とする、(ii) ロボットの所望の速度スクリューを推定し、正確な位置決めを達成するために、正確なシステム較正（カメラ、カメラ／ロボット）を必要とする。
図７は、位置ベースのビジュアルサーボングアプローチのブロック図である。ここでは、所望のポーズと現在のポーズの間のポーズの差が誤差を表しており、この誤差を最小化するために、ロボットの速度ネジを推定するために使用されます ($\dot{\bm{q}} = [V \ \Omega]^T$)。

## 例。タスクの整列と追跡

まず、対象物とロボットのエンドエフェクタである $^O{X^{\ast}}_G$ との間で一定の姿勢を達成し、維持することが課題であるとします。(Hutchinson et al. 1996)によれば、サーボシーケンス中は対象物のみを観測するため、これはEOL(Endpoint Open Loop)システムと考えられています。

マニピュレータは、エンドエフェクタフレーム内で制御される。図 8 によれば、$^OX_G = ^O{X^{\ast}}_G$ とすると、$^RX_G = ^R{X^{\ast}}_G$ となります。次に、最小化されるべき誤差関数は、現在のエンドエフェクタポーズと所望のエンドエフェクタポーズとの差として定義されてもよい。

$$
\Delta ^R\bm{t}_G = ^R\bm{t}_G - ^R\bm{t}^{\ast}_G \\

\Delta ^R\bm{\theta}_G = ^R\bm{\theta}_G - ^R\bm{\theta}^{\ast}_G
$$

ここで、$R^t_G$とRqGは前方運動方程式から既知であり、$R^t_G$と$R^q_G$を推定する必要がある。ロボットと所望のエンドエフェクターフレームとの間の同質変換は次式で与えられる。

$$
^R\bm{X}^{\ast}_G = ^R\bm{X}_C \ ^C\bm{X}_O \ ^O\bm{X}^{\ast}_G
$$

カメラとロボットの間のポーズはオフライン( カメラとロボットの座標フレームに関連した同質変換をオフラインで行った．マニピュレータチェーンの末端に LED を配置し，マニピュレータがあらかじめ設定されたいくつかの点を移動しながら画像上の位置を推定した．カメラの固有パラメータを知っていると仮定して，ロボットとカメラの間の変換を推定するために，(Kragic 2001)で紹介されているポーズ推定アプローチを使用した． )で推定され、カメラフレームに対する物体の相対的なポーズは、(Kragic 2001)で提示されたモデルベースのトラッキングシステムを使用して推定されます。(Eq. 3)の変換を展開すると、次のようになります。

$$
^R\bm{t}^{\ast}_G = ^R\bm{R}_C \ ^C\hat{\bm{R}}_O \ ^O\bm{t}^{\ast}_G + ^R\bm{R}_C \ ^C\hat{\bm{t}}_O + ^R\bm{t}_C
$$

ここで$^C\hat{\bm{R}}_O$と$^C\hat{\bm{t}}_O$はトラッキングアルゴリズムから得られる予測値を表します。同様の表現は、角速度を加えることで、回転の変化に対しても得ることができます（図8参照）。

$$
^R\bm{\Omega}^{\ast}_G = ^R\bm{\Omega}_C + ^C\hat{\bm{R}}_C \ ^C\hat{\bm{\Omega}}_O + ^R\bm{R}_C \ ^C\hat{\bm{R}}_O \ ^O\bm{\Omega}^{\ast}_C
$$

$R^\bm{R}_C$と$^C\hat{\bm{R}}_O$がゆっくりと変化する時間関数であると仮定すると、RW Gの積分は(Wilson et al. 1996)を与えます。

$$
^R\bm{\theta}^{\ast}_G \approx ^R\theta_C + ^R\bm{R}_C \ ^C\hat{\bm{\theta}}_O + ^R\hat{\bm{R}}_C \ ^C\hat{\bm{R}}_O \ ^O\theta^{\ast}_G
$$

(式4)と(式6)を(式2)に代入すると、(式2)が得られます。

$$
\Delta ^R\bm{t}_G = ^R\bm{t}_G-^R\bm{t}_C-^R\bm{R}_C \ ^C\hat{\bm{t}}_O - ^R\bm{R}_C \ ^C\hat{\bm{R}}_O \ ^O \bm{t}^{\ast}_G
$$

$$
\Delta ^R\bm{\theta}_G \approx ^R\bm{\theta}_G-^R\bm{\theta}_C-^R\bm{R}_C \ ^C\hat{\bm{\theta}}_O - ^R\bm{R}_C \ ^C\hat{\bm{R}}_O \ ^O \bm{\theta}^{\ast}_G
$$

これは最小化される誤差を表します。

$$
\bm{e} = \left[
    \begin{array}{c}
        \Delta^R\bm{t}_G\\
        \Delta^R\bm{\theta}_G
    \end{array}
    \right]
$$

誤差関数が定義された後、単純な比例制御則を使用して誤差をゼロに駆動します。ロボットの速度ネジは4のように定義されています。

$$
\dot{\bm q} \approx \bm{K}e
$$

物体のポーズの推定値を用いて、ポーズによる誤差関数を定義することで、ロボットの6つの自由度すべてを制御します。

実験シーケンスの1つの間に得られたいくつかの例の画像を図9に示す。任意の開始位置から、エンドエフェクタは、対象物に対して予め定義された定置ポーズに移動される（最初の行、左）。対象物が動き始めると、視覚システムはそのポーズ（CXO）を推定します。誤差は、（式８）に従って推定され、（式９）を使用してロボットの速度スクリューを刺激するために使用される。

一般的に、このアプローチの主な利点は、カメラ／ロボットの軌跡が直交座標で直接制御されることである。これにより、例えば、障害物回避のためのより簡単な軌道計画が可能になる。しかし、特にアイインハンドカメラ構成の場合には、ポーズ推定に用いられる画像特徴量が画像から外れることがある。その理由は、制御則には、画像平面の特徴座標に関する制約が組み込まれていないからである。カメラが粗く較正されている（すなわち、カメラパラメータがほぼ既知である）だけであれば、現在および所望のカメラポーズが正確に推定されず、その結果、（精度の点で）パフォーマンスが低下するか、あるいは視覚サーボタスクが完全に失敗することになる。この問題の解決策の一つは、タスクの実行中にターゲットとエンドエフェクタの両方が観測される終点閉ループシステムとしてサーボシステムを設計することである(Hutchinson et al. 1996)


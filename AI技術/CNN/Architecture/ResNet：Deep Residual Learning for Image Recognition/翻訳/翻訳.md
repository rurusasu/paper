# Deep Residual Learning for Image Recognition

# 概要

より深いニューラルネットワークの学習はより困難です。本研究では、従来よりも大幅に深いネットワークの学習を容易にする残差学習フレームワークを提案する。我々は、参照されない関数を学習するのではなく、層の入力を参照して残差関数を学習するように、層を明示的に再構成する。このような残差ネットワークは最適化が容易であり、深さを大幅に増やしても精度を向上させることができることを示す包括的な実証的証拠を提供する。ImageNetデータセットにおいて，VGGネット[40]よりも8倍深い152層までの残差ネットを評価したが，複雑さは依然として低い．これらの残差ネットのアンサンブルは，ImageNetテストセットで3.57%のエラーを達成した．この結果は，ILSVRC 2015の分類タスクで1位を獲得した．また、100層と1000層のCIFAR-10での解析結果も紹介します。

表現の深さは、多くの視覚認識タスクにおいて中心的な重要性を持っています。我々の非常に深い表現により、COCOオブジェクト検出データセットで28%の相対的な改善を得ました。深層残差ネットは、ILSVRCとCOCO2015のコンペに提出したもので、ImageNet検出、ImageNetローカライズ、COCO検出、COCOセグメンテーションのタスクで1位を獲得しました。

# 1. 緒言

深層畳み込みニューラルネットワーク[22, 21]は，画像分類において一連のブレークスルーをもたらした[21, 49, 39]．深層ネットワークは，低・中・高レベルの特徴量[49]と分類器をエンド・ツー・エンドの多層構造で自然に統合し，特徴量の「レベル」は積層数（深さ）によって豊かにすることができます．ネットワークの深さが非常に重要であることを示す最近の証拠[40, 43]があり、挑戦的なImageNetデータセット[35]における主要な結果[40, 43, 12, 16]はすべて、深さが16[40]から30[16]の「非常に深い」[40]モデルを利用しています。他の多くの非自明な視覚認識タスク[7, 11, 6, 32, 27]も、非常に深いモデルから大きな恩恵を受けている。

深さの重要性に迫られて、ある疑問が生まれました。より良いネットワークを学習することは，より多くの層を積み重ねることと同じくらい簡単なのだろうか？この問題を解決するための障害は，グラデーションの消失や爆発の問題 [14, 1, 8] であり，これが最初から収束の妨げになっていた．しかし，この問題は，正規化された初期化[23, 8, 36, 12]や中間正規化層[16]によってほぼ解決され，数十層のネットワークでもバックプロパゲーションを用いた確率的勾配降下法(SGD)の収束を開始できるようになりました[22]．

より深いネットワークが収束し始めると、劣化の問題が露呈します。ネットワークの深さが増すにつれて、精度が飽和し（当然かもしれませんが）、その後急速に劣化していきます。意外なことに，このような劣化はオーバーフィッティングによるものではなく，適切な深さのモデルに層を追加すると学習誤差が大きくなることが[10, 41]で報告されており，我々の実験でも十分に検証されている．図1はその典型的な例です。

学習精度の）低下は、すべてのシステムが同様に最適化しやすいわけではないことを示しています。より浅いアーキテクチャと、その上にさらにレイヤーを追加したより深いモデルを考えてみましょう。より深いモデルには、構築による解が存在します。追加された層はIDマッピングであり、その他の層は学習された浅いモデルからコピーされます。このような解法が存在するということは、深いモデルは浅いモデルよりも学習誤差が大きくならないはずです。しかし、実験によると、現在手元にあるソルバーでは、構築された解と同等以上の解を見つけることができません（または、実現可能な時間内に見つけることができません）。

本論文では、深層残差学習フレームワークを導入することで、この劣化問題に対処します。このフレームワークでは、いくつかの非線形層を重ねることで、目的のマッピングに直接適合させるのではなく、これらの非線形層に残差マッピングを適合させます。形式的には、望ましい基本的なマッピングを $H(x)$ とすると、積層された非線形層に $F(x) := H(x) - x$ の別のマッピングを適合させ、元のマッピングを $F(x)+x$ に再構成する。我々は、参照されていないオリジナルのマッピングを最適化するよりも、残余のマッピングを最適化する方が簡単であるという仮説を立てた。極端に言えば、もしアイデンティティーマッピングが最適であれば、非線形層の積み重ねによってアイデンティティーマッピングをフィットさせるよりも、残差をゼロにする方が簡単であろう。

$F(x)＋x$ の定式化は，「ショートカット接続」を持つフィードフォワード・ニューラル・ネットワークで実現できる（図2）．ショートカット接続 [2, 33, 48] は，1つ以上の層をスキップする接続です．今回の例では，ショートカット接続は単に同一性マッピングを行い，その出力は積層された層の出力に追加されます（図2）．アイデンティティ・ショートカット接続は，パラメータの追加や計算量の増加を伴わない．このネットワーク全体は、バックプロパゲーションを用いたSGDによってエンドツーエンドで学習することができ、ソルバーを変更することなく、一般的なライブラリ（Caffe [19]など）を用いて簡単に実装することができます。

ImageNet [35]を用いた包括的な実験を行い、劣化問題を示し、我々の手法を評価します。その結果、以下のことがわかりました。
1. 我々が開発した非常に深い残差ネットは，最適化が容易であるが，対応する「プレーン」ネット（単に層を重ねるだけのネット）は，深さが増すと学習誤差が大きくなる．
1. 我々の深い残差ネットは、深さを大幅に増やすことで容易に精度を向上させることができ、以前のネットワークよりも大幅に良い結果を得ることができる。

同様の現象は，CIFAR-10セット[20]でも示されており，最適化の難しさと我々の手法の効果が，特定のデータセットだけではないことを示唆している．我々は、このデータセットで100以上の層を持つモデルの学習に成功し、1000以上の層を持つモデルを探求している。

ImageNet分類データセット[35]では，非常に深い残差ネットによって優れた結果が得られた．我々の152層の残差ネットは，VGGネット[40]よりも複雑さが低いにもかかわらず，ImageNetでこれまでに発表された中で最も深いネットワークです．我々のアンサンブルは、ImageNetテストセットで3.57%のトップ5エラーを示し、ILSVRC 2015の分類コンペティションで1位を獲得しました。極めて深い表現は、他の認識タスクにおいても優れた汎化性能を発揮し、さらに1位を獲得するに至った。ILSVRC & COCO 2015のコンペティションでは、ImageNet detection、ImageNet localization、COCO detection、COCO segmentationで1位を獲得しました。このように、残差学習原理には汎用性があり、他の視覚問題や非視覚問題にも適用できることが期待されます。

# 2. 関連研究

**残差表現**: 画像認識において、VLADは辞書に対する残差ベクトルによって符号化する表現であり、フィッシャーベクトル[30]はVLADの確率的バージョン[18]として定式化できる。どちらも画像検索や分類のための強力な浅い表現である[4, 47]。ベクトル量子化においては，残差ベクトルの符号化[17]が元のベクトルの符号化よりも有効であることが示されている．

ローレベルビジョンやコンピュータグラフィックスでは，偏微分方程式（PDE）を解くために，広く使われているマルチグリッド法[3]が，システムを複数のスケールの部分問題として再構成し，各部分問題が粗いスケールと細かいスケールの間の残差解を担当する．マルチグリッド法に代わる手法として，2つのスケール間の残差ベクトルを表す変数に依存する階層的基底の事前条件付け[44, 45]があります．これらのソルバーは，解の残差を意識しない標準的なソルバーよりもはるかに速く収束することが示されています[3, 44, 45]．これらの手法は，優れた再定式化や前提条件の設定により，最適化を簡略化できることを示唆している．

**ショートカット接続**: ショートカット接続[2, 33, 48]につながる実践と理論は、長い間研究されてきました。多層パーセプトロン（MLP）を学習する初期のプラクティスは，ネットワークの入力から出力に接続された線形層を追加することである[33, 48]．[43, 24]では，いくつかの中間層を補助的な分類器に直接接続して，バニシング／エクスポージング・グラジェントに対処している．[38, 37, 31, 46]の論文では，層の応答，勾配，伝播した誤差をセンタリングする方法を提案しており，ショートカット接続によって実装されている．[43]では、「インセプション」層は、ショートカットブランチといくつかの深いブランチで構成される。

我々の研究と同時に、「ハイウェイ・ネットワーク」[41, 42]は、ゲート機能[15]を持つショートカット接続を提示している。これらのゲートはデータに依存し，パラメータを持っているが，我々のアイデンティティ・ショートカットはパラメータを持たない．ゲート付きのショートカットが「閉じている」（ゼロに近づいている）場合、ハイウェイ・ネットワークのレイヤーは非永続的な機能を表している。これに対して、我々の定式化は常に残余機能を学習する。我々のアイデンティティ・ショートカットは決して閉じられることはなく、すべての情報は常に通過し、学習すべき残余機能が追加される。また、high-wayネットワークは、極端に深さを増しても（例えば100層以上）、精度の向上は見られない。

# 3. 深層残差学習

## 3.1. 残差学習

ここでは、$H(x)$ を、いくつかの層（ネット全体でなくてもよい）でフィットさせるための基礎的なマッピングと考え、xをこれらの層の最初の層への入力とします。複数の非線形層が複雑な関数を漸近的に近似できると仮定すると、残差関数、すなわち $H(x)-x$ を漸近的に近似できると仮定するのと同じことになります（入力と出力が同じ次元であると仮定します）。そのため、積み重ねられた層が $H(x)$ を近似することを期待するのではなく、これらの層が残差関数 $F(x):= H(x) - x$ を近似することを明示します。どちらの形式でも（仮説通り）目的の関数を漸近的に近似できるはずですが、学習のしやすさは異なるかもしれません。

この再定式化は、劣化問題（図1左）に関する直観に反する現象が動機となっています。冒頭で述べたように、もし追加された層が同一性のマッピングとして構成されるならば、深いモデルは浅いモデルよりも学習誤差が大きくならないはずです。劣化の問題は、ソルバーが複数の非線形層で同一のマッピングを近似することが困難であることを示唆しています。残差学習の再定式化により、同一性マッピングが最適であれば、ソルバーは複数の非線形層の重みをゼロに近づけるだけで、同一性マッピングに近づけることができる。

実際のケースでは、同一性マッピングが最適である可能性は低いが、我々の再定式化は問題の前提条件を整えるのに役立つだろう。最適関数がゼロマッピングよりもアイデンティティマッピングに近い場合、ソルバーにとっては、関数を新たに学習するよりも、アイデンティティマッピングを参照して摂動を見つける方が簡単なはずである。我々は実験により（図7）、学習された残差関数は一般に小さな応答を持つことを示し、同一性マッピングが合理的な前提条件を提供することを示唆している。

## 3.2. ショートカットによるアイデンティティマッピング

また、数枚のスタック層ごとに残差学習を採用しています。図2にビルディング・ブロックを示す。形式的には、本稿では次のように定義されるビルディング・ブロックを考える。

---

<img src=>

---
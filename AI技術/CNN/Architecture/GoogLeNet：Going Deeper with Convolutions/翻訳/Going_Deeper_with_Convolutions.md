# Going Deeper with Convolutions

# 備考
# 著者

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich

# 掲載

"Going Deeper with Convolutions", Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)，pp.1--9, 2015．

# Abstract
我々は、ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14)において、分類と検出で新しい状態を達成した、コードネームInceptionと呼ばれる深層畳み込みニューラルネットワークアーキテクチャを提案する。このアーキテクチャの最大の特徴は、ネットワーク内のコンピューティングリソースの利用率を向上させたことです。慎重に作られた設計により、計算機予算を一定に保ちながら、ネットワークの深さと幅を増やしました。また、品質を最適化するために、ヘブの原理とマルチスケール処理の直感に基づいてアーキテクチャを決定しました。ILSVRC14に提出した「GoogLeNet」は、22層の深さを持つネットワークで、その品質は分類と検出の文脈で評価されています。

# 1. はじめに

この3年間で，深層学習や畳み込みネットワークの進歩により，物体の分類・検出能力が劇的に向上しました[10]．心強いニュースは，この進歩のほとんどが，より強力なハードウェア，より大きなデータセット，より大きなモデルの結果だけではなく，主に，新しいアイデア，アルゴリズム，改良されたネットワークアーキテクチャの結果であるということです．例えば、ILSVRC 2014の上位作品では、同じコンペティションの分類データセット以外に、検出目的で新たなデータソースは使用されていませんでした。ILSVRC 2014に提出したGoogLeNetは、2年前に優勝したKrizhevskyら[9]のアーキテクチャに比べて、実際には12倍も少ないパラメータしか使用していませんが、その一方で、精度は著しく向上しています。物体検出の分野では，大規模な深層ネットワークを単純に適用することではなく，Girshickら[6]によるR-CNNアルゴリズムのように，深層アーキテクチャと古典的なコンピュータビジョンの相乗効果によって，最大の成果が得られています．

また、モバイルコンピューティングやエンベデッドコンピューティングの普及に伴い、アルゴリズムの効率化、特に電力やメモリの使用量が重要になってきていることも注目すべき点です。本稿で紹介する深層アーキテクチャの設計において、精度の数値に固執するのではなく、この要因を考慮していることは注目に値します。ほとんどの実験において、モデルは推論時に15億回の乗算を行うという計算量を維持するように設計されており、純粋に学術的な好奇心で終わるのではなく、大規模なデータセットであっても合理的なコストで実世界で使用できるようになっています。

本論文では、コンピュータビジョン用の効率的な深層ニューラルネットワークアーキテクチャ、コードネーム「Inception」に焦点を当てます。この名前は、Linら[12]によるNetwork in net-work論文と、有名な「We need to go deep」というインターネットミーム[1]に由来しています。私たちの場合、「深い」という言葉は2つの異なる意味で使われています。まず、「Inceptionモジュール」という形で新しいレベルの組織を導入するという意味と、ネットワークの深さが増すというより直接的な意味があります。一般的に、Inceptionモデルは、Aroraら[2]による理論的研究からインスピレーションと指針を得ながら、[12]を論理的に集大成したものと考えることができます。このアーキテクチャの利点は、ILSVRC 2014の分類と検出の課題で実験的に検証されており、現在の技術水準を大幅に上回っています。

# 2. 関連作業

LeNet-5 [10]以降、畳み込みニューラルネットワーク（CNN）は、標準的な構造を持っています。すなわち、積み上げられた畳み込み層（オプションとして、コントラストの正規化と最大プール）の後に、1つ以上の完全連結層が続きます。この基本的なデザインのバリエーションは画像分類の分野で広く使われており、MNIST、CIFAR、そして最も注目すべきはImageNet分類チャレンジでこれまでに最高の結果を出している[9, 21]。Imagenetのような大規模なデータセットでは，オーバーフィッティングの問題に対処するためにドロップアウト[7]を使用しながら，層の数[12]と層のサイズ[21, 14]を増やすことが最近の傾向である．

層を最大にすると正確な空間情報が失われるという懸念があるものの，[9]と同じ畳み込みネットワークのアーキテクチャは，ローカリゼーション[9, 14]，物体検出[6, 14, 18, 5]，人間の姿勢推定[19]にも採用されています．

Serreら[15]は、霊長類の視覚野の神経科学モデルにヒントを得て、サイズの異なる一連の固定ガボール・フィルターを使用し、複数のスケールを処理しました。ここでは，同様の戦略を用いています．しかし、[15]の固定された2層の深層モデルとは異なり、Inceptionアーキテクチャではすべてのフィルタが学習されます。さらに、Inceptionのレイヤーは何度も繰り返され、GoogLeNetモデルの場合、22レイヤーの深層モデルになります。

Network-in-Networkは、Linら[12]によって提案された、ニューラルネットワークの表現力を高めるための手法です。Linらのモデルでは，ネットワークに11の畳み込み層を追加して深さを増しています．我々のアーキテクチャでは、このアプローチを多用しています。最も重要なことは、1x1畳み込みは主に次元削減モジュールとして使用され、ネットワークのサイズを制限する計算上のボトルネックを取り除くことです。これにより、パフォーマンスに大きな影響を与えることなく、ネットワークの深さだけでなく幅も大きくすることができます。

最後に，物体検出の最新技術は，Girshickら[6]によるRegions with Convolutional Neural Networks (R-CNN)法です．R-CNNは、全体的な検出問題を2つのサブ問題に分解します。すなわち、色やテクスチャなどの低レベルの手掛かりを利用して、カテゴリにとらわれない方法でオブジェクトの位置を提案し、CNN分類器を使ってそれらの位置でオブジェクト・カテゴリを識別します。このような2段階のアプローチは、低レベルの手がかりを用いたバウンディングボックス・セグメンテーションの精度と、最先端のCNNの強力な分類能力を活用しています。我々は同様のパイプラインを採用していますが、オブジェクトのバウンディングボックスの再現性を高めるためのマルチボックス[5]予測や、バウンディングボックスの提案をより適切に分類するためのアンサンブルアプローチなど、両方のステージで機能強化を図っています。

# 3. 動機と高レベルの考慮事項

深層ニューラルネットワークの性能を向上させる最も直接的な方法は、サイズを大きくすることです。これには、深さ（ネットワークレベルの数）と幅（各レベルのユニット数）の両方を増やすことが含まれます。これは、大量のラベル付き学習データがあれば、より高品質なモデルを簡単かつ安全に学習できる方法です。しかし、この単純な方法には2つの大きな欠点があります。

サイズが大きくなると、パラメータの数も多くなり、特に訓練セットのラベル付き例の数が限られている場合は、拡大されたネットワークがオーバーフィッティングしやすくなります。これは、強いラベルを付けたデータセットを入手するには手間と費用がかかり、ImageNetのような（1000クラスのILSVRCサブセットであっても）様々な細かい視覚カテゴリを区別するには、人間の専門的な評価者が必要になることが多いため、図1に示すように、大きな障害となります。

---

<ing src=>

---

ネットワークのサイズを一律に大きくすると、計算資源の使用量が劇的に増加するという欠点もあります。例えば、ディープビジョンネットワークでは、2つの畳み込み層を連鎖させた場合、そのフィルターの数を一律に増やすと、計算量が2次関数的に増加してしまいます。追加された容量が非効率的に使用された場合（例えば、ほとんどのウェイトがゼロに近い値になってしまった場合）、計算量の多くが無駄になります。計算機の予算は常に有限であるため、性能の質を高めることが主な目的であっても、無差別にサイズを増やすよりも、計算機資源を効率的に配分することが望ましい。

これらの問題を解決するための基本的な方法は、スパース性を導入し、畳み込みの中でも完全連結層をスパース層に置き換えることです。この方法は、生物学的なシステムを模倣するだけでなく、Aroraら[2]の画期的な研究により、より堅固な理論的裏付けを得ることができます。彼らの主な結果は、データセットの確率分布が大規模で非常に疎な深層ニューラルネットワークで表現できる場合、先行する層の活性化の相関統計を分析し、相関性の高い出力を持つニューロンをクラスタリングすることで、最適なネットワークトポロジーを層ごとに構築できるというものです。厳密な数学的証明には非常に強い条件が必要ですが、この記述が、よく知られているヘブの原理（一緒に発火するニューロンは一緒に配線される）と共鳴していることから、その根底にある考えは、実際にはそれほど厳しくない条件でも適用可能であることがわかります。

残念ながら、今日のコンピュータインフラは、非一様な疎なデータ構造上での数値計算に関しては非常に非効率的です。算術演算の回数を100回減らしたとしても、ルックアップやキャッシュミスによるオーバーヘッドが圧倒的に大きいため、スパースな行列への切り替えは利益につながらないかもしれません。このギャップは，CPU や GPU のハードウェアの詳細を利用して非常に高速な密行列の乗算を可能にする，着実に改良され，高度に調整された数値ライブラリを使用することでさらに広がります [16, 9]．また，非一様なスパースモデルでは，より高度なエンジニアリングとコンピューティングインフラが必要となります．現在のビジョン指向の機械学習システムのほとんどは、畳み込みを採用しているという理由だけで、空間ドメインのスパース性を利用しています。しかし、畳み込みは、前の層のパッチへの密な接続の集まりとして実装されています。ConvNetsは、対称性を崩して学習効果を高めるために、[11]以来、特徴次元でランダムで疎な接続テーブルを伝統的に使用してきましたが、並列計算をさらに最適化するために、[9]では完全な接続に戻る傾向がありました。現在のコンピュータビジョン用の最先端アーキテクチャは、画一的な構造をしています。フィルターの数が多く、バッチサイズが大きいため、密な計算を効率的に行うことができます。

このため、次の中間的なステップとして、理論で提案されているようにフィルターレベルのスパース性を利用しつつ、現在のハードウェアを利用して密な行列上の計算を利用するアーキテクチャに希望が持てないかという疑問が生じます。疎行列の計算に関する膨大な文献（例えば[3]）によると、疎行列を比較的密な部分行列にクラスタリングすることで、疎行列の乗算において競争力のある性能が得られる傾向があります。近い将来、同様の手法が非一様な深層学習アーキテクチャの自動構築に利用されるようになると考えても、不思議ではありません。

インセプションのアーキテクチャは、高度なネットワークトポロジー構築アルゴリズムの仮想的な出力を評価するケーススタディとしてスタートしました。このアルゴリズムは、視覚ネットワークについて[2]で示唆された疎な構造を近似し、その仮想的な出力を高密度で容易に入手できるコンポーネントでカバーしようとするものです。非常に推測的な試みではありましたが、[12]に基づくリファレンスネットワークと比較して、初期の段階でささやかな利益が観察されました。チューニングを重ねることでその差は広がり、Inceptionは[6]や[5]のベースネットワークとして、特にローカリゼーションや物体検出の文脈で有用であることが証明されました。興味深いことに、当初のアーキテクチャの選択のほとんどは、分離の際に疑問視され、徹底的にテストされましたが、局所的には最適に近いことが判明しました。インセプション・アーキテクチャがコンピュータ・ビジョンで成功を収めたとはいえ、その構築に導いた指導原理に起因するかどうかは、まだ疑問です。それを確かめるには、より詳細な分析と検証が必要です。

# 4. アーキテクチャの詳細

Inceptionアーキテクチャの主なアイデアは、畳み込みビジョンネットワークの最適な局所的疎構造が、どのように近似され、容易に入手可能な密なコンポーネントでカバーされるかを考えることです。翻訳不変性を仮定することは、ネットワークが畳み込みビルディングブロックから構築されることを意味することに注意してください。必要なのは、最適な局所構造を見つけ、それを空間的に繰り返すことです。Aroraら[2]は、レイヤーごとに構築する方法を提案しています。この方法では、最後のレイヤーの相関統計を分析し、相関性の高いユニットのグループにクラスタリングします。これらのクラスターが次の層のユニットとなり、前の層のユニットと接続される。ここでは、前の層の各ユニットが入力画像のある領域に対応していると仮定し、これらのユニットをフィルタバンクにグループ化します。下の層（入力に近い層）では、相関関係のあるユニットが局所的な領域に集中します。そのため、[12]で提案されているように、1つの領域に多くのクラスタが集中してしまい、それらは次の層の1x1の畳み込みの層でカバーすることができます。しかし、より大きなパッチ上の畳み込みでカバーできる、より空間的に広がったクラスターの数は少なくなり、より大きな領域上のパッチの数は減っていくことも予想されます。パッチアライメントの問題を回避するために、現在のInceptionアーキテクチャは、フィルタサイズが1x1、3x3、5x5に制限されています。この決定は、必要性というよりも利便性に基づいています。この決定は、必要性よりもむしろ利便性に基づいています。また、提案されているアーキテクチャは、これらすべての層とそれらの出力フィルターバンクを組み合わせて、次のステージの入力となる単一の出力ベクトルにしたものです。さらに、現在の畳み込みネットワークの成功にはプーリング処理が不可欠であることから、各段に別の並列プーリング経路を追加することで、さらに有益な効果が得られると考えられます（図2（a）参照）。

これらの「インセプション・モジュール」を重ねていくと、出力の相関統計が変化していきます。抽象度の高い特徴が上位のレイヤーに取り込まれると、その空間的な集中度は低下すると考えられます。このことから、3x3と5x5の畳み込みの比率は、高レイヤーになるほど高くなるはずです。

上記のモジュールの大きな問題点は、少なくともこのような素朴な形では、適度な数の5x5の畳み込みであっても、多数のフィルターを持つ畳み込み層の上では、法外にコストがかかるということです。この問題は、プーリングユニットが加わると、さらに顕著になります。出力フィルターの数は、前のステージのフィルターの数に等しくなります。プーリング層の出力と畳み込み層の出力を合成すると、ステージごとに出力数が増えるのは避けられない。このアーキテクチャは、最適なスパース構造をカバーできるかもしれませんが、非常に非効率的であり、数ステージで計算量が爆発してしまいます。

これは、Inceptionのアーキテクチャの2つ目のアイデアにつながります。つまり、そうしないと計算量が増えすぎてしまうような場合には、賢明に次元を減らすということです。これは、エンベッディングの成功に基づいています。低次元のエンベッディングであっても、比較的大きな画像パッチに関する多くの情報を含んでいる可能性があります。しかし，エンベッディングは，情報を高密度に圧縮して表現しており，圧縮された情報は処理が困難です．表現は（[2]の条件で要求されているように）ほとんどの場所でスパースに保たれ、信号を一括して集約しなければならない場合にのみ、信号を圧縮する必要があります。つまり、1x1の畳み込みは、高価な3x3や5x5の畳み込みの前にリダクションを計算するために使用されます。また、リダクションとして使用するだけでなく、整流された線形活性化の使用も含まれており、二重の目的で使用されています。最終的な結果は、図2(b)のようになります。

一般的に、インセプション・ネットワークは、上記のタイプのモジュールを重ねて構成されたネットワークであり、時折、グリッドの解像度を半分にするためにストライド2のマックス・プーリング層があります。技術的な理由（学習時のメモリ効率）から、インセプション・モジュールは高次の層でのみ使用し、低次の層は従来の畳み込み方式のままにしておくことが有益だと考えました。これは厳密には必要なことではなく、現在の実装ではインフラ的に非効率な部分があるためです。

このアーキテクチャの有用な点は、各ステージのユニット数を大幅に増やしても、後のステージで計算量が無秩序に増加しないことです。これは、パッチサイズを大きくして高価な畳み込みを行う前に、次元削減を行うことで実現しています。さらに、視覚情報はさまざまなスケールで処理され、次のステージで異なるスケールの特徴を同時に抽出できるように集約されるべきだという実用的な直感に基づいて設計されています。

また、計算機資源の利用効率が向上したことで、計算上の困難に陥ることなく、各ステージの幅とステージ数の両方を増やすことができます。Inceptionのアーキテクチャを利用して、少し劣っていても計算量の少ないバージョンを作ることができます。我々は、利用可能なすべてのノブとレバーを使って、計算資源のバランスを制御することで、Inceptionアーキテクチャを使用していない同様の性能のネットワークよりも3～10倍高速なネットワークを実現できることを発見しましたが、現時点では慎重な手動設計が必要です。
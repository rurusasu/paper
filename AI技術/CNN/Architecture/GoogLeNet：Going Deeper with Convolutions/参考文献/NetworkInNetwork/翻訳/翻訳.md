# Network-in-Network

# Abstract

我々は、受容野内の局所的なパッチに対するモデルの識別性を高めるために、"Network In Network"(NIN)と呼ばれる新しい深層ネットワーク構造を提案する。従来の畳み込み層では、線形フィルタと非線形活性化関数を用いて入力をスキャンしていました。その代わりに、より複雑な構造を持つマイクロニューラルネットワークを構築し、受容野内のデータを抽象化します。マイクロ・ニューラル・ネットワークのインスタンス化には，潜在的な関数近似である多層パーセプトロンを用いる．特徴量マップは、CNNと同様にマイクロネットワークを入力上でスライドさせることで得られ、次の層に供給されます。Deep NINは、上記のような構造を複数重ねることで実現できます。マイクロネットワークを用いて局所的なモデリングを強化することで、分類層では特徴マップの大域的な平均プーリングを利用することができ、従来の完全連結層に比べて解釈しやすく、オーバーフィッティングの可能性も低くなります。我々は、CIFAR-10およびCIFAR-100において、NINによる最先端の分類性能を実証し、SVHNおよびMNISTデータセットにおいても妥当な性能を示した。

畳み込みニューラルネットワーク（CNN）[1]は，畳み込み層とプーリング層を交互に配置した構成になっている．**畳み込み層は，線形フィルタと下層の受容野の内積をとり，それに続いて入力の局所部分ごとに非線形活性化関数をとります．結果として得られる出力は特徴マップと呼ばれる。**

CNNの畳み込みフィルターは、基礎となるデータパッチに対する一般化線形モデル（GLM）であり、GLMでは抽象度が低いと主張している。抽象度とは、その特徴が同じ概念のバリエーションに対して不変であることを意味します[2]。GLMをより強力な非線形関数近似器に置き換えることで、局所モデルの抽象化能力を高めることができます。GLMは、潜在的な概念のサンプルが線形分離可能な場合、つまり、概念のバリエーションがGLMで定義された分離平面の片側にすべて存在する場合に、良好な抽象化を達成することができます。したがって、従来のCNNは暗黙のうちに潜在的な概念が線形分離可能であると仮定している。しかし、同じ概念のデータは非線形多様体上に存在することが多く、したがって、これらの概念を捉える表現は一般的に入力の高度な非線形関数となります。NINでは、GLMを一般的な非線形関数近似器である「マイクロネットワーク」構造に置き換えます。本研究では，マイクロネットワークのインスタンスとして多層パーセプトロン[3]を選択した。多層パーセプトロンは，普遍的な関数近似器であり，バックプロパゲーションによって学習可能なニューラルネットワークである。

その結果、mlpconv層と呼んでいる構造をCNNと比較したのが図1である。線形畳み込み層もmlpconv層も、局所受容野を出力特徴ベクトルにマッピングします。mlpconv層は、非線形活性化関数を持つ複数の完全連結層からなる多層パーセプトロン（MLP）を用いて、入力された局所パッチを出力特徴ベクトルにマッピングする。このMLPは，すべての局所受容野で共有されます。特徴量マップは、CNNと同様にMLPを入力上でスライドさせることで得られ、次の層に供給されます。NINの全体的な構造は、複数のMLPconv層を積み重ねたものです。深層ネットワーク全体を構成する要素であるマイクロネットワーク（MLP）をmlpconv層の中に入れていることから、「Network In Network」（NIN）と呼ばれています。

CNNの分類に従来の完全連結層を採用する代わりに、最後のmlpconv層からの特徴マップの空間平均を、グローバルアベレージプーリング層を経由してカテゴリーの信頼度として直接出力し、その結果のベクトルをソフトマックス層に入力する。**従来のCNNでは、完全連結層がブラックボックスのように機能するため、目的コスト層からのカテゴリーレベルの情報がどのように前の畳み込み層に戻されるのかを解釈することが困難でした。**対照的に、グローバルアベレージプーリングは、マイクロネットワークを使ったより強力なローカルモデリングによって可能になった、特徴マップとカテゴリーの間の対応関係を強制するので、より意味があり解釈しやすい。さらに、**完全連結層はオーバーフィッティングの傾向があり、ドロップアウト正則化に大きく依存していますが[4][5]、グローバルアベレージプーリングはそれ自体が構造的正則化であり、全体の構造に対するオーバーフィッティングを本質的に防止します。**


# 2. 畳み込み式ニューラルネットワーク

古典的な畳み込みニューロンネットワーク [1] は，交互に積み重ねられた畳み込み層と空間プーリング層から構成されています．畳み込み層では，線形畳み込みフィルタに非線形活性化関数（整流器，シグモイド，tanhなど）を適用して，特徴量マップを生成します．線形整流器を例にとると、特徴マップは次のように計算できる。

$$
f_{i, j, k} = \max(w_{k}^T x_{i, j}, 0)
$$

ここで、$(i; j)$はフィーチャーマップのピクセルインデックス、$x_{ij}$は位置$(i; j)$を中心とした入力パッチを表し、$k$はフィーチャーマップのチャンネルのインデックスとして使用されます。

潜在的な概念のインスタンスが線形分離可能な場合、この線形畳み込みは抽象化には十分である。しかし、優れた抽象化を実現する表現は、一般に入力データの高度な非線形関数である。従来のCNNでは、潜在的な概念のすべてのバリエーションをカバーするために、フィルタの過剰なセットを利用することでこの問題を解決していた[6]。つまり、同じ概念の異なるバリエーションを検出するために、個々の線形フィルタを学習することができる。しかし、1つの概念に対してあまりにも多くのフィルタを使用すると、前の層からのバリエーションのすべての組み合わせを考慮する必要がある次の層に余分な負担をかけることになる[7]。CNNのように，上位層のフィルタは元の入力のより大きな領域にマッピングされる．下の層の低レベルの概念を組み合わせることで、より高いレベルの概念を生成します。そのため、より高いレベルの概念に結合する前に、各ローカルパッチに対してより良い抽象化を行うことが有益であると主張する。

最近のmaxoutネットワーク[8]では，アフィン特徴マップに対する最大プーリングによって特徴マップの数を減らしている（アフィン特徴マップとは，活性化関数を適用しない線形畳み込みの直接の結果である）．一次関数に対する最大化により，任意の凸関数を近似できる区分的線形近似器が得られる．線形分離を行う従来の畳み込み層と比較して、maxoutネットワークは、凸集合内にある概念を分離することができるため、より強力である。この改良により、いくつかのベンチマークデータセットにおいて、maxoutネットワークは最高の性能を発揮している。

しかし、maxoutネットワークは、潜在概念のインスタンスが入力空間の凸集合内に存在するという事前条件を課しているが、これは必ずしも成り立たない。潜在的な概念の分布がより複雑な場合には、より一般的な関数近似を採用する必要があります。そこで我々は、各畳み込み層の中にマイクロネットワークを導入し、ローカルパッチのより抽象的な特徴を計算する「ネットワーク・イン・ネットワーク」という新しい構造を導入することで、これを実現しようとしている。

マイクロネットワークを入力に対してスライドさせる手法は，いくつかの先行研究で提案されている．例えば，SMLP（Structured Multilayer Perceptron）[9]では，入力画像の異なるパッチに共有の多層パーセプトロンを適用し，別の作品では，顔検出のためにニューラルネットワークベースのフィルタを学習している[10]．しかし，これらはいずれも特定の問題のために設計されたものであり，スライディングネットワーク構造の1つの層しか含まれていません．NINは、より一般的な観点から提案されており、マイクロネットワークをCNN構造に統合し、あらゆるレベルの特徴をより良く抽象化することを追求しています。

# 3. Network in Network

まず、提案する「ネットワーク・イン・ネットワーク」構造の主要構成要素であるMLP畳み込み層とグローバルアベレージングプーリング層について、それぞれ3.1項と3.2項で説明します。続いて、NINの全体像を説明します（3.3項）。

## 3.1. MPL 畳み込み層

---

<img src=https://raw.githubusercontent.com/rurusasu/paper/master/AI%E6%8A%80%E8%A1%93/CNN/Architecture/GoogLeNet%EF%BC%9AGoing%20Deeper%20with%20Convolutions/%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE/NetworkInNetwork/%E7%94%BB%E5%83%8F/fig1.png>

線形畳み込み層とmlpconv層の比較。線形畳み込み層は線形フィルタを含み、mlpconv層はマイクロネットワーク（本稿では多層パーセプトロンを選択）を含む。どちらの層も局所的な受容野を潜在的な概念の信頼値にマッピングする。

---

潜在的な概念の分布に関する事前情報がない場合、潜在的な概念のより抽象的な表現を近似することができる普遍的な関数近似器を局所パッチの特徴抽出に用いることが望ましい。普遍関数近似器としては、放射状基底ネットワークや多層パーセプトロンがよく知られている。本研究では、2つの理由から多層パーセプトロンを選択した。まず、多層パーセプトロンは、バックプロパゲーションを用いて学習される畳み込みニューラルネットワークの構造と互換性がある。第二に、多層パーセプトロンはそれ自体が深層モデルとなりうるため、特徴再利用の精神に合致しています[2]。この新しいタイプの層は、本稿ではmlpconvと呼ばれており、MLPがGLMに代わって入力に対して畳み込みを行うものです。図1は、線形畳み込み層とmlpconv層の違いを示しています。mlpconv層で行われる計算は以下のようになります。

$$
f^1_{i, j, k_1} = \max({w^1_{k_1}}^Tx_{i, j}+b_{k_1}, 0) \\

f^n_{i, j, k_n} = \max({w^n_{k_n}}^Tx_{i, j}+b_{k_n}, 0)
$$

ここで，nは多層パーセプトロンの層数である。多層パーセプトロンの活性化関数には、整流線形ユニットを使用しています。

クロスチャンネル（クロスフィーチャーマップ）プーリングの観点から見ると、式2は通常の畳み込み層にカスケード接続されたクロスチャンネルパラメトリックプーリングと同等である。各プーリング層は、入力された特徴マップに対して重み付き線形再結合を行い、その後、整流器線形ユニットを通過する。クロスチャンネルプーリングされた特徴マップは、次の層で何度も何度もクロスチャンネルプーリングされます。このカスケード接続されたクロスチャネル・パラメータ・プーリング構造により、クロスチャネル情報の複雑で学習可能な相互作用が可能になります。

**クロスチャンネルパラメトリックプーリング層は、1x1畳み込みフィルタを持つ畳み込み層に相当します。**このように解釈することで、NINの構造をわかりやすく理解することができます。

**maxout層との比較**：maxoutネットワークのmaxout層は、複数のアフィンフィーチャーマップに対してmaxプーリングを行う[8]。maxout層の特徴量マップは以下のように計算される。

$$
f_{i, j, k}=\max_{m}(w^T_{k_m}x_{i, j})
$$

一次関数のMaxoutは、任意の凸関数をモデル化できる区分的一次関数を形成します。凸関数の場合、特定の閾値以下の関数値を持つサンプルは、凸セットを形成します。したがって、ローカルパッチの凸関数を近似することで、maxoutは、サンプルが凸集合内にあるコンセプトの分離超平面を形成する機能を持っています（例：l2ボール、凸円錐）。Mlpconv層は、maxout層とは異なり、凸関数近似器がユニバーサル関数近似器に置き換えられており、潜在的な概念の様々な分布をモデル化する能力が高くなっています。

## 3.2. Global Average Pooling

従来の畳み込みニューラルネットワークは，ネットワークの下位層で畳み込みを行います．**分類のためには，最後の畳み込み層の特徴マップがベクトル化され，完全連結層とソフトマックス・ロジスティック回帰層に供給されます [4] [8] [11]．この構造は，畳み込み構造と従来のニューラルネットワーク分類器をつなぐものである．この構造では，畳み込み層を特徴抽出器として扱い，その結果得られた特徴を従来の方法で分類します．**

しかし，完全連結層ではオーバーフィッティングが起こりやすく，ネットワーク全体の汎化能力が低下する．Hintonら[5]は，学習時に完全連結層の活性度の半分をランダムにゼロにするレギュラライザとしてDropoutを提案した．これにより，汎化能力が向上し 汎化能力を向上させ，オーバーフィッティングを大幅に防止している[4]．

本論文では、CNNにおける従来の完全連結層に代わるものとして、グローバルアベレージプーリングと呼ばれる別の戦略を提案する。**そのアイデアは、最後のmlpconv層で、分類タスクの対応するカテゴリごとに1つの特徴マップを生成することです。特徴マップの上に完全連結層を追加するのではなく、各特徴マップの平均値を取り、その結果のベクトルを直接ソフトマックス層に入力します。完全連結層と比較した場合のグローバルアベレージプーリングの利点は、特徴マップとカテゴリの間に対応関係を持たせることで、よりコンボリューション構造に近いものになることです。したがって、特徴量マップは、カテゴリ信頼度マップとして簡単に解釈できます。**もう1つの利点は、グローバルアベレージプーリングでは**最適化するパラメータがないため、この層でのオーバーフィッティングが回避されることです。さらに、グローバル・アベレージ・プーリングは、空間的な情報をまとめているため、入力の空間的な変換に対してより頑健である。**

グローバルアベレージプーリングは、特徴量マップが概念（カテゴリー）の信頼マップであることを明示的に強制する構造正則化として見ることができます。これは、GLMよりも信頼マップの近似を行うmlpconv層によって可能になります。

## 3.3. Network In Network の構造

---

<img src=https://raw.githubusercontent.com/rurusasu/paper/master/AI%E6%8A%80%E8%A1%93/CNN/Architecture/GoogLeNet%EF%BC%9AGoing%20Deeper%20with%20Convolutions/%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE/NetworkInNetwork/%E7%94%BB%E5%83%8F/fig2.png>

Fig2. Network In Networkの全体的な構造です。本論文では、NINには3つのMLPCONVレイヤーと1つのグローバルアベレージプーリングレイヤーの積層が含まれている。

---

NINの全体的な構造は、mlpconvレイヤーのスタックで、その上にグローバルアベレージプーリングと目的コストのレイヤーがあります。CNNやmaxoutネットワークのように、mlpconv層の間にサブサンプリング層を追加することもできます。図2は、3つのmlpconv層を持つNINを示しています。各mlpconv層の中には、3層のパーセプトロンがあります。NINもマイクロネットワークも、層の数は柔軟で、特定のタスクに合わせて調整することができます。
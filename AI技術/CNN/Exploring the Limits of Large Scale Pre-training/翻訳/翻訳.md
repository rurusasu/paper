# Exploring the Limits of Large Scale Pre-training

# Abst

近年の大規模機械学習の発展は、データ、モデルサイズ、学習時間を適切にスケールアップすることで、事前学習の改善がほとんどの下流タスクに有利に伝達されることを観察することができることを示唆している。本研究では、この現象を系統的に研究し、上流の精度を上げると下流のタスクの性能が飽和することを証明しました。具体的には、Vision Transformers、MLP-Mixer、ResNetsについて、パラメータ数が1000万から100億の範囲で4800回以上の実験を行い、最大規模の画像データ（JFT、ImageNet21K）で学習し、20以上の下流の画像認識タスクで評価した。その結果、飽和現象を反映し、上流と下流の性能の非線形関係を捉えた下流性能のモデルを提案した。さらに、このような現象が発生する理由を掘り下げて理解するために、私たちが観察した飽和現象は、モデルの層を通して表現が進化する方法と密接に関係していることを示しました。また、さらに極端な例として、アップストリームとダウンストリームのパフォーマンスが相反する場合を紹介します。つまり、ダウンストリームのパフォーマンスを向上させるためには、アップストリームの精度を落とす必要があるのです。

# 1. Intro

転移学習や少数ショット学習に関する最近の目覚ましい進歩は、モデルをスケールアップして膨大なコーパスのデータで学習することが、データが少ない、あるいは全くない下流のタスクでの性能向上に向けての主な障害になるという新たな方向性を示唆している。顕著な例として、[Brown et al., 2020]では、大規模なコーパスのデータで学習した大規模な変換モデル[Vaswani et al., 2017]であるGPT-3が、多くの自然言語処理（NLP）タスクやベンチマークにおいて、少数ショットの設定で実質的な性能を達成することを示しています。画像認識タスクでは、Instagramの画像[Mahajan et al., 2018]やJFT-300[Sun et al., 2017]でのトレーニングが、転移学習および数ショット学習の設定で非常に効果的であることが証明されています[Goyal et al., 2021, Kolesnikov et al., 2019, Pham et al., 2020, Dosovitskiy et al., 2020, Dumoulin et al.］ 例が提供されていない場合（ゼロショット）でも、インターネット上の4億個の画像-テキストペアを用いて対照的な損失で学習した画像エンコーダモデルとテキストエンコーダモデルのペアで構成されるCLIP［Radford et al.，2021］は、驚くべき性能を発揮します。

上記のすべての開発は、暗黙のうちに2つの一貫した見解を促しています。

1. モデルとデータのサイズをスケールアップすることで、性能が大幅に向上すること
1. 性能向上が望ましい形で下流のタスクに移行すること。

Kaplanら[2020]は、1つ目の見解を支持する、より焦点を絞った実証研究において、言語モデリングタスクにおけるモデルサイズ、データ、および計算を適切にスケールアップすることで、性能が飽和しない形で戻ってくることを示しています。Belloら[2021]、Tan and Le[2019]は、画像認識タスクにおいても好ましいスケーリングが実現できることを示している。第2の見解もまた、最近の焦点を当てた研究の対象となっている。Hernandezら[2021]は、[Kaplanら, 2020, Tayら, 2021b]と同様の有利なスケーリング法則が、NLPタスクの転送および少数ショットの設定で成立することを示している。おそらく我々に最も近い先行研究であるKornblithら[2019]は、ImageNet[Russakovskyら、2015]でのパフォーマンスと下流の画像認識タスクとの間に線形関係1があることを観察している。

上記の見解を採用することは、今後大きな意味を持ちます。これらの見解によると、**1つの巨大なコーパスのパフォーマンスを向上させるために計算機や研究の労力を費やすことは、多くの下流のタスクをほぼ無料で解決できるため、有益であると考えられます。** **また、上流の性能を向上させる一方で、下流のタスクについては、その向上が直線的な傾向に基づいて予測できるため、心配する必要がないということです。** 前述の研究は説得力のあるストーリーを提供していますが、大きな欠点があります。計算機の制限により、ハイパーパラメータ値の異なる選択に対するパフォーマンスは報告されていません。スケーリングプロットは，各スケールで選択されたハイパーパラメータが固定されているか，単純なスケーリング関数で決定されている場合には，より好ましいと思われます．さらに、多くの場合、目的は最先端の結果を改善することであり、したがって、ハイパーパラメータの選択における努力のほとんどは、当然のことながら、より高いスケールに集中しており、これはスケーリング・プロットを著しく偏らせている。しかし、**スケーリングの研究では、ハイパーパラメータのすべての可能な値が与えられたときに、モデルの下流側での最高のパフォーマンスに関心があります。さらに、ほとんどのスケーリング研究では、限られた範囲での挙動が報告されており、スケーリングのダイナミクスをさらに理解することなく、そのスケーリングを単純に外挿すると、研究した範囲の外でスケーリングが保持される理由が先験的に存在しないため、有害になる可能性があります。**

本論文では、大規模な上流タスクでの改善点を、少数ショットと転移学習の両方のシナリオで、広範囲の下流タスクに転移できるかどうかを体系的に調査する。上記の欠点を解決するために、我々の研究の一部は、4800以上のVision Transformer [Dosovitskiy et al., 2020]、MLP-Mixer [Tolstikhin et al., 2021]およびResNet [Dosovitskiy et al., 2020]モデルのメタ研究である。これらのモデルは、303Mの画像と18Kのクラスを持つJFT[Sun et al., 2017]または14Mの画像と21Kのクラスを持つImageNet21K[Deng et al., 2009]のいずれかで事前に学習され、少数ショットおよび伝達学習の設定で様々なダウンストリームデータセットで評価されます。我々の25のダウンストリームタスクは、VTAB [Zhai et al., 2019]、MetaDataset [Triantafillou et al., 2019]、Wilds [Koh et al., 2020]やメディカルイメージングなどのベンチマークに含まれる標準的なデータセットを幅広くカバーしています。

---

![fig1](https://raw.githubusercontent.com/rurusasu/paper/master/AI%E6%8A%80%E8%A1%93/CNN/Exploring%20the%20Limits%20of%20Large%20Scale%20Pre-training/%E7%94%BB%E5%83%8F/fig1.png)

図1: 1500種類以上のVision Transformers、1400種類のMLP-Mixer、16種類の最も性能の高いResNetに基づいた、異なる下流 (DS) タスクと上流 (US) タスクの性能 (ResNetのサンプル数は少ないが、これは我々の調査に支障はない。詳細はセクション1.1を参照してください) を用いて，様々な設定を行いました．これらのモデルは，JFTで事前に学習され，数ショット (25ショット) の設定で評価されています．図2は同じプロットですが、JFTとImageNet21Kの2つの異なる上流タスクを含む4800回以上の実験を1ショットと25ショットで行っています。点の凸包も考慮しています。これは、これらのモデルを異なる確率で選択して作られたランダムな分類器の性能を捉えているからです。上流側の性能が向上すると、下流側の性能が飽和していきます。US精度が100％に達しても、DS精度は100％よりもかなり低い値に飽和してしまうのです。上流側と下流側の精度には非線形の関係があり，これをべき乗則関数でモデル化することで，USの精度が与えられたときのDSの性能を予測した。水平線は、上流側の精度が100％に達した場合に予測される下流側の精度です。ここでは，ハイパーパラメータの選択の影響を把握し，スケーリングがUS性能を通じてDS性能に影響を与えるという事実を考慮して，通常のDS-scaleプロットの代わりにDS-s-USプロットを調査した．図13は、多くの関連研究で行われているように、精度の対数スケーリングを行った場合のプロットです。図14は、上流がImageNet21Kの場合の同じプロットです。

---

**我々は、画像認識タスクにおける少数ショット学習と転移学習のパフォーマンスにおけるスケールの役割を研究し、スケーリング (およびハイパーパラメータのチューニング) がワンモデル・フィッツ・オールのソリューションにつながらないという強力な経験的証拠を提供する。** しかし、まだ多くの未解決の課題が残されており、その中心となるのが、下流のタスクにおけるデータの多様性の問題です。我々は、この現象を初めて大規模かつ体系的に調査し、その理由を考察した。図1では、さまざまなモデルとダウンストリームタスクを対象に、ダウンストリーム（DS）とアップストリーム（US）のパフォーマンスを比較しています。USの精度を上げると、ほとんどの場合、DSの精度は100％を大幅に下回る値で飽和することがわかる。また、この飽和現象は例外ではなく、一般的な傾向であり、ショット数やUSタスクの選択に対しても頑健であることが分かりました (図2参照)。このギャップは、ノイズなどのDSタスクのみに依存する要因ではなく、USタスクとDSタスクの関係に依存することを立証した。さらに、同じようなUS精度を持つモデル群があったとしても、DSタスクによって最適なモデルは異なります。

---

![fig2](https://raw.githubusercontent.com/rurusasu/paper/master/AI%E6%8A%80%E8%A1%93/CNN/Exploring%20the%20Limits%20of%20Large%20Scale%20Pre-training/%E7%94%BB%E5%83%8F/fig2.png)

図2：4800種類以上の実験（2974個のVision Transformers、1593個のMLP-Mixer、249個のResNets）に基づく、下流（8種類のタスク）と上流のパフォーマンスの比較。実験は，上流のデータセット（JFTまたはImageNet21k）と，数ショット評価のショット数（1および25）に基づいてグループ化されている．点線は，DS-vs-USプロット上の点の凸包を示している．上流の2つのタスクで異なる値で飽和が起こるということは、飽和はDSタスクだけに依存するのではなく、むしろUSタスクとDSタスクの関係が重要であることを示唆している。

---

貢献度 本論文の主な貢献度は以下の通りです。

- 大規模な研究により、スケールアップやハイパーパラメータ、アーキテクチャの選択によって上流 (US) タスクの性能を向上させると、下流 (DS) タスクの性能が飽和することが明らかになりました。我々の実験では、いくつかのDSタスクが、調査した範囲内で完全に飽和状態に達しました (セクション2)。
- 同様のUS精度を持つモデル群が与えられた場合、あるDSタスクTDS1に対するベストモデルは、別のDSタスクTDS2に対するベストモデルと比較して、はるかに悪いパフォーマンスを示すことが実証されました (図6)。
- 実験の規模を考えると、提案モデルがDS-vs-USプロットのポイントの密度に影響されないことは非常に重要である。我々は、実験の凸包にべき乗則を当てはめることで、下流の精度の予測に対するサンプリングバイアスの影響を回避し、サンプルサイズの変化に対するモデルの頑健性を示すことができると主張し、実証しました (2.2項)。
- 上流側の精度と下流側の精度の間に非線形の関係があることを確認した後，与えられた上流側の精度に対する下流側の性能を予測するために，これらの関係をべき乗則曲線でモデル化し，サンプル数が少なくてもその挙動をよく捉えていることを確認しました (セクション2.2)．
- モデルサイズ，データサイズ，計算量の拡大がDSの性能にどのように影響するかを調べ，これらのパラメータが主にUSの性能を通じてDSの性能に影響することを示した (セクション2.3)．
- DSの性能が飽和する理由を調査し，この挙動は，学習済みモデルの高次層における特徴表現の有用性によって捉えられることを示す (セクション3)．
- さらに，上流側と下流側の性能の不一致を調査し，ハイパーパラメータの選択によっては，両者が相反する可能性があることを示した．特に、事前学習 (上流タスク) で使用したヘッドの最適なハイパーパラメータが、USとDSで異なることを紹介します。そして、この不一致の理由を明らかにします (セクション4)。すなわち、重み減衰や学習率などの頭部のハイパーパラメータを変更することで、頭部で圧縮された情報を下層に押し下げることができ、上流タスクでの性能低下と、上流タスクに関連する下流タスクでの性能向上をもたらすことができます。これはレイヤーマージンや重みのL2ノルムで捉えることができます。
- 最後に，上流データのサイズ，精度の一般的なスケーリングの選択，ショット数，転移と数ショットの設定，アーキテクチャなどのいくつかの選択に対して，我々の観察結果がどのように頑健であるかを示します（セクション5）．
関連する仕事 我々に最も近い研究は、Kornblithら[2019]の研究である。彼らは、数ショット、転送、ランダムな初期化のシナリオについて、12のデータセットでImageNet [Russakovsky et al., 2015]の事前学習が画像分類のパフォーマンスに与える影響を調査しています。彼らは，ImageNetでのパフォーマンスが，DSタスクでのパフォーマンスに (ロジットスケーリングで) 線形に変換されることを示している．しかし、彼らはその値の外挿を考慮していません。両者とも、様々な実験を通じて事前学習の効果を調べていますが、「上流のパフォーマンスの向上が下流のパフォーマンスの向上につながるか」という問いに対する回答には、2つの大きな違いがあります。**まず、DSとUSのパフォーマンスを比較した場合、明らかな「飽和」現象が存在することを確認しました。** 図1を見ると、AとBの2つのモデルを比較したときに、モデルAのUS精度がはるかに高く、DS精度が低いというさまざまなケースがありますが、これは例外ではなく、むしろ大多数のケースです。基本的には、DS-USプロットにおいて、一方が右にあり、他方が下にある2つの点は、このようなケースの例です。次に、各DSタスクにおいて、式1のように最適なモデルがべき乗則でスケーリングされていることがわかりますが、各アーキテクチャにおいて、最適なモデルはDSタスクごとに異なり、これはトレーニングのハイパーパラメータに依存します（図6参照）。つまり、2つのDSタスク（TDS1 , TDS2）を考慮した場合、モデルAはUSとTDS1では優れた性能を発揮するが、DS2では優れた性能を発揮するとは結論づけられないケースが多数あります。この結論の違いは、以前の研究では考慮する精度値の範囲が限られていたためではないかと考えています。このような結論の違いに加えて、我々はこの飽和挙動の背景にある理由を調査する。さらに、セクション4では、USとDSの性能が相反する場合、つまり、USの性能低下がDSの性能向上につながるシナリオを検討します。Zhai et al., 2021]が、事前学習時に頭部の重み減衰を大きくすると、DSの性能が向上する一方で、USの性能が低下することを指摘していることにヒントを得て、頭部のハイパーパラメータ（重み減衰と学習率の両方）をさらに調査し、これらの操作が頭部に蓄積された情報を下層に押し下げることに着目することで説明できることを示しています。その他の関連研究については、付録Aをご覧ください。
# A Survey on Image Data Augumentation for Deep Learning

# 著者

Connor Shorten, Taghi M. Khoshgoftaar

# Abstract

ディープ畳み込みニューラルネットワークは、多くのコンピュータビジョンタスクにおいて非常に優れた性能を発揮しています。しかし、これらのネットワークは、オーバーフィットを避けるためにビッグデータに大きく依存しています。**オーバーフィットとは、ネットワークが学習データを完全にモデル化するために、非常に高い分散を持つ関数を学習してしまう現象のことです。**しかし、残念ながら、医療画像解析など、多くのアプリケーション領域ではビッグデータを利用できないことが多いのが現状です。本調査では、限られたデータの問題をデータ空間で解決するData Augmentationに焦点を当てています。**データオーグメンテーションは、トレーニングデータセットのサイズと品質を向上させ、それらを用いてより優れたディープラーニングモデルを構築できるようにする一連の技術を包含しています。**本調査では、幾何学的変換、色空間の拡張、カーネルフィルタ、画像の混合、ランダム消去、特徴空間の拡張、敵対的トレーニング、生成的敵対ネットワーク、ニューラルスタイル転送、メタ学習などの画像拡張アルゴリズムを取り上げています。本調査では、GANに基づく増強手法の応用を中心に取り上げている。本論文では、オーグメンテーション技術に加えて、テスト時間の増大、解像度の影響、最終データセットのサイズ、カリキュラム学習など、データオーグメンテーションの他の特徴についても簡単に議論する。本調査では、データオーグメンテーションのための既存の手法、有望な開発、データオーグメンテーションを実施するためのメタレベルの決定を提示する。読者は、データオーグメンテーションがどのようにモデルのパフォーマンスを向上させ、限られたデータセットを拡張してビッグデータの能力を活用することができるかを理解することができるでしょう。

# Introduction

ディープラーニングモデルは、識別タスクにおいて信じられないほどの進歩を遂げてきました。 これは、ディープネットワークアーキテクチャの進歩、強力な計算能力、ビッグデータへのアクセスによって促進されてきました。 ディープニューラルネットワークは、畳み込みニューラルネットワーク（CNN）の開発により、画像分類、物体検出、画像分離などのコンピュータビジョンタスクへの応用に成功しています。これらのニューラルネットワークは、画像の空間特性を保持するパラメータ化された疎結合のカーネルを利用しています。 畳み込み層は、画像の空間分解能を順次ダウンサンプルしながら、特徴マップの深さを拡大していきます。 この一連の畳み込み変換により，手作業で作成されたものよりもはるかに低次元で，より有用な画像の再表現を作成することができる．CNNの成功は、ディープラーニングをコンピュータビジョンのタスクに適用することへの関心と楽観的な考えに拍車をかけています。

深層畳み込みネットワークをコンピュータビジョンのタスクに適用することで、現在のベンチマークを向上させようとする研究は、多くの分野で行われています。 これらのモデルの一般化能力を向上させることは、最も困難な課題の一つである。一般化可能性とは、以前に見たデータ（訓練データ）と見たことのないデータ（テストデータ）で評価したときのモデルの性能差のことを指します。一般化可能性が低いモデルは、訓練データにオーバーフィットしています。オーバーフィットを発見する1つの方法は、トレーニング中の各エポックでのトレーニングと検証の精度をプロットすることです。下のグラフは、学習エポックの精度を可視化したときのオーバーフィットの様子を示しています[図1]。

---

![図1](https://raw.githubusercontent.com/rurusasu/paper/master/AI%E6%8A%80%E8%A1%93/%E7%94%BB%E5%83%8F%E3%82%AA%E3%83%BC%E3%82%AE%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%86%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3/A%20Survey%20on%20Image%20Data%20Augumentation%20for%20Deep%20Learning/%E7%94%BB%E5%83%8F/%E5%9B%B31.png)

図1：左側のプロットは、トレーニング率が減少し続けると検証エラーが増加し始める変曲点を示しています。トレーニングの増加により、モデルはトレーニング・データにオーバーフィットし、トレーニング・セットと比較してテスト・セットでのパフォーマンスが悪くなっています。対照的に、右側のプロットは、トレーニングとテストの誤差の間の望ましい関係を持つモデルを示しています。

---

有用なディープラーニングモデルを構築するためには、検証誤差が訓練誤差とともに減少し続けなければなりません。データオーグメンテーションは、これを達成するための非常に強力な方法です。拡張されたデータは、可能なデータポイントのより包括的なセットを表すので、トレーニングセットと検証セット、および将来のテストセット間の距離を最小限に抑えることができます。

本調査の焦点であるデータオーグメンテーションは、オーバーフィッティングを減らすために開発された唯一の手法ではありません。以下の数段落では、ディープラーニングモデルのオーバーフィッティングを回避するために利用可能な他の解決策を紹介します。 このリストは、読者にデータオーグメンテーションの文脈をより広く理解してもらうことを目的としています。

一般化性能を向上させるための他の多くの戦略は、モデルのアーキテクチャ自体に焦点を当てています。 これにより、AlexNet [1]からVGG-16 [2]、ResNet [3]、Inception-V3 [4]、DenseNet [5]に至るまで、より複雑なアーキテクチャが次々と開発されてきました。ドロップアウト正則化、バッチ正規化、転移学習、事前学習などの機能的なソリューションは、より小さなデータセットへの適用のためにディープラーニングを拡張するために開発されてきました。これらのオーバーフィッティングの解決策の簡単な説明を以下に示す。ディープラーニングにおける正則化手法の完全な調査は、Kukackaら[6]によってまとめられています。 これらのオーバーフィッティングの解決法についての知識は、読者に他の既存のツールについての情報を与え、データ拡張とディープラーニングの高レベルのコンテキストをフレーミングすることになります。

* ドロップアウト[7]は、訓練中に実行されて選択されたニューロンの活性化値をゼロにする正則化手法である。 この制約により、ネットワーク内の少数のニューロンの予測能力に頼るのではなく、よりロバストな特徴を学習するようにネットワークを強制する。 Tompsonら[8]は、この考えを、個々のニューロンではなく特徴マップ全体をドロップアウトする「空間ドロップアウト」を持つ畳み込みネットワークに拡張した。

* バッチ正規化 [9] は、レイヤ内の活性化のセットを正規化する別の正規化技術です。 正規化は、各活性化からバッチ平均を減算し、バッチ標準偏差で除算することで機能します。 この正規化技術は、標準化とともに、ピクセル値の前処理における標準的な技術です。

* 転送学習 [10, 11] は、オーバーフィットを防ぐためのもう一つの興味深いパラダイムです。 転送学習は、ImageNet [12]のような大規模なデータセット上でネットワークを学習し、それらの重みを新しい分類タスクの初期重みとして使用することで動作します。 一般的には，完全に接続された層を含むネットワーク全体をコピーするのではなく，畳み込み層の重みだけをコピーします． 多くの画像データセットは、ビッグデータでよりよく学習される低レベルの空間特性を共有しているため、この方法は非常に効果的です。 転送されたデータ領域間の関係を理解することは、現在進行中の研究課題である[13]。 Yosinskiら[14]は、転送可能性が、主に高次の層のニューロンの特殊化と共適応ニューロンの分割の難しさによって負の影響を受けることを発見している。

* 事前学習[15]は、概念的には転移学習と非常によく似ている。 プレトレーニングでは，ネットワークアーキテクチャを定義し，ImageNet [12]のような大規模なデータセットで学習する．これが伝達学習と異なる点は，VGG-16 [2]やResNet [3]のようなネットワークアーキテクチャを，重みと同様に伝達しなければならない点である．事前学習により、大規模データセットを用いた重みの初期化が可能になる一方で、ネットワークアーキテクチャの設計に柔軟性を持たせることが可能になる。

* ワンショット学習とゼロショット学習 [16, 17] アルゴリズムは、非常に限られたデータでモデルを構築するための別のパラダイムである。 ワンショット学習は、顔認識アプリケーションで一般的に使用されている[18]。ワンショット学習へのアプローチとしては、ネットワークが1つまたは数個のインスタンスでしか訓練されていなくても画像分類が可能なような距離関数を学習するシャムネットワーク[19]を使用する方法がある。もう一つの非常に人気のあるワンショット学習のアプローチは、メモリ拡張ネットワークス[20]の使用である。ゼロショット学習はより極端なパラダイムで、ネットワークがWord2Vec [21]やGloVe [22]のような入力と出力のベクトル埋め込みを使用して、記述的属性に基づいて画像を分類するものである。

上記の手法とは対照的に、データオーグメンテーションは、問題の根源である学習データセットからオーバーフィットにアプローチします。 これは、オーグメンテーションによって元のデータセットからより多くの情報を抽出できるという前提の下で行われる。 これらの拡張は、データの反りやオーバーサンプリングによって、学習データセットのサイズを人為的に増大させます。 データのゆがみ補正は、既存の画像をラベルが保存されるように変換します。 これには、幾何学的変換や色変換、ランダム消去、敵対的トレーニング、ニューラルスタイル転送などの拡張が含まれます。 オーバーサンプリング拡張では、合成インスタンスを作成し、それをトレーニングセットに追加します。これには、画像の混合、特徴空間の拡張、ジェネラティブな敵対的ネットワーク（GAN）が含まれます。 オーバーサンプリングとデータ・ワープは、互いに排他的な二分法ではありません。 例えば、GANサンプルは、データセットをさらに膨らませるために、ランダムクロッピングと重ねることができます。最終的なデータセットのサイズ、テスト時間の増大、カリキュラム学習、解像度の影響に関する決定は、この調査では「画像データ増大の設計上の考慮事項」のセクションで取り上げています。個々の補強技術の説明は、「画像データ補強の技術」のセクションで列挙します。[図２]にデータオーグメンテーションの簡単な分類を示します。

---

![図2](https://raw.githubusercontent.com/rurusasu/paper/master/AI%E6%8A%80%E8%A1%93/%E7%94%BB%E5%83%8F%E3%82%AA%E3%83%BC%E3%82%AE%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%86%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3/A%20Survey%20on%20Image%20Data%20Augumentation%20for%20Deep%20Learning/%E7%94%BB%E5%83%8F/%E5%9B%B32.png)

図2：図中の色付きの線は、対応するメタ学習スキームがどのデータ増強手法を使用しているかを示しており、例えば、ニューラル・スタイル・トランスファーを用いたメタ学習はニューラル増強[36]でカバーされています。

---

画像増強技術について議論する前に、問題の背景を整理し、そもそも画像認識を困難なものにしている理由を考えることが有用です。 猫と犬のような古典的な識別例では、画像認識ソフトウェアは、視点、照明、オクルージョン、背景、スケールなどの問題を克服しなければなりません。**データオーグメンテーションの課題は、これらの並進不変性をデータセットに焼き込むことで、結果として得られるモデルがこれらの課題にもかかわらず良好なパフォーマンスを発揮するようにすることです。**

データセットが大きいほど、より良いディープラーニングモデルが得られるというのは、一般的に受け入れられている考え方です[23, 24]。 しかし、膨大なデータセットを組み立てるのは、データの収集とラベル付けの手作業のため、非常に困難な作業になる可能性があります。限られたデータセットは、医療画像解析において特に一般的な課題です。 ビッグデータを考慮すると、深層畳み込みネットワークは、Estevaら[25]によって実証されたように、皮膚病変の分類などの医用画像解析タスクに対して非常に強力であることが示されています。 このことから、肝臓病変の分類、脳スキャンの解析、皮膚病変の分類などの医用画像解析タスク[26]でのCNNの使用に触発され、継続的な研究が行われています。研究されている画像の多くはコンピュータ断層撮影（CT）や磁気共鳴画像（MRI）スキャンから得られたものであるが，どちらも収集には高価で手間がかかる． 疾患の希少性、患者のプライバシー、ラベリングには医療専門家が必要であること、医療画像処理に必要な費用と手作業が必要であることなどから、大規模な医用画像データセットを構築することは非常に困難である。 これらの障害により、医用画像分類の応用の観点から、画像データオーグメンテーション、特にGANベースのオーバーサンプリングに関する多くの研究が行われてきました。

データオーグメンテーションの有効性に関する多くの研究では、ベンチマークのために人気のある学術的な画像データセットを利用しています。これらのデータセットには、MNISTの手書き数字認識、CIFAR-10/100、ImageNet、tiny-imagenet-200、SVHN (ストリートビューハウスナンバー)、Caltech-101/256、MIT places、MIT-Adobe 5K dataset、Pascal VOC、Stan-ford Carsなどがあります。最もよく議論されているデータセットは、CIFAR-10、CIFAR-100、Ima-geNetである。 オープンソースのデータセットの拡大により、研究者はデータオーグメンテーション技術の性能結果を比較するための様々なケースを手に入れることができます。ImageNetのようなこれらのデータセットのほとんどはビッグデータに分類されます。多くの実験では、限られたデータ問題をシミュレートするために、データセットのサブセットに自分自身を制約しています。

限られたデータセットに焦点を当てることに加えて、クラスの不均衡の問題と、Data Augmentationがどのようにして有用なオーバーサンプリングの解決策になるかについても検討します。 クラス不均衡とは、多数派と少数派のサンプルの比率が偏っているデータセットのことです。Leevyら[27]は、データタイプ間の高クラス不均衡に対する既存の解決策の多くを記述しています。 我々の調査では、画像データにおけるクラスバランスのオーバーサンプリングが、どのようにしてデータオーグメンテーションを用いて行われるかを示します。

ディープラーニングやニューラルネットワークモデルの多くの側面は、人間の知性との比較を描いています。 例えば、転移学習の人間の知性の逸話は、音楽の学習で例示されています。2人の人がギターの弾き方を学ぼうとしていて、1人がすでにピアノの弾き方を知っている場合、ピアノを弾く人の方が早くギターを弾けるようになる可能性が高いと思われます。 音楽学習と同様に、Ima-geNet画像を分類できるモデルは、ランダムな重みを持つモデルよりも、CIFAR-10画像上でより良いパフォーマンスを発揮する可能性が高い。

データオーグメンテーションは想像力や夢想に似ている。 人間は経験に基づいて異なるシナリオを想像します。 想像力は、私たちの世界をよりよく理解するのに役立ちます。 GANやNeural Style Transferのようなデータオーグメンテーションの手法は、画像をよりよく理解できるように画像の改変を「想像」することができます。本論文の残りの部分は以下のように構成されている。 読者にデータ拡張とディープラーニングの歴史的背景を与えるために、簡単な「背景」が提供されていま。 「画像データ拡張技術」では、実験結果とともに各画像拡張技術について詳細に説明しています。"Design considerations for image Data Augmentation "では、テスト時間の増大や画像解像度の影響など、増大処理の追加的な特徴について議論しています。本論文は、事前に提出された資料の「ディスカッション」、「今後の課題」、「結論」で締めくくられています。


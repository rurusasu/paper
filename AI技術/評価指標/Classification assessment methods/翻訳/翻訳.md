# Classification assessment methods

# 備考
## 著者
Alaa Tharwat

## 掲載
''Classification assessment methods," {\itApplied Computing and Informatics}, Vol. 16, No. 2, 25 pages, 2020.

# Abstract

分類技術は、様々な科学分野で多くの応用がなされています。分類アルゴリズムの評価にはいくつかの方法があります。このような指標の分析とその重要性は、異なる学習アルゴリズムを評価するために正しく解釈されなければならない。これらの尺度の多くはスカラーメトリクスであり、その中にはグラフィカルな手法を用いたものもある。本論文では、この分野に関心のある研究者のための総合的な資料となることを目的として、分類評価指標の詳細な概要を紹介します。この概説は、まず、2値分類問題や多クラス分類問題における混同行列の定義を強調することから始まります。また、多くの分類尺度についても詳細に説明し、各尺度におけるバランスのとれたデータと不均衡なデータの影響を示す。例示的な例が紹介され、(1)2値分類問題や多値分類問題におけるこれらの尺度の計算方法、(2)バランスのとれたデータや不均衡なデータに対するいくつかの尺度のロバスト性を示しています。さらに、受信機動作特性(ROC)、精度-リコール、検出誤差トレードオフ(DET)曲線のようないくつかのグラフィカルな測定値が詳細に示されています。さらに、ROC,PR,DET曲線をプロットする前処理の手順を説明するために、ステップバイステップのアプローチで、異なる数値例を示している。

# 2. Classification

評価方法は、分類器の性能を評価し、分類器のモデル化を導く上で重要な要素です。分類プロセスには大きく分けて、訓練段階、検証段階、テスト段階の3つの段階があります。モデルは入力パターンを用いて訓練され、このフェーズを訓練フェーズと呼ぶ。これらの入力パターンは、モデルを訓練するために使用される訓練データと呼ばれます。この段階では、分類モデルのパラメータが調整されます。訓練誤差は、訓練されたモデルが訓練データにどれだけ適合するかを測定します。しかし、訓練されたモデルが訓練段階で使用される同じデータに適合するため、訓練誤差は常にテスト誤差や検証誤差よりも小さくなります。学習アルゴリズムの目標は、学習データから学習して、目に見えないデータのクラスラベルを予測することです。しかし、テストサンプルのクラスラベルや出力が未知であるため、テスト誤差やサンプル外誤差を推定することはできません。これが，検証フェーズが訓練されたモデルの性能を評価するために使用される理由です．検証段階では，検証データは，モデルのハイパーパラメタを調整しながら，訓練されたモデルの偏りのない評価を提供する．

クラスの数に応じて、クラスが2つしかない2クラス分類と、クラスの数が2つより多い多クラス分類の2種類の分類問題があります。2つのクラス、すなわち、正のクラスを$P$、負のクラスを$N$とする2値分類があるとします。未知のサンプルは、$P$または$N$に分類され、学習段階で学習された分類モデルは、未知のサンプルの真のクラスを予測するために使用されます。この分類モデルは、連続または離散の出力を生成します。分類モデルから生成される離散出力は、未知/テストサンプルの予測された離散クラス・ラベルを表し、連続出力はサンプルのクラス・メンバーシップ確率の推定を表します。

---

![図1](https://raw.githubusercontent.com/rurusasu/paper/master/AI%E6%8A%80%E8%A1%93/%E8%A9%95%E4%BE%A1%E6%8C%87%E6%A8%99/Classification%20assessment%20methods/%E7%94%BB%E5%83%8F/%E5%9B%B31.png)

図1:2x2混同行列の例を示します。2つの真のクラスPとNがあり，予測されたクラスの出力は真か偽かである．

---

[図1]は、2x2混同行列または分割表の要素を表す4つの可能な出力があることを示しています。 **緑の対角線は正しい予測値を、ピンクの対角線は正しくない予測値を表しています。** サンプルが陽性であり，陽性に分類された場合，すなわち正しく分類された陽性サンプルは，真の陽性(TP)としてカウントされ，陰性に分類された場合は，偽陰性(FN)またはタイプIIエラーとみなされます．サンプルが陰性であり、陰性に分類された場合は真陰性(TN)とみなされ、陽性に分類された場合は偽陽性(FP)、誤報、またはタイプIエラーとみなされます。次のセクションで紹介するように、混同行列は、多くの一般的な分類メトリックを計算するために使用されます。

---

![図2](https://raw.githubusercontent.com/rurusasu/paper/master/AI%E6%8A%80%E8%A1%93/%E8%A9%95%E4%BE%A1%E6%8C%87%E6%A8%99/Classification%20assessment%20methods/%E7%94%BB%E5%83%8F/%E5%9B%B32.png)

図2：マルチクラス分類テストのための混同行列の例。

---

図2は、3つのクラス（A,B,C）を持つマルチクラス分類問題の混同行列を示している。このように、TPAはクラスAの真の陽性サンプル数、すなわちクラスAから正しく分類されたサンプル数であり、EABはクラスAからクラスBとして誤って分類されたサンプル数、すなわち誤分類されたサンプル数である。したがって、Aクラスの偽陰性$(FN_A)$は$E_{AB}$と$E_{AC}$の和 $(FN_A=E_{AB}+E_{AC})$ であり、これはクラスBまたはCとして誤って分類されたすべてのクラスAのサンプルの和を示している。一方、行に位置する予測されたクラスの偽陽性$FN$は、その行のすべてのエラーの合計を表します。例えば、クラスA $(FP_A)$ の偽陽性は次のように計算されます $(FP_A = E_{BA}+E_{CA})$ 。$m \times m$の混同行列では、$m$個の正しい分類と$m^2-m$の誤りの可能性のあるがある[22]。

# 2.1. Classification metrics with imbalanced data.

あるデータセットのあるクラスのサンプル数が他のクラスのサンプル数を上回っている場合には，異なる評価方法が不均衡なデータに敏感に反応します[25]．これを説明するために，図1の混同行列を考えてみましょう．クラス分布は、正のサンプルと負のサンプルの間の比率 $\frac{P}{N}$ で、左列と右列の間の関係を表します。 **両方の列からの値を使用する評価メトリックは、[8]で報告されているように、不均衡なデータに敏感になります。** 例えば，accuracy や precision のような評価指標の中には，混同行列の両列の値を使用しているものがあります．したがって，**このようなメトリクスでは，異なるクラスからの修正ラベルの数を区別することはできません[11]。**  幾何平均（GM）やユーデンの指数（YI）のように、両方の列の値を使用するメトリクスがあり、これらのメトリクスはバランスの取れたデータと不均衡なデータで使用できるので、この事実は部分的に正しいです。これは、片方の列の値を用いるメトリクスは、クラス分布の変化を打ち消すと解釈することができる。しかし、両カラムの値を用いるメトリクスの中には、クラス分布の変化が相殺されてしまうため、不均衡データの影響を受けないものもある。例えば、精度は$Acc=\frac{TP+TN}{TP+TN+FP+FN}$、$GM$は以下のように定義される。

$$
GM = \sqrt{TPR \times TNR} = \sqrt{\frac{TP}{TP+FN} \times \frac{TN}{TN+FP}}
$$

したがって，両方のメトリクスは混同行列の両列からの値を使用します．クラス分布の変更は，ネガティブクラス/ポジティブクラスのサンプル数を増減させることで得ることができる．同じ分類性能で、負のクラスのサンプル数を$\alpha$ 倍に増やしたと仮定すると、$TN$と$FP$の値はそれぞれ$\alpha TN$と$\alpha FP$になり、精度は次のようになります。

$$
ACC = \frac{TP+\alpha TN}{TP +\alpha TN + \alpha FP +FN}
\neq
\frac{TP +TN}{TP+TN+FP+FN}
$$

つまり、クラス分布の変化によって精度が左右されることになります。一方、$ＧＭ$ メトリックは

$$
GM = \sqrt{\frac{TP}{TP+FN} \times \frac{\alpha TN}{\alpha TN +\alpha FP}}
=
\sqrt{\frac{TP}{TP+FN}\times \frac{TN}{TN+FP}}
$$

となり、負のクラスの変化は互いに打ち消し合うことになります。これが$GM$メトリックが不均衡データに適している理由です。同様に、どのようなメトリックでも、それが不均衡なデータに対して敏感かどうかを調べることができます。

# 2.2. Accuracy and error rate

精度（Acc）は、分類性能の指標として最も一般的に用いられるものの一つであり、以下のように、正しく分類されたサンプル数と総サンプル数の比として定義されている[20]。

$$
ACC = \frac{TP+TN}{TP+TN+FP+FN}
$$

ここで、ＰおよびＮはそれぞれ陽性サンプル数および陰性サンプル数を示す。

精度指標の補数は、エラー率(ERR)または誤分類率である。このメトリックは、正と負の両方のクラスからの誤分類されたサンプルの数を表し、次のように計算されます、$EER=1-Acc=\frac{FP+FN}{TP+TN+FP+FN}$[4]。精度とエラー率の両方のメトリクスは、不均衡なデータに対して敏感である。精度に関するもう1つの問題は，2つの分類器が同じ精度を得ることができても，それらが提供する正誤判定の種類に関して異なる性能を発揮することである[9]．しかし，Takaya SaitoとMarc Rehmsmeierは，彼らの例でバランスのとれたデータと不均衡なデータの精度値が同じであることを発見したため，精度は不均衡なデータに適していると報告しています[17]．彼らの例で精度値が同一であった理由は、バランスデータと不均衡データの $TP$ と $TN$ の和が同一であったためである。

# 2.3. Sensitivity and specificity

感度、真正率（TPR）、ヒット率、またはリコールは、分類器の正のサンプルの総数に対する正に分類されたサンプルの割合を表しており、式（2）[20]に従って推定される。一方、特異度、真陰性率(TNR)、または逆リコールは、式(2) [20]のように、陰性サンプルの総数に対する正しく分類された陰性サンプルの比率で表されます。したがって、特異度は正しく分類された陰性検体の割合を表し、感度は正しく分類された陽性検体の割合を表す。一般的に、感度と特異度は2種類の精度と考えることができ、第1は実際の陽性サンプルに対するものであり、第2は実際の陰性サンプルに対するものである。感度は混同行列の同じ列にあるTPとFNに依存し，同様に特異度は同じ列にあるTNとFPに依存する．

$$
TPR = \frac{TP}{TP+FN} = \frac{TP}{P}
$$
$$
TNR = \frac{TN}{FP+TN} = \frac{TN}{N}
$$

また、精度は、感度と特異度の観点から以下のように定義することができる[20]。

$$
ACC = \frac{TP+TN}{TP+TN+FP+FN} \\
= TPR \times \frac{P}{P+N}+TNR \times \frac{N}{P+N} \\
=\frac{TP}{TP+EN}\frac{P}{P+N}+\frac{TN}{TN+FP}\frac{N}{P+N}\\
=\frac{TP}{P+N}+\frac{TN}{P+N} \\
=\frac{TP+TN}{TP+TN+FP+FN}
$$

# 2.4. False positive and false negative rates

偽陽性率（FPR）は偽警報率（FAR）またはフォールアウトとも呼ばれ、陰性サンプルの総数に対する不正確に分類された陰性サンプルの比率を表します[16]。言い換えれば、不正確に分類された陰性サンプルの割合です。したがって、式(4)の特異度を補完するものである[21]。偽陰性率（FNR）またはミス率は、誤って分類された陽性サンプルの割合です。したがって、これは感度測定を補完するものであり、式(5)で定義されています。FPRとFNRの両方ともデータ分布の変化には敏感ではなく、したがって、両方のメトリックは不均衡なデータで使用することができます[9]。

$$
FPR = 1-TNR = \frac{FP}{FP+TN} = \frac{FP}{N}
$$
$$
FNR = -TPR = \frac{FN}{FN+TP}=\frac{FN}{P}
$$

# 2.5. Predictive values

正の予測値（PPV）または精度は、式（6）[20]で示されるように、正の予測されたサンプルの総数に対 して正のサンプルが正しく分類された割合を表している。一方、陰性予測値（NPV）、逆精度、または真陰性精度（TNA）は、式（7）[16]で示された陰性予測された検体の総数に対する陰性検体の割合を測定します。これら2つの測定値は、不均衡なデータに対して敏感である[21,9]。偽発見率（FDR）は PPV、偽省略率（FOR）は NPV を補完する尺度である（式(6)、(7)参照）。


# 2.9. Illustrative example

このセクションでは、2つの例を紹介します。これらの例は、2 つのクラスまたは複数のクラスを使用して、分類メトリックを計算する方法を説明します。

## 2.9.1  Binary classification example.

この例では、2つのクラス（AとB）、すなわち2値分類があり、各クラスのサンプル数が100個であると仮定します。Aクラスは正のクラスを表し、Bクラスは負のクラスを表します。AクラスとBクラスの正しく分類されたサンプル数はそれぞれ70個と80個です。したがって、TP；TN；FP；FNの値は、それぞれ70、80、20、30である。異なる分類メトリックの値は以下の通りである。


## 2.9.2. Multi-classification example.

---

![図5]()

---

この例では、A,B,Cの3つのクラスがあり、分類テストの結果を図4に示します。図から、$TP_A$、$TP_B$、$TB_C$の値はそれぞれ80、70、90となり、図4の対角線を表している。


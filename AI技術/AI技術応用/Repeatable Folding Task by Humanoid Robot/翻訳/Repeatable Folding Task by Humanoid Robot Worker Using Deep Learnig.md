# Repeatable Folding Task by Humanoid Robot Worker Using Deep Learnig

# Abstract
生産ラインの労働者として働くことができる機械学習ベースのヒューマノイドロボットを開発するための実用的な最先端の方法を提案します。提案されたアプローチは、データを収集する直観的な方法を提供し、次の特性を示します。タスク実行機能、タスク反復機能、一般化可能性、および容易な適用可能性。提案されたアプローチは、モニターを備えたリアルタイムユーザーインターフェースを利用し、ヘッドマウントディスプレイを使用して一人称視点を提供します。このインターフェースを介して、特に従来の方法では適用が困難なタスクのタスク操作データを収集するために、テレオペレーションが使用されます。提案されたアプローチでは、2段階の深層学習モデルも利用されます。ディープコンボリューショナルオートエンコーダーは画像の特徴を抽出して画像を再構築し、完全に接続されたディープタイムディレイニューラルネットワークは、抽出された画像の特徴と運動角度信号からロボットタスクプロセスのダイナミクスを学習します。「Nextage Open」ヒューマノイドロボットは、提案されたモデルを評価するための実験プラットフォームとして使用されます。テスト用に35の訓練された感覚モーターシーケンスと5つの訓練されていない感覚モーターシーケンスを使用したオブジェクト折りたたみタスク。オンライン生成を使用してトレーニング済みモデルをテストすると、オブジェクト折りたたみタスクの成功率が77.8％であることが実証されています。

# 備考
[元論文](https://github.com/rurusasu/paper/blob/master/AI%E6%8A%80%E8%A1%93/AI%E6%8A%80%E8%A1%93%E5%BF%9C%E7%94%A8/%E5%85%83%E8%AB%96%E6%96%87/Repeatable%20Folding%20Task%20by%20Humanoid%20Robot%20Worker%20Using%20Deep%20Learnig.pdf)

# 要約
以下の目標を達成できるような機械学習と遠隔操作手法を用いたロボットの制御について提案している．
1. タスク機能：<br>ロボットには十分な知識があり、特定のタスクを実行できます。<br>→**Deep Learning(DL)を使用．**
2. 反復能力：<br>ロボットはタスクを繰り返し実行できます。<br>**→すべてのシーケンスは同じポーズで開始・終了する．**
3. 一般化可能性：<br>ロボットは、複数のタスクと類似のオブジェクトを使用して同じタスクを実行できます。<br>**→DLを使用．**
4. 工場のロボット（非後方駆動ロボット）への容易な適用性：<br>提案された方法は、制限がほとんどなく、簡単に適用できるはずです。<br>**→遠隔操作技術を使用してロボットの構成を無視できるデータを取得．**

オートエンコーダで圧縮した特徴とモータ角度を，映画のテープのように時間単位でTime-Delay Neural Network(TDNN)へ渡すことによってロボットに動作を学習させる．

Trainig Data
| Data | params |
|  --- | --- |
| image | 112x112x3 |
| モータ角度 | 12 DOF |
| グリッパ信号 | 2 DOF |
| データモデル | 約28,000 step |

## 著者
Pin-Chu Yang, Kazuma Sasaki, Kanata Suzuki, Kei Kase, Shigeki Sugano, and Tetsuya Ogata

## 掲載
IEEE ROBOTICS AND AUTOMATION LETTERS, VOL. 2, NO. 2, APRIL 2017.

# 1. Introduction
少子化と長寿化が進む中で、将来的には人工妊娠中絶の不足が予想されます。人手不足は効率性の低下とコスト増を招き、消費者に不可欠な製品を生産するメーカーはコスト増に耐えられなくなる可能性があり、最終的には社会全体に影響を及ぼすことになります。また、市場の需要の変化に伴い、製造業では少量多品種生産への転換が進んでいます。 したがって、さまざまな製品を扱うことができる生産設備がさらに必要になります[1]。そのような施設は、より柔軟で、製品ごとに異なるタスクを実行できる必要があります。これを達成するために、複数の自由度アームを備えたロボット、画像認識システム、および事前に設計されたモーションシーケンスを備えた大規模データベースが、可能なソリューションを表しています。ただし、このソリューションは、同様のタイプの製品を製造または分解する工場には適さない場合があります。現在の事前設計方法を利用して、これらのジョブを人間の労働者から置き換えることはほぼ不可能です。同様のタイプの製品を生産する場合、製造タスクは反復的であり、限られた人間の知能を必要とする傾向があります。したがって、将来の労働力不足がこの種の生産に影響を与えることが予想されます。

最近、「深い」構造を持つ人工ニューラルネットワーク学習モデル（つまり、ディープニューラルネットワーク）は、画像と音声の認識で成功を収めています。この機械学習法は、ロボットのタスクに適用されることが期待されています。ただし、非構造化環境の機械学習モデルを構築するには、かなりのトレーニングデータが必要です。

通常、データの収集には直接教示と模倣学習の方法が利用されます。ただし、ハードウェアの制限と制御の問題により、タスクに適したシステムの構築が妨げられる可能性があります。この研究の対象は、人間の労働者がいる生産ラインなど、不確実な環境でタスクを実行できるロボットを制御するための機械学習ベースのモデルです。したがって、実用的なアプリケーション、運用システムの利便性、繰り返しタスクを自動的に実行する機能、および複数のタスクを実行する機能を考慮する必要があります。
これらの要件を満たすために、次の目標を設定します。
1. **タスク機能：**<br>ロボットには十分な知識があり、特定のタスクを実行できます。
2. **反復能力：**<br>ロボットはタスクを繰り返し実行できます。
3. **一般化可能性：**<br>ロボットは、複数のタスクと類似のオブジェクトを使用して同じタスクを実行できます。
4. **工場のロボット（非後方駆動ロボット）への容易な適用性：**<br>提案された方法は、制限がほとんどなく、簡単に適用できるはずです。

パフォーマンスを評価し、実用的なケースをテストするために、このタイプのタスクを達成するための事前に設計されたパイプラインメソッドの難しさから、ソフトオブジェクトの折りたたみがタスクとして選択されました。

前のセクションで述べたように、折り畳みタスクは、動的な環境情報に起因するロボットによる折り畳みプロセス中の予測できない変化に対処するために使用できる、よく知られた困難な操作タスクです。一般化および反復能力は、トレーニングデータで決して使用されないオブジェクトでテストできます。このタスクは、提案されたモデルの有効性を実証するために、特定のセットアップとキャリブレーションのない状況で使用されます。実験結果は、提案されたモデルがスマートロボットワーカーの実現に有望であることを示しており、さまざまなタスクに適用できることが期待されます。

残りのペーパーは次のように構成されています。2章では、行われた関連作業を紹介します。このセクションでは、このペーパーの課題と貢献についても説明します。3章では、この調査のアプローチを紹介します。これには、データ収集プロセス、モデルのアーキテクチャ、およびトレーニング方法の紹介が含まれます。4章では、ロボットの動き、トレーニングデータセットの収集に使用されるオブジェクト、モデルトレーニングのパラメーター、および結果を含む実験設定について説明します。５章では、タスク実行のパフォーマンスを向上させるための堅牢性と可能な方法について説明します。6章で結果をまとめ、提案されたアプローチの有効性を示します。

# 2. RELATED WORK
ロボットが物体の把持やボルトの挿入などの操作タスクを実行できるかどうかが調査されています[2]。衣服を折りたたむ[3]、[4]、ケーブルを配線する[5]、[6]など、より複雑な作業も研究されています。これらの研究は、こうしたタスクがさまざまなアプローチで実行できることを実証しています。ただし、これらの方法の成功率は通常、人間が設計した制御、画像特徴抽出、および環境に依存します。以前の研究で提案されたアプローチは、複雑で不確実な状況を特徴とする環境に適用するのが難しい作業である可能性があります。

研究者は、操作タスクの方法を開発し、ある種のスマートコントロールを組み込むことを試みました。深層学習法は、静的画像認識に適用されています[7]。この方法は、人間の認識能力よりもかなり高い認識率を達成しました。超解像畳み込みネットワーク[8]は、インターネットベースのwaifu2x超解像サービス[9]に影響を与えました。ディープラーニングは、たとえば自動運転車の研究[10]や未定義の物体把握[11]〜[13]など、シーケンシャルデータのトレーニングにも使用されています。

機械学習法は、ロボットのタスクにも適用されています[14]。この研究では、軌道ポリシーとロボットアームアクチュエータのトルク信号をトレーニングする強化学習法を提案しました。野田ら[15]は、複数の行動の学習と複数のモダリティからの情報を自動的に考慮するモデルを提案しました。彼らのモデルは、2つの完全に接続されたニューラルネットワーク、つまり画像特徴抽出ネットワークと動的学習モデルを組み合わせたものです。野田ら趣味のサイズのヒューマノイドロボットであるNAOを使用して、複数の周期的な運動動作にモデルを適用しました。鈴木や野田らによって提案されたものと同様の構造を採用した。そして、ソフトオブジェクトの折りたたみタスクにPR2ロボットを使用することに成功しました[16]。ただし、それらの方法は反復可能なタスクを実行できず、[16]、[15]は両方とも、逆駆動可能なロボットにのみ適用できるダイレクトティーチング方法を使用します。さらに、これらの方法では、感覚運動データの有効性を判断することは困難です。

本研究では、インタラクティブなロボット環境情報（感覚運動情報）に対する動的情報の影響に焦点を当て、提案手法を評価するためにヒューマノイドロボットと折り畳みタスクを使用しています。私たちのアプローチに基づいて、産業レベルのヒューマノイドロボットがパイプライン方式ではなく高い適応性を必要とするタスクを実行できるようにすることは効果的です。さらに、当社のアーキテクチャにより、強化学習の評価関数を設計する労力を排除し、実験者の操作経験からの直接学習に置き換えることができます。

# 3. APPROACH
不確実な環境で動作する適応可能なタスク実行ヒューマノイドロボットに適用できる深層学習方法を実現するアプローチを提示します。目標1.および3.では、不確実な環境に適した十分に高い一般化可能性を実現するために、ロボットから取得した感覚運動情報を学習するためにディープラーニングが適用されます。その情報は、提案されたモデルにタスク操作を実行することを許可します。深層学習で目標2.を達成するために、すべてのシーケンスは同じロボットポーズで開始および終了するように設計されています。目標4.では、遠隔操作技術を使用して、ロボットの構成を無視できるデータを取得します。

ロボットタスクにディープラーニングを適用するには、（1）データ収集、（2）トレーニング、（3）タスク生成フェーズが必要です。アプローチのフローを図1に示します。「Teleoperation Training」、「Learning Model」、および「GenerateMotions for Tasks」は、それぞれデータ収集、トレーニング、およびタスク生成フェーズに対応しています。

![Process approach](https://raw.githubusercontent.com/rurusasu/paper/master/AI%E6%8A%80%E8%A1%93/AI%E6%8A%80%E8%A1%93%E5%BF%9C%E7%94%A8/Repeatable%20Folding%20Task%20by%20Humanoid%20Robot/%E7%94%BB%E5%83%8F/Process%20approach.png)\
画像1. 提案されたアプローチのプロセスは、主に3つのフェーズに分かれています。

## A. Data Collection Phase
![Sensory-motor experiece sharing](https://raw.githubusercontent.com/rurusasu/paper/master/AI%E6%8A%80%E8%A1%93/AI%E6%8A%80%E8%A1%93%E5%BF%9C%E7%94%A8/Repeatable%20Folding%20Task%20by%20Humanoid%20Robot/%E7%94%BB%E5%83%8F/Sensory-motor%20experience%20sharing.bmp)
図2.感覚運動体験の共有：リモートモニタリングまたはヘッドマウントディスプレイを使用して、オペレーターはロボットを直接操作し、同じ感覚を共有して適切な感覚運動体験（データ）を取得できます。これにより、収集された感覚運動データが人間の感覚で動作し、ディープラーニングに効果的であることが期待されます。

データ収集はディープラーニングの重要なステップであり、正確なタイミングのモーションを必要とするタスクには特に重要です。いくつかの深層学習の研究では、ロボットアームがタスクを実行するように誘導するために直接教育を使用しています。ただし、直接教示は、特殊な製品の製造に通常使用される逆駆動可能なロボットにのみ適用できます。このようなロボットは、通常は逆駆動できないロボットのみを装備し、実際のタスク操作のための機能を必要とする工場にとって非常に高価になる可能性があります。現在の研究の目的は、ロボットの制限なしに効果的なデータ収集方法を提案することです。

1) 遠隔操作訓練：粟野らの遠隔操作法を適用してトレーニングデータを収集し、人間ロボットの協調行動を実現しました[17]。現在の研究では同じ手法を利用しており、特にあらゆる種類のロボット、特に後進できないロボットに適用できます。

テレオペレーション(遠隔操作技術)には、完全に自律的なものから完全な手動制御まで、および混合イニシアチブ相互作用まで、あらゆる量の制御が含まれます。このような複合コマンドプロシージャは、データ収集に利点をもたらします。完全に自律的なコマンドシステムは、事前に設計された動作と動作を利用して、予測可能な動作のプログラミング時間を短縮できます。半自律コマンドは、軍隊が使用する誘導ミサイルなどの誘導情報を必要とする自己自律的な動作を指します。半自律的なコマンドプロセスは、高精度を必要とする動作に対して満足のいくロボットの動作を提供します。手動制御では、人間のオペレーターがロボットのアクチュエーターを直接制御してタスクを実行します。

センサー信号や画像データなどの一部のデータは、遠隔操作中に収集できます。これらのシーケンシャルデータは、異なる自律コマンドレベルでロボットから直接収集されます。さらに、図2に示すように、ロボットモーターの角度を含む感覚運動データと、ロボットに取り付けられたカメラでキャプチャされた画像データがトレーニングフェーズで収集されます。

## B Traning Model Phase
![Orverview of the model](https://raw.githubusercontent.com/rurusasu/paper/master/AI%E6%8A%80%E8%A1%93/AI%E6%8A%80%E8%A1%93%E5%BF%9C%E7%94%A8/Repeatable%20Folding%20Task%20by%20Humanoid%20Robot/%E7%94%BB%E5%83%8F/Orverview%20of%20the%20model.bmp)\
図3. モデルの概要：（a）aDCAEは画像を抽出し、特徴情報を圧縮できます。モデル構造の半分（中間層から出力層）は、抽出された画像の特徴に情報をデコード（再構築）するために使用されます。 （b）TDNNは、ロボットから取得した抽出された画像の特徴とモーションデータのウィンドウサイズのステップで時系列データを学習および生成します。


収集されたデータは、この研究で提案された深層学習モデルのトレーニングに使用されます。モデルは、図3に示すように2つの部分で構成されています。この2段階のEnd-to-Endトレーニングプロセスにより、提案されたモデルは生の入力データを適応的に処理して、環境の小さな変化を処理し、出力コマンド信号から対応する動作を実行できます。

1) **Deep Convolutional Autoencoder(DCAE):**\
   畳み込みニューラルネットワーク（CNN）は、特に画像認識のための強力な画像処理ツールです。CNNにはスライディングフィルターが含まれています。これは、空間的に局所的な入力パターンに対する強力な応答を活用し、入力画像全体をカバーできる生体細胞に似ています。CNNは、より少ないパラメーターを使用しながら、完全に接続されたニューラルネットワークよりもかなり多くの入力ディメンションを処理できます。これにより、トレーニング時間が大幅に短縮され、画像処理または同様のデータ入力のパフォーマンスが向上します。さらに、深い畳み込み層構造のモデルは、エッジから画像の部分的な部分まで、さまざまなレベルの特徴にデータを抽出できます。この研究では、畳み込み層を使用して、高解像度の画像を小さなサイズの特徴マップに処理できるDCAEを提示しました。ストライドのある畳み込みレイヤーは、特徴を抽出し、情報の次元をダウンサンプリングできます。デコンボリューションレイヤーは、エンコードされたフィーチャマップから画像を再構築するために使用されます。訓練されたDCAEでは、モデル構造の半分（中間層への入力層）を使用して、元の入力画像と比較して、情報を小サイズの画像特徴にエンコード（圧縮）します。これらのエンコードされた画像機能は、入力画像の状態を表し、より少ない次元で高解像度の入力情報を提供できます。バッチ正規化は、学習を最適化し、問題の過剰適合の可能性を減らすために使用されます。DCAE構造を表Iに示します。ネットワークは、出力画像層で入力画像データを再構築するために訓練されます。この研究では、DCAEのトレーニングデータは、ロボットに取り付けられたカメラから取得した連続画像を利用します。各入力画像のターゲットは元の入力データであり、平均二乗誤差（MSE）は、Adam最適化[18]を使用してニューラルネットワークの重みを変更するために使用されます。

   ![Table1 DCAE](https://raw.githubusercontent.com/rurusasu/paper/master/AI%E6%8A%80%E8%A1%93/AI%E6%8A%80%E8%A1%93%E5%BF%9C%E7%94%A8/Repeatable%20Folding%20Task%20by%20Humanoid%20Robot/%E7%94%BB%E5%83%8F/Table1%20DCAE.png)


2) Time-Delay Neural Network:\
   TDNNは、時間ステップウィンドウ[19]で時系列データ用にトレーニングされた完全に接続されたフィードフォワードニューラルネットワークです。野田らマルチモダリティ信号でTDNNを使用し、趣味のサイズのロボットであるNAOでマルチ動作を実現したモデルを提案しました[15]。深層構造のレイヤーを持つTDNNは、シーケンシャルデータを正常に再構築でき、入力情報をシフトすることで連続シーケンスを生成することができます。TDNNは、時間の経過とともに入力ウィンドウをシフトし、抽出された画像（カメラ画像）の特徴と動き（モーター角度）をリアルタイムで繰り返し入力することで実行されるオンライン生成に使用できます。

   この研究で使用したTDNNの構造を表IIに示します。TDNNは、複数の感覚運動信号入力を使用して逐次情報を学習できます。DCAEおよびロボットの動きから抽出された画像の特徴は、TDNNモデルに適用されます。TDNNの入力は、データセットからのデータの固定ウィンドウサイズステップです。トレーニング中、各入力データのターゲットは元の入力データであり、MSEはAdam最適化を使用して重みを変更するために使用されます。TDNNトレーニングデータセットは、トレーニングデータを経時的にスライドさせて作成されます。

   ![Table2 TDNN](https://raw.githubusercontent.com/rurusasu/paper/master/AI%E6%8A%80%E8%A1%93/AI%E6%8A%80%E8%A1%93%E5%BF%9C%E7%94%A8/Repeatable%20Folding%20Task%20by%20Humanoid%20Robot/%E7%94%BB%E5%83%8F/Table2%20TDNN.png)

## C. Task Generation Phase
![Online generation](https://raw.githubusercontent.com/rurusasu/paper/master/AI%E6%8A%80%E8%A1%93/AI%E6%8A%80%E8%A1%93%E5%BF%9C%E7%94%A8/%E7%94%BB%E5%83%8F/Repeatable%20Folding%20Task%20by%20Humanoid%20Robot/Online%20generation.bmp)\
画像4. オンライン生成は、抽出された画像の特徴や動きをリアルタイムでTDNNに継続的に入力しながら、ウィンドウを時間とともにスライドさせることで実行されます。 （a）：結合された順次情報がステップ（b）を介してスライドします。前のモーションをコピーして、モーションの欠落の問題に対処します。


訓練された深層学習モデルは、ロボットにタスクを実行させるために、リアルタイムの感覚運動情報とともに使用されます。これはオンライン生成と呼ばれます。すべての実行中、カメラの画像はまずDCAEに送られ、画像情報が特徴ベクトルに圧縮されます。次に、これらの特徴ベクトルと関節角を、TDNNの入力のために時間をかけて設計された窓サイズで結合した。最後に、図4（a）に示すように、このウィンドウを時間をかけてスライドさせ、最後のステップの組み合わせ情報を継続的に置き換え、このスライドした順次情報をTDNNに入力して予測ステップを生成します。ここで、$I$は画像の特徴を示し、Mは対応する時間Tでの動きを示します。

これらの予測されたステップデータは、ロボットに送信される次のステップ実行コマンドに使用できます。ウィンドウサイズのシーケンシャル情報にスライドアクションを進める場合、最後のステップを実行した元のモーションがコピーされてからTDNNに送られ、スライドを実行するとモーションが失われるという問題に対処します（図4（b）。さらに、より安定したスムーズなモーションを実現するために、実行されたコマンドは、予測されたステップを直接使用するのではなく、アウトソーシング信号と予測された信号を組み合わせます。パーセンテージで表された信号の組み合わせは、次のように計算できます。

$$Signal_{exe} = P \times Signal_{out} + (1-P) \times Signal_{pre}$$

ここで、$P$は組み合わせの入力パーセンテージパラメーター、$Signal_{exe}$は実行コマンド、$Signal_{out}$はアウトソーシング信号、$Signal_{pre}$はTDNNからの予測信号です。最後に、タスクに依存する推定方法を使用して、オンライン生成結果のパフォーマンスを評価します。

# 4. EXPERIMENT
![Training Object Configurations](https://raw.githubusercontent.com/rurusasu/paper/master/AI%E6%8A%80%E8%A1%93/AI%E6%8A%80%E8%A1%93%E5%BF%9C%E7%94%A8/Repeatable%20Folding%20Task%20by%20Humanoid%20Robot/%E7%94%BB%E5%83%8F/Training%20Object%20Configurations.png)\
画像5. トレーニングオブジェクトの構成。

この研究では、KAWADA RoboticsのNextage Open Robotが実験プラットフォームとして使用されています[20]。このロボットには、バックドライブできない6つのDOFアームが2つと、正確なタスク操作用のカメラが搭載されています。ロボットは草で覆われたテーブルの前に置かれます。人工芝シートは、ロボットが制限された範囲をはるかに超えて動作するときに損傷を防ぐための緩衝領域を提供します。ここで、実験課題とは、実験者がランダムに布を配置する布折り作業です。折りたたみ課題の動作挙動を4つのトレーニングオブジェクトとともに図6に示し、その構成を図5に示します。衣服の配置位置、向き、大きさが異なるため、視覚情報はロボットの作業に大きく貢献します。

![Folding task](https://raw.githubusercontent.com/rurusasu/paper/master/AI%E6%8A%80%E8%A1%93/AI%E6%8A%80%E8%A1%93%E5%BF%9C%E7%94%A8/Repeatable%20Folding%20Task%20by%20Humanoid%20Robot/%E7%94%BB%E5%83%8F/Folding%20task.bmp)\
図6. 3つの異なる遠隔操作モードを使用して生成された折りたたみタスクの動作。モーションの詳細については、まずグラブポイント（x、yの位置）を決定し、オペレーターが操作コマンド（1〜3）を入力することで自動的にモーションが実行されます。次に、ロボットがグラブモーションを実行すると（モーションを生成してグリッパーを閉じる）、プロセスコントロールは自動的に手動制御モードに切り替わり、3Dマウスを使用してエンドエフェクターの位置を直接制御します（3–5）。最後に、オペレーターが操作を完了すると、「END」コマンドボタンが押され、ロボットがタスクを完了し、ホームポジションに戻ってアクションを完了することができます（5〜6）。シーケンスはホームポジションで終了し、タスク反復能力要件を満たすリミットサイクルアトラクタを作成します。

トレーニングモデルのトレーニングデータとテストデータは、感覚運動データです。これには、ロボットから取得したモーター角度とカメラ画像が含まれます。カメラ画像の解像度は112×112×3ch（37,632次元、RGB）で、モーター角度は12 DOF(degree of freedom：自由度)で、各グリッパー信号はDOF（2 DOF）ごとにあります。データは10 FPSで記録され、各タスクシーケンスには約70秒必要です。トレーニングモデルには、約28,000ステップのデータが使用されます。

## A. Model Training
DCAEは学習率 $\alpha = 0.0002$ 、$\beta_1= 0.75$（ADAMパラメーター）、トレーニングとテストデータのミニバッチサイズ= 200でトレーニングされます。 50,000回の反復をトレーニングするGPU計算サポートを備えたChainer [21]を使用すると、トレーニングには13,849秒（約4時間）かかります。 TDNNは、トレーニングおよびテストデータの学習率$\alpha= 0.0002$、$\beta_1= 0.7$、およびミニバッチサイズ= 250でトレーニングされます。 70,000回の反復をトレーニングするためのGPU計算サポートを備えたChainerを使用すると、トレーニングには7,864秒（約2時間）かかります。

## B. Motion Generation
最初に、トレーニングプロセス中に使用されるトレーニング済みシーケンスと未トレーニングシーケンスを生成します。トレーニングデータの画像への連続入力は、オンライン生成におけるトレーニング済みモデルのパフォーマンスを検証するために利用されます。このプロセスでは、MSE(平均二乗誤差)を使用して予測パフォーマンスを推定します。モーションの平均予測誤差は、トレーニングされた画像データの35シーケンスとテスト画像データの5つのシーケンスをそれぞれ関連付けることにより、各シーケンスのステップあたり0.00501と0.10682です。次に、訓練されたオブジェクトと訓練されていないオブジェクトを使用したオンライン生成によるタスクの成功率を確認します。ここでは、3つのタイプのトレーニングされていない位置データのトレーニングされた布と3つのトレーニングされていない布がテストに使用されています。各布は、ロボットが到達する範囲内で3回ランダムに配置されます（小さな回転でシフトします）。結果を図7に示す。

![Results of online](https://raw.githubusercontent.com/rurusasu/paper/master/AI%E6%8A%80%E8%A1%93/AI%E6%8A%80%E8%A1%93%E5%BF%9C%E7%94%A8/Repeatable%20Folding%20Task%20by%20Humanoid%20Robot/%E7%94%BB%E5%83%8F/Results%20of%20online%20generation.bmp)\
図7. 訓練されたオブジェクトと訓練されていないオブジェクトを使用したオンライン生成の結果

![Table3 Success Rate Evaluated](https://raw.githubusercontent.com/rurusasu/paper/master/AI%E6%8A%80%E8%A1%93/AI%E6%8A%80%E8%A1%93%E5%BF%9C%E7%94%A8/Repeatable%20Folding%20Task%20by%20Humanoid%20Robot/%E7%94%BB%E5%83%8F/Table3%20Success%20Rate%20Evaluated.png)


タスクのパフォーマンスを評価するために、（a）実行された動作と（b）領域変更率を使用して成功率を定義します。

a) 実行された動作：この評価では、「掴まれた」動作(Grabbed)と「折り畳まれた」動作(Folded)が評価され、ロボットがオンライン生成中にタスクを実行するかどうかが決定されます。さまざまな行動の成功率を表IIIに示します。

b) 面積変化率：この評価では、切り抜かれた画像の領域検出が利用されます。この領域はピクセルを表し、切り抜かれた画像は常に布全体を覆っています。ここでは、開始状態（図6（1）の前）と終了状態（図6（6）の後）の両方でエリアを検出するエリア変更率（ACレート）を定義し、これら2つの状態の差を利用して成功率を評価します。面積変化率の詳細を表IVに示します。異なる面積変化率による成功率を図8に示します。

![Table4 AREA CHANGED RATE](https://raw.githubusercontent.com/rurusasu/paper/master/AI%E6%8A%80%E8%A1%93/AI%E6%8A%80%E8%A1%93%E5%BF%9C%E7%94%A8/Repeatable%20Folding%20Task%20by%20Humanoid%20Robot/%E7%94%BB%E5%83%8F/Table4%20AREA%20CHANGED%20RATE.png)

![Success rate relative](https://raw.githubusercontent.com/rurusasu/paper/master/AI%E6%8A%80%E8%A1%93/AI%E6%8A%80%E8%A1%93%E5%BF%9C%E7%94%A8/Repeatable%20Folding%20Task%20by%20Humanoid%20Robot/%E7%94%BB%E5%83%8F/Success%20rate%20relative.bmp)\
図8. 面積変化率に対する成功率。

## C. Reiteration Ability Test
![Reiteration ability test](https://raw.githubusercontent.com/rurusasu/paper/master/AI%E6%8A%80%E8%A1%93/AI%E6%8A%80%E8%A1%93%E5%BF%9C%E7%94%A8/Repeatable%20Folding%20Task%20by%20Humanoid%20Robot/%E7%94%BB%E5%83%8F/Reiteration%20ability%20test%20experiment.bmp)\
図9. 人間とロボットの相互作用による反復能力テスト実験。


繰り返し能力を評価するために、実験者はロボットに面したテーブルの前に立ち、ロボットが折りたたみタスクを実行している間、タスクを妨害します（図9）。ロボットがオンライン生成中に邪魔されてもタスクを繰り返すことができることが確認され、提案されたモデルのロバスト性を証明します。

## D. Online Generation with Untrained Object
布を折る作業は、本を閉じる作業とよく似ています。したがって、本は訓練されていないオブジェクトでのテストに使用されます。図10に示すように、このタスクは正常に実行されます。

![Book-closing test]()\
図10. 訓練されたオブジェクトを使用したオンライン生成テスト：本を閉じるテスト。

# 5. DISCUSSION
以前の研究[16]と同様に構造化されたトレーニングモデルと比較して、この研究は高次元の画像と14自由度のデュアルアームを使用しており、オンライン生成の難易度を大幅に高めています。彼らのアプローチは単純なタスクしか実行できませんでしたが、教育方法とモデルの制限により、長い動的行動にはうまくいきませんでした。したがって、我々は、遠隔操作訓練方法に基づく新しいモデルと、この問題を管理することができるDCAETDNNモデルを紹介します。私たちの実験を通じて、提案されたモデルは、より高次元の画像データを管理する強力な能力を示し、モデルがTDNNオンライン生成に対して比較的安定した信号を提供できることが証明されました。実験から、視覚的な情報は、安定した環境設定で行うタスクに十分です。ただし、将来の見通しとしてモダリティを増やすことで改善できるさまざまな環境に対する精度と適応性が期待されます。

## A. Robustness
![Image feature reconstructed](https://raw.githubusercontent.com/rurusasu/paper/master/AI%E6%8A%80%E8%A1%93/AI%E6%8A%80%E8%A1%93%E5%BF%9C%E7%94%A8/Repeatable%20Folding%20Task%20by%20Humanoid%20Robot/%E7%94%BB%E5%83%8F/Image%20feauture%20reconstructed.bmp)\
図11. 人間の干渉中に訓練されていないオブジェクトで再構築された画像の特徴。画像情報には人間の干渉が含まれており、DCAE + TDNNモデルはこの情報を管理してタスクを実行できます。

ロボットは、堅牢性に比べて、実験中にオブジェクトを変更したり、オブジェクトの位置を移動したりするなど、人間の実験者の手がプロセスを妨害した場合でもタスクを実行できます。これは、ノイズの多い環境でパフォーマンスを維持できるという点で、提案されたモデルの堅牢性を示しています。提案されたモデルは、生成中の感覚運動情報のダイナミクスに対処できます（図11）。

## B. Increasing Operation Speed
タスクの実行速度は、特に産業用アプリケーションにとって重要な要素です。オンライン生成中、この速度は予測と実行の速度、および元のトレーニングデータシーケンスに強く依存します。タスクのパフォーマンス速度がテストされ、ティーチングデータのサンプリング速度（元のシーケンスの長さの半分）を減らし、予測と実行の速度（2回の予測と1回のみ動作します）。 4倍の生成速度でテストに成功しました（700ステップは約180ステップに減少しました。平均70秒は10 FPS実行レートで約18秒に減少しました）。その結果、ロボットのタスク実行速度を上げることができます。

# 6. CONSTLUCTION
この研究の目標は、一般化可能性を維持しながら繰り返し折りたたみ作業を実行できるヒューマノイドロボットワーカーを実現するための有用な方法を提案することです。ソフトオブジェクトの折りたたみタスクは、提案されたアプローチの適応性と有効性を評価するために選択されます。私たちの実験では、遠隔操作でトレーニングデータが正常に収集され、提案されたアプローチにより、バックドライブ不可能なヒューマノイドロボットが折りたたみタスクを完了できるようになりました。これは、実験で検証されています。その結果、私たちの目的は達成され、提案された方法は困難なタスクの問題に対処するための実行可能なモデルとして証明されます。

将来的には、提案されたモデルは複数のタスクシーケンスに適用されます。不定形オブジェクトの取得や、より動的なタスクなど、より実用的なアプリケーションを実現できます。さらに、タスクのパフォーマンス速度は、事前に設計されたパイプラインメソッドと同等であることが期待されており、複数のタスクシーケンスの種類が増えることが予想されます。
# PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes

## 備考
## 著者
Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, Dieter Fox
## 掲載
"PoseCNN: A Convolutional Neural Network for 6D Ob-ject Pose Estimation in Cluttered Scenes.", arXiv preprint arXiv:1711.00199, 2017.

# Abstract
ロボットが現実の世界と相互作用するには、既知のオブジェクトの6Dポーズを推定することが重要です。この問題は、オブジェクト間の多様性や、乱雑さ(clutter)、オブジェクト同士の妨害(occlusion)によって引き起こされるシーンの複雑さのために困難です。この作業では、6Dオブジェクトポーズ推定用の新しい畳み込みニューラルネットワークであるPoseCNNを紹介します。 PoseCNNは、画像の中心を特定し、カメラからの距離を予測することにより、オブジェクトの3D変換を推定します。オブジェクトの3D回転は、クォータニオン表現に回帰することで推定されます。また、PoseCNNが対称オブジェクトを処理できるようにする新しい損失関数も紹介します。さらに、YCB-Videoデータセットという6Dオブジェクトポーズ推定用の大規模なビデオデータセットを提供します。私たちのデータセットは、133,827フレームの92ビデオで観察されたYCBデータセットからの21オブジェクトの正確な6Dポーズを提供します。 YCBVideoデータセットとOccludedLINEMODデータセットで広範な実験を行い、PoseCNNがオクルージョンに対して非常に堅牢であり、対称オブジェクトを処理でき、入力としてカラー画像のみを使用して正確なポーズ推定を提供できることを示します。深度データを使用してポーズをさらに洗練する場合、深度データを使用してポーズをさらに洗練する場合、私たちのアプローチは、困難なOccluded LINEMODデータセットで最先端の結果を実現します。コードとデータセットは、https://rse-lab.cs.washington.edu/projects/posecnn/で入手できます。

# 1. INTRODUCTION
3Dでオブジェクトを認識し、それらのポーズを推定することは、ロボットタスクで幅広い用途があります。たとえば、オブジェクトの3D位置と方向を認識することは、ロボット操作にとって重要です。また、デモンストレーションからの学習など、ヒューマンロボットのインタラクションタスクにも役立ちます。しかし、現実の世界にはさまざまなオブジェクトがあるため、それらの推定は困難です。なぜなら，それぞれのオブジェクトの3D形状は異なり、画像上の外観は、照明条件、シーンの乱雑さ、オブジェクト間のオクルージョンの影響を受けるからです。

従来、6Dオブジェクトのポーズ推定の問題は、3Dモデルと画像の間で特徴点を一致させることによって対処されています[20、25、8]。ただし、これらの方法では、一致する特徴点を検出するために、オブジェクトに豊富なテクスチャ(背景)が必要です。その結果、背景のないオブジェクトを処理できません。深度カメラの登場により、RGB-Dデータを使用して背景のないオブジェクトを認識する方法がいくつか提案されています[13、3、2、26、15]。テンプレートベースの方法[13、12]では、オクルージョンによって認識パフォーマンスが大幅に低下します。あるいは、6D姿勢推定の2D-3D対応を確立するために画像ピクセルを3Dオブジェクト座標に回帰する学習を実行する方法[3、4]は、対称オブジェクトを処理できません。

本論文では、既存のメソッドの制限を克服しようとする6Dオブジェクト姿勢推定の一般的なフレームワークを提案します。 PoseCNNという名前のend to endの6Dポーズ推定用の新しい畳み込みニューラルネットワーク（CNN）を紹介します。 PoseCNNの背後にある重要なアイデアは、ポーズ推定タスクをさまざまなコンポーネント(構成要素)に分離することです。これにより、ネットワークはコンポーネント間の依存関係と非依存関係を明示的にモデル化できます。具体的には、PoseCNNは図1に示すように3つの関連タスクを実行します。

![PoseCNN for 6D object pose estimation]()\
図1 <br>6Dオブジェクトポーズ推定用の新しいPoseCNNを提案します。ネットワークは、セマンティックラベリング、3D平行移動推定、3D回転回帰の3つのタスクを実行するようにトレーニングされています。

最初に、PoseCNNは入力画像の各ピクセルのオブジェクトラベルを予測します。次に、各ピクセルから中心に向かう単位ベクトルを予測して、オブジェクトの中心の2Dピクセル座標を推定します。セマンティックラベルを使用して、オブジェクトに関連付けられた画像ピクセルは、画像内のオブジェクトの中心位置に投票します。さらに、ネットワークはオブジェクトの中心までの距離も推定します。既知のカメラの組み込みを想定すると、2Dオブジェクトの中心とその距離の推定により、3D変換Tを復元できます。最後に、3D回転Rは、オブジェクトのバウンディングボックス内で抽出された畳み込み特徴をRのクォータニオン表現に回帰することによって推定されます。これから説明するように、RとTを推定するための2D中心投票とそれに続く回転回帰(rotation regression)は、テクスチャ/テクスチャのないオブジェクトに適用でき、オクルージョンに対してもネットワークが投票するようにトレーニングされているため、オクルージョンに対してロバストです。

対称オブジェクトの処理は、姿勢推定のもう1つの課題です。オブジェクトの向きが異なると、同じ観測結果が生成される可能性があるためです。たとえば、図5に示す赤いボウルまたはウッドブロックの向きを一意に推定することはできません。OccludedLINEMODデータセット[17]などのポーズベンチマークデータセットは、そのようなオブジェクトの特別な対称評価を考慮しますが、対称性は通常ネットワークトレーニング中は無視されます。ただし、オブジェクトの対称性に関してネットワークからの推定が正しい場合でも、ネットワークはオブジェクトの向きの高い損失などの一貫性のない損失信号を受信するため、これは悪いトレーニングパフォーマンスをもたらす可能性があります。この観察から発想を得て、オブジェクトの3D形状のマッチングに焦点を当てた新しい損失関数であるShapeMatch-Lossを紹介します。この損失関数が形状対称性を持つオブジェクトに対して優れた推定を生成することを示します。

6Dポーズ推定のベンチマークデータセットであるOccludedLINEMODデータセット[17]でこのメソッドを評価します。この困難なデータセットで、PoseCNNは色のみとRGB-Dの両方の姿勢推定で最先端の結果を実現します（姿勢の微調整には、反復最接近点（ICP）アルゴリズムで深度画像を使用します）。私たちの方法を完全に評価するために、YCB-Videoという名前の大規模なRGB-Dビデオデータセットをさらに収集しました。データセット内のオブジェクトは異なる対称性を示し、さまざまなポーズと空間構成で配置され、それらの間に厳しいオクルージョンを生成します。

要約すると、私たちの仕事には次の主要な貢献があります。
* PoseCNNという6D物体姿勢推定のための新しい畳み込みニューラルネットワークを提案します。私たちのネットワークは、エンドツーエンドの6Dポーズ推定を実現し、オブジェクト間のオクルージョンに対して非常に堅牢です。
* 対称物体の姿勢推定のための新しいトレーニング損失関数ShapeMatch-Lossを紹介します。
* 21のYCBオブジェクトに6Dポーズ注釈を提供する6Dオブジェクトポーズ推定用の規模なRGB-Dビデオデータセットを提供します。

この論文は以下のように構成されています。関連する作業について説明した後、6Dオブジェクトポーズ推定用のPoseCNNを紹介し、その後に実験結果と結論を示します。

## 2. RELATED WORK
文献の6Dオブジェクト姿勢推定方法は、テンプレートベースの方法と特徴ベースの方法に大きく分類できます。テンプレートベースの方法では、固定テンプレートが作成され、入力画像のさまざまな場所をスキャンするために使用されます。それぞれの場所で、類似性スコアが計算され、これらの類似性スコアを比較することで最良の一致が得られます[12、13、6]。 6Dポーズ推定では、テンプレートは通常、対応する3Dモデルをレンダリングすることによって取得されます。最近、2Dオブジェクト検出方法がテンプレートマッチングとして使用され、特にディープラーニングベースのオブジェクト検出器で、6Dポーズ推定のために拡張されています[28、23、16、29]。テンプレートベースのメソッドは、テクスチャ(背景)のないオブジェクトの検出に役立ちます。ただし、オブジェクトがオクルードされるとテンプレートの類似性スコアが低くなるため、オブジェクト間のオクルージョンをうまく処理できません。

特徴ベースの方法では、局所特徴を対象点または画像内のすべてのピクセルから抽出し、3Dモデルの特徴と照合して2Dと3Dの対応を確立し、そこから6Dポーズを復元できます[20、25、30 、22]。機能ベースのメソッドは、オブジェクト間のオクルージョンを処理できます。ただし、ローカルフィーチャ(局所特徴)を計算するには、オブジェクトに十分なテクスチャが必要です。テクスチャのないオブジェクトを処理するために、機械学習技術を使用して特徴記述子を学習するいくつかの方法が提案されています[32、10]。 2Dと3Dの対応を確立するために、各ピクセルの3Dオブジェクト座標位置に直接回帰するいくつかのアプローチが提案されています[3、17、4]。しかし、3D座標回帰では、対称オブジェクトを処理するときにあいまいさが発生します。

この作業では、ディープラーニングフレームワークでテンプレートベースの方法と機能ベースの方法の両方の利点を組み合わせます。ネットワークでは、ボトムアップのピクセル単位のラベリングとトップダウンのオブジェクトポーズの回帰を組み合わせています。最近、Amazon Picking Challenge（APC）での競争のおかげで、6Dオブジェクトポーズ推定の問題がさらに注目されています。 APCの特定の設定について、いくつかのデータセットとアプローチが導入されています[24、35]。当社のネットワークは、適切なトレーニングデータが提供されている限り、APC設定に適用される可能性があります。

# 3. PoseCNN
入力画像が与えられた場合、6Dオブジェクトポーズ推定のタスクは、オブジェクト座標系Oからカメラ座標系Cへの厳密な変換を推定することです。オブジェクトの3Dモデルが利用可能であり、オブジェクト座標系がモデルの3D空間。ここでの剛体変換は、3D回転Rと3D平行移動Tを含むSE（3）変換で構成されます。Rは、オブジェクト座標系OのX軸、Y軸、Z軸の周りの回転角度を指定します。 Tはカメラ座標系CでのOの原点の座標です。イメージングプロセスでは、Tは画像内のオブジェクトの位置とスケールを決定し、Rはオブジェクトの画像の外観に影響を与えます。これら2つのパラメーターは異なる視覚的特性を持っているため、RとTの推定を内部的に分離する畳み込みニューラルネットワークアーキテクチャを提案します。

## A. Overview of the Network

![Architecture of PoseCNN for 6D object pose estimation]()\
図2. 6Dオブジェクトポーズ推定のためのPoseCNNのアーキテクチャ。

図2は、6Dオブジェクトポーズ推定のためのネットワークのアーキテクチャを示しています。ネットワークには2つの段階があります。最初のステージは、13の畳み込みレイヤーと4つのmaxpoolingレイヤーで構成され、入力画像から異なる解像度の特徴マップを抽出します。抽出された特徴はネットワークで実行されるすべてのタスクで共有されるため、この段階はネットワークのバックボーンです。第2ステージは、第1ステージで生成された高次元の特徴マップを低次元のタスク固有の機能に埋め込む埋め込みステップで構成されます。次に、ネットワークは、6Dポーズ推定につながる3つの異なるタスク、つまり、セマンティックラベリング、3D平行移動推定、3D回転回帰を実行します。

## B. Semantic Labeling
画像内のオブジェクトを検出するために、セマンティックラベリングを使用します。この場合、ネットワークは各画像ピクセルをオブジェクトクラスに分類します。バウンディングボックスを使用したオブジェクト検出に頼る最近の6Dポーズ推定方法[23、16、29]と比較して、セマンティックラベリングは、オブジェクトに関する豊富な情報を提供し、オクルージョンをより適切に処理します。

図2に示すように、セマンティックラベリングブランチの埋め込みステップは、特徴抽出ステージによって生成されたチャネル次元512を持つ2つの特徴マップを入力として受け取ります。 2つの特徴マップの解像度は、それぞれ元の画像サイズの1 / 8および1 / 16です。ネットワークは、最初に2つの畳み込み層を使用して、2つの特徴マップのチャネル次元を64に減らします。次に、デコンボリューションレイヤーを使用して1 = 16特徴マップの解像度を2倍にします。その後、2つの特徴マップが合計され、元の画像サイズの特徴マップを取得するために、別のデコンボリューションレイヤーを使用して解像度が8倍に増加します。最後に、畳み込み層は特徴マップに作用し、ピクセルのセマンティックラベリングスコアを生成します。この層の出力にはn個のチャネルがあり、n個のセマンティッククラスの数があります。トレーニングでは、softmaxクロスエントロピー損失を適用して、セマンティックラベリングブランチをトレーニングします。テスト中、softmax関数はピクセルのクラス確率を計算するために使用されます。セマンティックラベリングブランチの設計は、セマンティックラベリングのための[19]の完全たたみ込みネットワークに触発されました。前の作品のシーンのラベル付けにも使用されています[34]。

## C. 3D Translation Estimation

![Illustration of the object coordinate system]()\
図3. オブジェクト座標系とカメラ座標系の図。<br> 3D平行移動は、オブジェクトの2D中心を特定し、カメラから3D中心距離を推定することによって推定できます。

図3に示すように、3D並進$\bm{T}=(T_x,T_y,T_z )^T$は、カメラ座標系におけるオブジェクトの原点の座標です。 $\bm{T}$を推定する素朴な方法は、画像の特徴を直接$\bm{T}$に回帰することです。ただし、オブジェクトは画像の任意の場所に表示される可能性があるため、このアプローチは一般化できません。また、同じカテゴリの複数のオブジェクトインスタンスを処理することはできません。したがって、画像内の2Dオブジェクトの中心を特定し、カメラからのオブジェクト距離を推定することにより、3D変換を推定することを提案します。見るために、画像への$\bm{T}$の投影が$c=(c_x,c_y )^T$であると仮定します。ネットワークが画像内の$c$を特定し、深さTzを推定できる場合、ピンホールカメラを想定した次の投影方程式に従って$T_x$と$T_y$を復元できます。

$$
\left[
    \begin{array}{r}
        c_x \\
        c_y
    \end{array}
\right] =
\left[
    \begin{array}{r}
        f_x \frac{T_x}{T_z} + p_x\\
        f_y \frac{T_y}{T_z} + p_y
    \end{array}
\right]
$$

ここで、$f_x$と$f_y$はカメラの焦点距離を示し、$(p_x,p_y )^T$は主点です。オブジェクトの原点$O$がオブジェクトの重心である場合、$c$をオブジェクトの2D中心と呼びます。

2Dオブジェクトの中心を特定する簡単な方法は、既存のキーポイント検出方法のように中心点を直接検出することです[22、7]。ただし、オブジェクトの中心が塞がれている場合、これらのメソッドは機能しません。画像パッチが検出のためにオブジェクトの中心に投票する従来のImplicit Shape Model（ISM）に触発されて[18]、画像の各ピクセルの中心方向に回帰するようにネットワークを設計します。具体的には、画像上のピクセル$\bm{p}=(x,y)^T$の場合、3つの変数に回帰します。

$$
(x,y) = 
\left(
    \begin{array}{r}
        n_x = \frac{c_x - x}{||c-p||}, 
        n_y = \frac{c_y - y}{||c-p||},
        T_z
    \end{array}
\right)
$$

変位ベクトル$c-p$に直接回帰するのではなく、単位長ベクトル$n=(n_x,n_y )^T= \frac{c-p}{||c-p||}$に回帰するようにネットワークを設計することに注意してください。つまり、スケールである2D中心方向です。 これはスケール不変であるため、トレーニングが容易です（実験的に確認したとおり）。

私たちのネットワークの中心回帰ブランチ（図2）は、畳み込み層と逆畳み込み層のチャネルの次元が異なることを除いて、セマンティックラベリングブランチと同じアーキテクチャを使用しています。このブランチは各オブジェクトクラスの3つの変数に回帰する必要があるため、64次元ではなく128次元空間に高次元機能を埋め込みます。このブランチの最後のたたみ込み層のチャネル次元は$3 \times n$で、$n$はオブジェクトクラスの数です。トレーニングでは、平滑化されたL1損失関数が[11]のように回帰に適用されます。
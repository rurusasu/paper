# PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes

## 備考
## 著者
Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, Dieter Fox
## 掲載
"PoseCNN: A Convolutional Neural Network for 6D Ob-ject Pose Estimation in Cluttered Scenes.", arXiv preprint arXiv:1711.00199, 2017.

# Abstract
ロボットが現実の世界と相互作用するには、既知のオブジェクトの6Dポーズを推定することが重要です。この問題は、オブジェクト間の多様性や、乱雑さ(clutter)、オブジェクト同士の妨害(occlusion)によって引き起こされるシーンの複雑さのために困難です。この作業では、6Dオブジェクトポーズ推定用の新しい畳み込みニューラルネットワークであるPoseCNNを紹介します。 PoseCNNは、画像の中心を特定し、カメラからの距離を予測することにより、オブジェクトの3D変換を推定します。オブジェクトの3D回転は、クォータニオン表現に回帰することで推定されます。また、PoseCNNが対称オブジェクトを処理できるようにする新しい損失関数も紹介します。さらに、YCB-Videoデータセットという6Dオブジェクトポーズ推定用の大規模なビデオデータセットを提供します。私たちのデータセットは、133,827フレームの92ビデオで観察されたYCBデータセットからの21オブジェクトの正確な6Dポーズを提供します。 YCBVideoデータセットとOccludedLINEMODデータセットで広範な実験を行い、PoseCNNがオクルージョンに対して非常に堅牢であり、対称オブジェクトを処理でき、入力としてカラー画像のみを使用して正確なポーズ推定を提供できることを示します。深度データを使用してポーズをさらに洗練する場合、深度データを使用してポーズをさらに洗練する場合、私たちのアプローチは、困難なOccluded LINEMODデータセットで最先端の結果を実現します。コードとデータセットは、https://rse-lab.cs.washington.edu/projects/posecnn/で入手できます。

# 1. INTRODUCTION
3Dでオブジェクトを認識し、それらのポーズを推定することは、ロボットタスクで幅広い用途があります。たとえば、オブジェクトの3D位置と方向を認識することは、ロボット操作にとって重要です。また、デモンストレーションからの学習など、ヒューマンロボットのインタラクションタスクにも役立ちます。しかし、現実の世界にはさまざまなオブジェクトがあるため、それらの推定は困難です。なぜなら，それぞれのオブジェクトの3D形状は異なり、画像上の外観は、照明条件、シーンの乱雑さ、オブジェクト間のオクルージョンの影響を受けるからです。

従来、6Dオブジェクトのポーズ推定の問題は、3Dモデルと画像の間で特徴点を一致させることによって対処されています[20、25、8]。ただし、これらの方法では、一致する特徴点を検出するために、オブジェクトに豊富なテクスチャ(背景)が必要です。その結果、背景のないオブジェクトを処理できません。深度カメラの登場により、RGB-Dデータを使用して背景のないオブジェクトを認識する方法がいくつか提案されています[13、3、2、26、15]。テンプレートベースの方法[13、12]では、オクルージョンによって認識パフォーマンスが大幅に低下します。あるいは、6D姿勢推定の2D-3D対応を確立するために画像ピクセルを3Dオブジェクト座標に回帰する学習を実行する方法[3、4]は、対称オブジェクトを処理できません。

本論文では、既存のメソッドの制限を克服しようとする6Dオブジェクト姿勢推定の一般的なフレームワークを提案します。 PoseCNNという名前のend to endの6Dポーズ推定用の新しい畳み込みニューラルネットワーク（CNN）を紹介します。 PoseCNNの背後にある重要なアイデアは、ポーズ推定タスクをさまざまなコンポーネント(構成要素)に分離することです。これにより、ネットワークはコンポーネント間の依存関係と非依存関係を明示的にモデル化できます。具体的には、PoseCNNは図1に示すように3つの関連タスクを実行します。
# Repeatable Folding Task by Humanoid Robot Worker Using Deep Learnig

# Abstract
生産ラインの労働者として働くことができる機械学習ベースのヒューマノイドロボットを開発するための実用的な最先端の方法を提案します。提案されたアプローチは、データを収集する直観的な方法を提供し、次の特性を示します。タスク実行機能、タスク反復機能、一般化可能性、および容易な適用可能性。提案されたアプローチは、モニターを備えたリアルタイムユーザーインターフェースを利用し、ヘッドマウントディスプレイを使用して一人称視点を提供します。このインターフェースを介して、特に従来の方法では適用が困難なタスクのタスク操作データを収集するために、テレオペレーションが使用されます。提案されたアプローチでは、2段階の深層学習モデルも利用されます。ディープコンボリューショナルオートエンコーダーは画像の特徴を抽出して画像を再構築し、完全に接続されたディープタイムディレイニューラルネットワークは、抽出された画像の特徴と運動角度信号からロボットタスクプロセスのダイナミクスを学習します。「Nextage Open」ヒューマノイドロボットは、提案されたモデルを評価するための実験プラットフォームとして使用されます。テスト用に35の訓練された感覚モーターシーケンスと5つの訓練されていない感覚モーターシーケンスを使用したオブジェクト折りたたみタスク。オンライン生成を使用してトレーニング済みモデルをテストすると、オブジェクト折りたたみタスクの成功率が77.8％であることが実証されています。

# 備考
## 著者
Pin-Chu Yang, Kazuma Sasaki, Kanata Suzuki, Kei Kase, Shigeki Sugano, and Tetsuya Ogata

## 掲載
IEEE ROBOTICS AND AUTOMATION LETTERS, VOL. 2, NO. 2, APRIL 2017.

# 1. Introduction
少子化と長寿化が進む中で、将来的には人工妊娠中絶の不足が予想されます。人手不足は効率性の低下とコスト増を招き、消費者に不可欠な製品を生産するメーカーはコスト増に耐えられなくなる可能性があり、最終的には社会全体に影響を及ぼすことになります。また、市場の需要の変化に伴い、製造業では少量多品種生産への転換が進んでいます。 したがって、さまざまな製品を扱うことができる生産設備がさらに必要になります[1]。そのような施設は、より柔軟で、製品ごとに異なるタスクを実行できる必要があります。これを達成するために、複数の自由度アームを備えたロボット、画像認識システム、および事前に設計されたモーションシーケンスを備えた大規模データベースが、可能なソリューションを表しています。ただし、このソリューションは、同様のタイプの製品を製造または分解する工場には適さない場合があります。現在の事前設計方法を利用して、これらのジョブを人間の労働者から置き換えることはほぼ不可能です。同様のタイプの製品を生産する場合、製造タスクは反復的であり、限られた人間の知能を必要とする傾向があります。したがって、将来の労働力不足がこの種の生産に影響を与えることが予想されます。

最近、「深い」構造を持つ人工ニューラルネットワーク学習モデル（つまり、ディープニューラルネットワーク）は、画像と音声の認識で成功を収めています。この機械学習法は、ロボットのタスクに適用されることが期待されています。ただし、非構造化環境の機械学習モデルを構築するには、かなりのトレーニングデータが必要です。

通常、データの収集には直接指導と模倣学習の方法が利用されます。ただし、ハードウェアの制限と制御の問題により、タスクに適したシステムの構築が妨げられる可能性があります。この研究の対象は、人間の労働者がいる生産ラインなど、不確実な環境でタスクを実行できるロボットを制御するための機械学習ベースのモデルです。したがって、実用的なアプリケーション、運用システムの利便性、繰り返しタスクを自動的に実行する機能、および複数のタスクを実行する機能を考慮する必要があります。
To satisfy these requirements, we set the following objectives.
これらの要件を満たすために、次の目標を設定します。
1. タスク機能：ロボットには十分な知識があり、特定のタスクを実行できます。
2. 反復能力：ロボットはタスクを繰り返し実行できます。
3. 一般化可能性：ロボットは、複数のタスクと類似のオブジェクトを使用して同じタスクを実行できます。
4. 工場のロボット（非後方駆動ロボット）への容易な適用性：提案された方法は、制限がほとんどなく、簡単に適用できるはずです。

パフォーマンスを評価し、実用的なケースをテストするために、このタイプのタスクを達成するための事前に設計されたパイプラインメソッドの難しさから、ソフトオブジェクトの折りたたみがタスクとして選択されました。

パフォーマンスを評価し、実用的なケースをテストするために、このタイプのタスクを達成するための事前に設計されたパイプラインメソッドの難しさから、ソフトオブジェクトの折りたたみがタスクとして選択されました。前のセクションで述べたように、折り畳みタスクは、動的な環境情報に起因するロボットによる折り畳みプロセス中の予測できない変化に対処するために使用できる、よく知られた困難な操作タスクです。一般化および反復能力は、トレーニングデータで決して使用されないオブジェクトでテストできます。このタスクは、提案されたモデルの有効性を実証するために、特定のセットアップとキャリブレーションのない状況で使用されます。実験結果は、提案されたモデルがスマートロボットワーカーの実現に有望であることを示しており、さまざまなタスクに適用できることが期待されます。

残りのペーパーは次のように構成されています。2章では、行われた関連作業を紹介します。このセクションでは、このペーパーの課題と貢献についても説明します。3章では、この調査のアプローチを紹介します。これには、データ収集プロセス、モデルのアーキテクチャ、およびトレーニング方法の紹介が含まれます。4章では、ロボットの動き、トレーニングデータセットの収集に使用されるオブジェクト、モデルトレーニングのパラメーター、および結果を含む実験設定について説明します。５章では、タスク実行のパフォーマンスを向上させるための堅牢性と可能な方法について説明します。6章で結果をまとめ、提案されたアプローチの有効性を示します。

# 2. RELATED WORK
ロボットが物体の把持やボルトの挿入などの操作タスクを実行できるかどうかが調査されています[2]。衣服を折りたたむ[3]、[4]、ケーブルを配線する[5]、[6]など、より複雑な作業も研究されています。これらの研究は、こうしたタスクがさまざまなアプローチで実行できることを実証しています。ただし、これらの方法の成功率は通常、人間が設計した制御、画像特徴抽出、および環境に依存します。以前の研究で提案されたアプローチは、複雑で不確実な状況を特徴とする環境に適用するのが難しい作業である可能性があります。

研究者は、操作タスクの方法を開発し、ある種のスマートコントロールを組み込むことを試みました。深層学習法は、静的画像認識に適用されています[7]。この方法は、人間の認識能力よりもかなり高い認識率を達成しました。超解像畳み込みネットワーク[8]は、インターネットベースのwaifu2x超解像サービス[9]に影響を与えました。ディープラーニングは、たとえば自動運転車の研究[10]や未定義の物体把握[11]〜[13]など、シーケンシャルデータのトレーニングにも使用されています。

機械学習法は、ロボットのタスクにも適用されています[14]。この研究では、軌道ポリシーとロボットアームアクチュエータのトルク信号をトレーニングする強化学習法を提案しました。野田ら[15]は、複数の行動の学習と複数のモダリティからの情報を自動的に考慮するモデルを提案しました。彼らのモデルは、2つの完全に接続されたニューラルネットワーク、つまり画像特徴抽出ネットワークと動的学習モデルを組み合わせたものです。野田ら趣味のサイズのヒューマノイドロボットであるNAOを使用して、複数の周期的な運動動作にモデルを適用しました。鈴木や野田らによって提案されたものと同様の構造を採用した。そして、ソフトオブジェクトの折りたたみタスクにPR2ロボットを使用することに成功しました[16]。ただし、それらの方法は反復可能なタスクを実行できず、[16]、[15]は両方とも、逆駆動可能なロボットにのみ適用できるダイレクトティーチング方法を使用します。さらに、これらの方法では、感覚運動データの有効性を判断することは困難です。

現在の研究では、インタラクティブなロボット環境情報（感覚運動情報）に対する動的情報の影響に焦点を当て、提案手法を評価するためにヒューマノイドロボットと折り畳みタスクを使用しています。私たちのアプローチに基づいて、産業レベルのヒューマノイドロボットがパイプライン方式ではなく高い適応性を必要とするタスクを実行できるようにすることは効果的です。さらに、当社のアーキテクチャにより、強化学習の評価関数を設計する労力を排除し、実験者の操作経験からの直接学習に置き換えることができます。

# 3. APPROACH
不確実な環境で動作する適応可能なタスク実行ヒューマノイドロボットに適用できる深層学習方法を実現するアプローチを提示します。目標1.および3.では、不確実な環境に適した十分に高い一般化可能性を実現するために、ロボットから取得した感覚運動情報を学習するためにディープラーニングが適用されます。その情報は、提案されたモデルにタスク操作を実行することを許可します。深層学習で目標IIを達成するために、すべてのシーケンスは同じロボットポーズで開始および終了するように設計されています。客観的IVでは、遠隔操作技術を使用して、ロボットの構成を無視できるデータを取得します。

ロボットタスクにディープラーニングを適用するには、（1）データ収集、（2）トレーニング、（3）タスク生成フェーズが必要です。アプローチのフローを図1に示します。「Teleoperation Training」、「Learning Model」、および「GenerateMotions for Tasks」は、それぞれデータ収集、トレーニング、およびタスク生成フェーズに対応しています。

![Process approach](https://raw.githubusercontent.com/rurusasu/paper/master/AI%E6%8A%80%E8%A1%93/AI%E6%8A%80%E8%A1%93%E5%BF%9C%E7%94%A8/%E7%94%BB%E5%83%8F/Repeatable%20Folding%20Task%20by%20Humanoid%20Robot/Process%20approach.png)\
画像1. 提案されたアプローチのプロセスは、主に3つのフェーズに分かれています。

## A. Data Collection Phase
![Sensory-motor experiece sharing](https://raw.githubusercontent.com/rurusasu/paper/master/AI%E6%8A%80%E8%A1%93/AI%E6%8A%80%E8%A1%93%E5%BF%9C%E7%94%A8/%E7%94%BB%E5%83%8F/Repeatable%20Folding%20Task%20by%20Humanoid%20Robot/Sensory-motor%20experience%20sharing.bmp)
図2.感覚運動体験の共有：リモートモニタリングまたはヘッドマウントディスプレイを使用して、オペレーターはロボットを直接操作し、同じ感覚を共有して適切な感覚運動体験（データ）を取得できます。これにより、収集された感覚運動データが人間の感覚で動作し、ディープラーニングに効果的であることが期待されます。

データ収集はディープラーニングの重要なステップであり、正確なタイミングのモーションを必要とするタスクには特に重要です。いくつかの深層学習の研究では、ロボットアームがタスクを実行するように誘導するために直接教育を使用しています。ただし、直接教示は、特殊な製品の製造に通常使用される逆駆動可能なロボットにのみ適用できます。このようなロボットは、通常は逆駆動できないロボットのみを装備し、実際のタスク操作のための機能を必要とする工場にとって非常に高価になる可能性があります。現在の研究の目的は、ロボットの制限なしに効果的なデータ収集方法を提案することです。
1）	遠隔操作訓練：粟野らの遠隔操作法を適用してトレーニングデータを収集し、人間ロボットの協調行動を実現しました[17]。現在の研究では同じ手法を利用しており、特にあらゆる種類のロボット、特に後進できないロボットに適用できます。

1) 遠隔操作訓練：粟野らの遠隔操作法を適用してトレーニングデータを収集し、人間ロボットの協調行動を実現しました[17]。現在の研究では同じ手法を利用しており、特にあらゆる種類のロボット、特に後進できないロボットに適用できます。

テレオペレーション(遠隔操作技術)には、完全に自律的なものから完全な手動制御まで、および混合イニシアチブ相互作用まで、あらゆる量の制御が含まれます。このような複合コマンドプロシージャは、データ収集に利点をもたらします。完全に自律的なコマンドシステムは、事前に設計された動作と動作を利用して、予測可能な動作のプログラミング時間を短縮できます。半自律コマンドは、軍隊が使用する誘導ミサイルなどの誘導情報を必要とする自己自律的な動作を指します。半自律的なコマンドプロセスは、高精度を必要とする動作に対して満足のいくロボットの動作を提供します。手動制御では、人間のオペレーターがロボットのアクチュエーターを直接制御してタスクを実行します。

センサー信号や画像データなどの一部のデータは、遠隔操作中に収集できます。これらのシーケンシャルデータは、異なる自律コマンドレベルでロボットから直接収集されます。さらに、図2に示すように、ロボットモーターの角度を含む感覚運動データと、ロボットに取り付けられたカメラでキャプチャされた画像データがトレーニングフェーズで収集されます。

## B Traning Model Phase
![Orverview of the model](https://raw.githubusercontent.com/rurusasu/paper/master/AI%E6%8A%80%E8%A1%93/AI%E6%8A%80%E8%A1%93%E5%BF%9C%E7%94%A8/%E7%94%BB%E5%83%8F/Repeatable%20Folding%20Task%20by%20Humanoid%20Robot/Orverview%20of%20the%20model.bmp)\
図3. モデルの概要：（a）aDCAEは画像を抽出し、特徴情報を圧縮できます。モデル構造の半分（中間層から出力層）は、抽出された画像の特徴に情報をデコード（再構築）するために使用されます。 （b）TDNNは、ロボットから取得した抽出された画像の特徴とモーションデータのウィンドウサイズのステップで時系列データを学習および生成します。


収集されたデータは、この研究で提案された深層学習モデルのトレーニングに使用されます。モデルは、図3に示すように2つの部分で構成されています。この2段階のEnd-to-Endトレーニングプロセスにより、提案されたモデルは生の入力データを適応的に処理して、環境の小さな変化を処理し、出力コマンド信号から対応する動作を実行できます。

1) **Deep Convolutional Autoencoder(DCAE):**\
   畳み込みニューラルネットワーク（CNN）は、特に画像認識のための強力な画像処理ツールです。CNNにはスライディングフィルターが含まれています。これは、空間的に局所的な入力パターンに対する強力な応答を活用し、入力画像全体をカバーできる生体細胞に似ています。CNNは、より少ないパラメーターを使用しながら、完全に接続されたニューラルネットワークよりもかなり多くの入力ディメンションを処理できます。これにより、トレーニング時間が大幅に短縮され、画像処理または同様のデータ入力のパフォーマンスが向上します。さらに、深い畳み込み層構造のモデルは、エッジから画像の部分的な部分まで、さまざまなレベルの特徴にデータを抽出できます。この研究では、畳み込み層を使用して、高解像度の画像を小さなサイズの特徴マップに処理できるDCAEを提示しました。ストライドのある畳み込みレイヤーは、特徴を抽出し、情報の次元をダウンサンプリングできます。デコンボリューションレイヤーは、エンコードされたフィーチャマップから画像を再構築するために使用されます。訓練されたDCAEでは、モデル構造の半分（中間層への入力層）を使用して、元の入力画像と比較して、情報を小サイズの画像特徴にエンコード（圧縮）します。これらのエンコードされた画像機能は、入力画像の状態を表し、より少ない次元で高解像度の入力情報を提供できます。バッチ正規化は、学習を最適化し、問題の過剰適合の可能性を減らすために使用されます。DCAE構造を表Iに示します。ネットワークは、出力画像層で入力画像データを再構築するために訓練されます。この研究では、DCAEのトレーニングデータは、ロボットに取り付けられたカメラから取得した連続画像を利用します。各入力画像のターゲットは元の入力データであり、平均二乗誤差（MSE）は、Adam最適化[18]を使用してニューラルネットワークの重みを変更するために使用されます。
2) Time-Delay Neural Network:\
   TDNNは、時間ステップウィンドウ[19]で時系列データ用にトレーニングされた完全に接続されたフィードフォワードニューラルネットワークです。野田らマルチモダリティ信号でTDNNを使用し、趣味のサイズのロボットであるNAOでマルチ動作を実現したモデルを提案しました[15]。深層構造のレイヤーを持つTDNNは、シーケンシャルデータを正常に再構築でき、入力情報をシフトすることで連続シーケンスを生成することができます。TDNNは、時間の経過とともに入力ウィンドウをシフトし、抽出された画像（カメラ画像）の特徴と動き（モーター角度）をリアルタイムで繰り返し入力することで実行されるオンライン生成に使用できます。

   この研究で使用したTDNNの構造を表IIに示します。TDNNは、複数の感覚運動信号入力を使用して逐次情報を学習できます。DCAEおよびロボットの動きから抽出された画像の特徴は、TDNNモデルに適用されます。TDNNの入力は、データセットからのデータの固定ウィンドウサイズステップです。トレーニング中、各入力データのターゲットは元の入力データであり、MSEはAdam最適化を使用して重みを変更するために使用されます。TDNNトレーニングデータセットは、トレーニングデータを経時的にスライドさせて作成されます。

## C. Task Generation Phase
訓練された深層学習モデルは、ロボットにタスクを実行させるために、リアルタイムの感覚運動情報とともに使用されます。これはオンライン生成と呼ばれます。すべての実行中、カメラの画像はまずDCAEに送られ、画像情報が特徴ベクトルに圧縮されます。次に、TDNNを入力するために設計されたウィンドウサイズのトラフ時間で、これらの特徴ベクトルと関節角度を組み合わせました。最後に、図4（a）に示すように、このウィンドウを時間をかけてスライドさせ、最後のステップの組み合わせ情報を継続的に置き換え、このスライドした順次情報をTDNNに入力して予測ステップを生成します。ここで、は画像の特徴を示し、Mは対応する時間Tでの動きを示します。

これらの予測されたステップデータは、ロボットに送信される次のステップ実行コマンドに使用できます。ウィンドウサイズのシーケンシャル情報にスライドアクションを進める場合、最後のステップを実行した元のモーションがコピーされてからTDNNに送られ、スライドを実行するとモーションが失われるという問題に対処します（図4（b）。さらに、より安定したスムーズなモーションを実現するために、実行されたコマンドは、予測されたステップを直接使用するのではなく、アウトソーシング信号と予測された信号を組み合わせます。パーセンテージで表された信号の組み合わせは、次のように計算できます。

$$Signal_{exe} = P \times Signal_{out} + (1-P) \times Signal_{pre}$$

ここで、$P$は組み合わせの入力パーセンテージパラメーター、$Signal_{exe}$は実行コマンド、$Signal_{out}$はアウトソーシング信号、$Signal_{pre}$はTDNNからの予測信号です。最後に、タスクに依存する推定方法を使用して、オンライン生成結果のパフォーマンスを評価します。

4. EXPERIMENT
   この研究では、KAWADA RoboticsのNextage Open Robotが実験プラットフォームとして使用されています[20]。このロボットには、バックドライブできない6つのDOFアームが2つと、正確なタスク操作用のカメラが搭載されています。ロボットは草で覆われたテーブルの前に置かれます。人工芝シートは、ロボットが制限された範囲をはるかに超えて動作するときに損傷を防ぐための緩衝領域を提供します。ここで、実験課題とは、実験者がランダムに布を配置する布折り作業です。折りたたみ課題の動作挙動を4つのトレーニングオブジェクトとともに図6に示し、その構成を図5に示します。衣服の配置位置、向き、大きさが異なるため、視覚情報はロボットの作業に大きく貢献します。

   トレーニングモデルのトレーニングデータとテストデータは、感覚運動データです。これには、ロボットから取得したモーター角度とカメラ画像が含まれます。カメラ画像の解像度は112×112×3ch（37,632次元、RGB）で、モーター角度は12 DOFで、各グリッパー信号はDOF（2 DOF）ごとにあります。データは10 FPSで記録され、各タスクシーケンスには約70秒必要です。トレーニングモデルには、約28,000ステップのデータが使用されます。

   ## A. Model Training
   DCAEは学習率α= 0.0002、β1= 0.75（ADAMパラメーター）、トレーニングとテストデータのミニバッチサイズ= 200でトレーニングされます。 50,000回の反復をトレーニングするGPU計算サポートを備えたChainer [21]を使用すると、トレーニングには13,849秒（約4時間）かかります。 TDNNは、トレーニングおよびテストデータの学習率α= 0.0002、β1= 0.7、およびミニバッチサイズ= 250でトレーニングされます。 70,000回の反復をトレーニングするためのGPU計算サポートを備えたChainerを使用すると、トレーニングには7,864秒（約2時間）かかります。

   ## B. Motion Generation
   最初に、トレーニングプロセス中に使用されるトレーニング済みシーケンスと未トレーニングシーケンスを生成します。トレーニングデータの画像への連続入力は、オンライン生成におけるトレーニング済みモデルのパフォーマンスを検証するために利用されます。このプロセスでは、MSEを使用して予測パフォーマンスを推定します。モーションの平均予測誤差は、トレーニングされた画像データの35シーケンスとテスト画像データの5つのシーケンスをそれぞれ関連付けることにより、各シーケンスのステップあたり0.00501と0.10682です。次に、訓練されたオブジェクトと訓練されていないオブジェクトを使用したオンライン生成によるタスクの成功率を確認します。ここでは、3つのタイプのトレーニングされていない位置データのトレーニングされた布と3つのトレーニングされていない布がテストに使用されています。各布は、ロボットが到達する範囲内で3回ランダムに配置されます（小さな回転でシフトします）。結果を図7に示す。

   タスクのパフォーマンスを評価するために、（a）実行された動作と（b）領域変更率を使用して成功率を定義します。

   a) 実行された動作：この評価では、「掴まれた」動作と「折り畳まれた」動作が評価され、ロボットがオンライン生成中にタスクを実行するかどうかが決定されます。さまざまな行動の成功率を表IIIに示します。

   b) 面積変化率：この評価では、切り抜かれた画像の領域検出が利用されます。この領域はピクセルを表し、切り抜かれた画像は常に布全体を覆っています。ここでは、開始状態（図6（1）の前）と終了状態（図6（6）の後）の両方でエリアを検出するエリア変更率（ACレート）を定義し、これら2つの間の差を状態は、成功率を評価するために利用されます。面積変化率の詳細を表IVに示します。異なる面積変化率による成功率を図8に示します。

    ## C. Reiteration Ability Test
    繰り返し能力を評価するために、実験者はロボットに面したテーブルの前に立ち、ロボットが折りたたみタスクを実行している間、タスクを妨害します（図9）。ロボットがオンライン生成中に邪魔されてもタスクを繰り返すことができることが確認され、提案されたモデルのロバスト性を証明します。

    ## D. Online Generation with Untrained Object
    布を折る作業は、本を閉じる作業とよく似ています。したがって、本は訓練されていないオブジェクトでのテストに使用されます。図10に示すように、このタスクは正常に実行されます。


5. 
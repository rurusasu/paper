# Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation

# 備考

* Fine-Tuningを初めて提案

## 著者
Ross Girshick，Jeff Donahue，Trevor Darrell，Jitendra Malik，UC Berkeley
## 掲載
"Rich feature hierarchies for accurate object detection and semantic segmentation，" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014, pp. 580-587 

# Abstract
正規のPASCAL VOCデータセットで測定された物体検出の性能は、ここ数年で横ばいになっています。最も性能の高い手法は、最も性能の高い手法は、複数の低レベルの画像特徴と高レベルのコンテキストを組み合わせた複雑なアンサンブルシステムです。本論文では、VOC 2012の以前の最良の結果と比較して平均精度（mAP）を30％以上向上させ、53.3％のmAPを達成する、シンプルでスケーラブルな検出アルゴリズムを提案します。私たちのアプローチは、2つの重要な洞察を組み合わせています。
1. 大容量畳み込みニューラルネットワーク(CNN)をボトムアップ領域提案に適用することで、オブジェクトをローカライズしてセグメント化できます。
2. ラベル付けされたトレーニングデータが少ない場合には、補助タスクのための教師付き事前トレーニングを行い、その後にドメイン固有の微調整を行うことで、パフォーマンスを大幅に向上させることができます。
領域提案をCNNと組み合わせているので、CNNの特徴を持つ領域を提案することができまず。この手法をR-CNNと呼びます。また、R-CNNを、最近提案された同様のCNNアーキテクチャに基づくスライディングウィンドウ検出器であるOverFeatと比較します。 200クラスのILSVRC2013検出データセットにおいて，R-CNNがOverFeatを大きく上回ることがわかりました．完全なシステムのソースコードは http://www.cs.berkeley.edu/ 〜rbg/rcnn にあります．

# 1. Intriduction
特徴は重要です。過去10年間のさまざまな視覚認識タスクの進歩は、SIFT [29]とHOG [7]の使用に大きく依存してきました。しかし、標準的な視覚認識タスクであるPASCAL VOC物体検出[15]のパフォーマンスを見ると、2010年から2012年の間は進歩が遅れており、アンサンブルシステムを構築したり、成功した手法のマイナーなバリエーションを採用したりすることでわずかな進歩しか得られていないことが一般的に認められています。

SIFTとHOGはブロック単位の方位ヒストグラムであり、霊長類の視覚経路の最初の皮質領域であるV1の複合体細胞と大まかに関連付けることができる表現です。しかし、認識はその数段階下流で行われることもわかっており、視覚認識のためには、より情報量の多い特徴量を計算するための階層的な多段階のプロセスが存在する可能性があることを示唆しています。

福島の「ネオコグニトロン」[19]は、パターン認識のための生物学にヒントを得た階層的・シフト不変のモデルであり、まさにそのようなプロセスの初期の試みでした。ただし、ネオコグニトロンには、教師付きトレーニングアルゴリズムがありませんでした。Rumelhartら[33]をベースに、LeCunら[26]は、バックプロパゲーションによる確率的勾配降下が、ネオコグニトロンを拡張したモデル群である畳み込みニューラルネットワーク（CNN）のトレーニングに有効であることを示しました。

CNNは1990年代に盛んに使用されていましたが（[27]など）、サポートベクターマシンの台頭により時代遅れになりました。 2012年に，Krizhevskyら[25]がImageNet Large Scale Visual Recognition Challenge (ILSVRC) [9, 10]で画像の分類精度を大幅に向上させ，CNNへの関心を再燃させた．彼らの成功は，120万枚のラベル付き画像を用いて大規模なCNNを学習させ，LeCunのCNNにいくつかの工夫（例えば，max(x; 0)による非線形性の修正や "dropout "正則化）を加えたことによります．

ILSVRC 2012 のワークショップでは、ImageNet の結果の重要性について活発な議論が行われました。その中心的な論点は以下のようなものです。
> ImageNet上のCNNの分類結果は、PASCAL VOCチャレンジの物体検出結果にどの程度まで一般化できるのか？

我々は、画像分類と物体検出の間のギャップを埋めることで、この疑問に答えます。本論文は、CNNが、より単純なHOGのような機能に基づくシステムと比較して、PASCAL VOCでの物体検出性能を劇的に向上させることができることを示した最初のものです。この結果を達成するために、我々は2つの問題に焦点を当てました。すなわち、ディープネットワークによる物体の局所化と、少量の注釈付き（アノテーションされた）検出データのみで大規模モデルをトレーニングすることです。

画像分類とは異なり、検出には、画像内の（多くの）物体を特定する必要があります。1つのアプローチでは、局所化（ローカライゼーション）を回帰問題として組み立てます。しかし、Szegedyら[38]の研究では、この方法は実際にはうまくいかないかもしれないことが示されています（彼らはVOC 2007のmAPが30.5%であるのに対し、私たちの方法では58.5%であったと報告しています）。もう一つの方法は、スライディングウィンドウ検出器を構築することです。 CNNは少なくとも20年以上前からこの方法で使用されており、通常は顔[32, 40]や歩行者[35]のような制約のある物体カテゴリに対して使用されています。高い空間分解能を維持するために、これらのCNNには通常、2つの畳み込み層とプーリング層しかありません。私たちは、スライディングウィンドウアプローチの採用も検討しました。しかし、5つの畳み込み層を持つ我々のネットワークの上位のユニットは、入力画像に非常に大きな受容野(195 $\times$ 195ピクセル)とストライド(32 $\times$ 32ピクセル)を持っており、スライディングウィンドウのパラダイム内での正確なローカリゼーションは未解決の技術的課題となっています。

その代わりに、物体検出[39]とセマンティックセグメンテーション[5]の両方で成功している「領域を用いた認識」パラダイム[21]を用いて、CNNローカリゼーションの問題を解決します。この方法では、入力画像に対して約2000のカテゴリに依存しない領域の提案を生成し、CNNを使用して各提案から固定長の特徴ベクトルを抽出し、各領域をカテゴリ固有の線形SVMで分類します。単純な手法（アフィン画像ワーピング）を用いて、領域の形状に関係なく、各領域提案からの固定サイズのCNN入力を計算します。図1に手法の概要と結果の一部を示す。私たちのシステムは領域提案とCNNを組み合わせているので、この手法をR-CNNと呼びます．R-CNN: Regions with CNN features.

この論文の更新版では、200クラスのILSVRC2013検出データセットでR-CNNを実行することにより、R-CNNと最近提案されたOverFeat [34]検出システムを直接比較します。OverFeatはスライディングウィンドウCNNを検出に使用しており、これまでILSVRC2013の検出では最も性能の高い方法でしたが、R-CNNはOverFeatを大幅に上回り，mAPが31.4%であったのに対し，24.3%であったことがわかりました．

検出で直面する2番目の課題は、ラベル付けされたデータが不足しており、現在利用可能な量では大規模なCNNを訓練するには不十分であるということです。この問題に対する従来の解決策は，教師なしの事前学習を行い，その後に教師ありの微調整を行うことです（例えば，[35]）。本論文の2番目の原則的な貢献は、大規模な補助データセット(ILSVRC)での教師付き事前学習と、小さなデータセット(PASCAL)での領域固有の微調整が、データが少ないときに大規模のCNNを学習するための効果的なパラダイムであることを示すことです。私たちの実験では、検出を微調整すると、mAPのパフォーマンスが8パーセント向上します検出のための微調整により，mAPの性能が8%ポイント向上した．微調整後，VOC 2010では54%のmAPを達成したが，高度に調整されたHOGベースの変形部分モデル(DPM)では33%でした[17, 20]。また、同時期に行われたDonahueらの研究[12]も紹介します。彼らの実験は、KrizhevskyのCNNがブラックボックス特徴抽出器として（微調整なしで）使用でき、シーン分類、きめ細かな部分分類、ドメイン適応を含むいくつかの認識タスクで優れた性能を発揮することを示している。

私たちのアプローチの故障モードを理解することは、それを改善するためにも重要です。そのため、Hoiemらの検出分析ツールの結果を報告します。 [23]。この分析の直接の結果として、単純なバウンディングボックス回帰法が、支配的なエラーモードである誤ローカリゼーションを大幅に減少することを示します。

技術的な詳細を作成する前に、R-CNNはリージョンで動作するため、セマンティックセグメンテーションのタスクに拡張することは自然であることに注意してください。マイナーな変更により、PASCAL VOCセグメンテーションタスクで競争力のある結果を達成し、VOC 2011テストセットでの平均セグメンテーション精度は47.9％です。

# 2. Object detection

我々の物体検出システムは3つのモジュールから構成されている。最初のモジュールでは、カテゴリに依存しない領域提案を生成します。これらの提案は、我々の検出器で利用可能な候補検出のセットを定義します。 2番目のモジュールは、各領域から固定長の特徴ベクトルを抽出する大規模な畳み込みニューラルネットワークである。3番目のモジュールは、クラス固有の線形SVMのセットである。このセクションでは、各モジュールの設計決定、テスト時間の使用方法、パラメータの学習方法の詳細、およびPASCAL VOC 2010-12での結果を示す。

# 2.1.  Module design

## Region proposals

最近の様々な論文では、カテゴリに依存しない領域提案を生成するための手法が提供されている。例えば，オブジェクト性[1]，選択的探索[32]，カテゴリに依存しないオブジェクト提案[11]，制約付きパラメトリック最小カット(CPMC) [5]，マルチスケールコンビナトリアルグルーピング[3]，そしてCiresanら[6]は，領域提案の特殊なケースである規則的な間隔の四角い作物にCNNを適用して有糸分裂細胞を検出しています．R-CNNは特定の領域提案法には依存しないが，先行する検出作業（例えば，[32, 35]）との制御された比較を可能にするために選択的探索を用いる．

## Feature extraction (特徴抽出).

Krizhevskyetal[22]によって記述されたCNNのCaffe[21]実装を用いて，各領域提案から4096次元の特徴ベクトルを抽出する． 特徴量は，平均減算された227 $\times$ 227のRGB画像を，5つの畳み込み層と2つの完全に接続された層を介して前方に伝播させることによって計算される． ネットワークアーキテクチャの詳細については、[21, 22]を参照されたい。

領域提案の特徴を計算するためには，まず，その領域の画像データをCNNと互換性のある形に変換しなければならない（そのアーキテクチャでは，固定の 227 $\times$ 227 ピクセルサイズの入力が必要である）．任意の形をした領域を変換する方法は数多くあるが，その中でも最も単純なものを選ぶ．候補領域のサイズやアスペクト比にかかわらず，その周囲の狭い境界ボックス内のすべてのピクセルを必要なサイズにワープする．ワープする前に、ワープしたサイズでは、元のボックスの周りにワープした画像コンテキストのピクセル数がちょうど $p$ 個になるように、タイトバウンディングボックスを拡張します（ここでは、$p=16$ を使用します）。図2は、歪んだ学習領域のランダムなサンプリングを示しています。 補足資料では、ワープの代替案について説明しています。

# 2.3. Training

## Supervised pre-training(教師付き事前学習).

我々は，画像レベルのアノテーションのみを用いて，大規模な補助データセット（ILSVRC2012分類）上でCNNを識別的に事前学習させた（このデータではバウンディングボックスラベルは利用できない）．事前学習はオープンソースのCaffe CNNライブラリ[24]を用いて行った．簡単に言えば、我々のCNNはKrizhevskyら[25]の性能とほぼ一致しており、ILSVRC2012分類検証セットでは2.2%ポイント高いトップ1エラー率を得ている。 この不一致は，訓練プロセスの単純化によるものである．

## Domain-specific fine-tuning(ドメイン固有の微調整).

新しいタスク（検出）と新しい領域（ゆがんだ提案ウィンドウ）に適応させるために，ゆがんだ領域の提案のみを用いた CNN パラメータの確率的勾配降下（SGD）学習を継続する．CNNのImageNet固有の1000個の分類層をランダムに初期化された(N+1)個の分類層(ここでNは物体クラスの数、背景は1)に置き換える以外は、CNNアーキテクチャは変更していない。VOCでは $N = 20$ 、ILSVRC2013では $N = 200$ である。0.5 IoU 以上の重なりを持つ領域提案をすべてそのボックスのクラスの正、それ以外を負として扱う。 我々は、0.001（初期の事前学習率の1/10）のSGD ata学習レートを開始し、初期化を妨害しないようにしながら、微調整して進歩を遂げることができます。各 SGD の反復処理では、サイズ 128 のミニバッチを構築するために、32 個の正の窓（全クラスにわた り）と 96 個の背景窓を一様にサンプリングします。 ポジティブウィンドウはバックグラウンドに比べて非常に稀であるため、サンプリングをポジティブウィンドウに偏らせています。

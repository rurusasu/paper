# Semantic Pose Using Deep Networks Trained on Synthetic RGB-D

# 備考

Why RGB-D?
* クラッタ(ノイズ)の影響を受けにくく，クラスを超えて一般化された幾何学的特徴を持つ
* 観測されたことのないオブジェクトインスタンスのポーズを推定する必要があるから


# Abst

本研究では，RGB-D画像から室内シーンを理解するという問題に取り組む．具体的には、一般的な家具のクラスのインスタンス、その空間的広がり、および一般化されたクラスモデルに対するポーズを見つけることを提案する。これを実現するために、我々は、クラス、ポーズ、および物体の位置を同時に予測する、深くて広い、複数出力の畳み込みニューラルネットワーク（CNN）を使用します。また、大規模なRGB-Dトレーニングセット（特にポーズ付きのもの）がないため、オンザフライのレンダリングパイプラインを使用して、トレーニングと並行してリアルな散らかった部屋のシーンを生成しています。そして、比較的少量のアノテーション付きRGB-Dデータを用いて転移学習を行ったところ、我々のモデルは難易度の高い実シーンでもうまくアノテーションを行うことができることがわかった。その結果、我々のモデルは、難易度の高い実シーンの注釈付けを成功させることができました。重要なのは、我々の学習したネットワークは、非常に限られた手がかりからクラスとポーズを推測し、非常に高い精度で、非常に散らかったシーンのノイズやスパースな観測を理解することができることです。さらに、我々のニューラルネットワークは中程度の深さしかなく、クラス、ポーズ、位置を同時に計算するため、既存の手法に比べて全体の実行時間が大幅に短縮され、すべての出力パラメータを並行してシミュレートすることができます。

# Intro

自律型システムが研究室の枠を超えて活躍するためには、必然的に遭遇するであろう乱雑な屋内環境を理解する能力を身につける必要があります。そのため、本研究では、単一の人工ニューラルネットワークを用いて、上記のタスクを協調して実行するアーキテクチャを提案する。

特に、初めて見る物体の位置を分類しようとする場合には、非常に困難な作業となります。2次元の色情報のみを考慮すると、この問題はさらに悪化します。というのも、乱雑な環境下では、識別可能な物体の見え方が大きく変化しやすいからです。一方、3次元の幾何学的特徴は、クラッターの影響を受けにくく、（特に家具の場合）クラスを超えて一般化された幾何学的特徴を持つ傾向があります。そこで今回は、標準的なRGBチャンネルに加えて、3Dジオメトリック特徴を使用しています。

これは、これまでに観測されたことのないオブジェクトインスタンスのポーズを推定する必要があるからです。例えば、人間が椅子に座るのを手助けするというタスクを考えてみましょう。手助けをするためには、今まで見たことのない種類の椅子であっても、背もたれ、座面、支えている脚のポーズを決定できなければなりません。この研究では、合成モデルで学習した広くて深い多段式のCNNを使って、このようなタスクが驚くほどの精度で可能であることを示すものである。さらに、多くの種類の家具が混在する複雑なシーンでも、そのようなポーズを推定できることを示します（例えば、図3参照）。

図1に示したアプローチは、比較的複雑なCNNアーキテクチャを用いて、物体のクラス、ポーズ、位置の推定という3つのサブタスクを同時に解決するものです。このネットワークの特徴は、クラスの出力をポーズと位置を計算するネットワーク層に再結合していることであり、単一のアーキテクチャで複数のクラスのポーズを正確に決定することができる。さらに、この大規模なネットワークの学習には、何千もの3Dオブジェクトモデルのデータセットからランダムに配置されたインスタンスで構成された、合成レンダリングされたRGB-Dシーンを使用しています。学習シーンは、CPUとセカンダリGPU上で、プライマリGPUでの学習と同時にオンザフライで生成されるため、実質的に制限のないサイズの学習セットを、完全に隠蔽された計算コストで得ることができます。最後に、実際の屋内RGB-Dシーンのモダリティにネットワークを適応させるために、実際の注釈付き画像の小さなセットを使って、少数の転移学習を繰り返し行いました。

本手法の有効性を実証するために，合成シーンと実写シーンの両方で様々な実験を行いました．難しい実データセットにおいて、我々のポーズ推定と分類の結果は、既存の手法を上回るものであった。また、実データと合成データの両方で質的・量的な結果を示し、本システムがシーンの意味的な理解を抽出する能力があることを示しました。さらに、これらのタスクをネットワークを介して1回のフォワードパスで行うことで、既存の手法よりも大幅に速い結果を得ることができます。

# 2. Synthetic RGB-D Scenes

RGB-DデータにディープCNNを使用する際の主な障害の一つは、大規模なアノテーションを施したデータセットがないことである。特にポーズデータでは、ディープネットワークの学習に必要なサイズのアノテーションを行うことができません。一方、合成データは、ラベル付きのセグメンテーションと正確なポーズを無料で提供しているが、写真のようにリアルなシーンをレンダリングすることが難しいため、未だに広く利用されていない。しかし、RGB-Dデータは、奥行きのあるデータを簡単にリアルに表現できるため、合成データの利用に適しています。センサーのアクティブモデルをシミュレートするだけで、照明やテクスチャー、表面の構成などはほとんど無視できます。

合成シーンは，仮想の部屋の中にランダムにオブジェクトモデルを順次配置していくことで生成されます．各オブジェクトを配置する際には、そのメッシュが他のオブジェクトや部屋の表面と交差しないようにします。さらに、シーンのリアリティを高めるために、コンテキストキューを利用しています。大きな家具（ソファやベッドなど）は壁の近くに、椅子はテーブルや机の近くに、モニターは常に机の上に配置するようにしています。また、部屋の天井にある光源をランダムに配置することで、レンダリングされた輝度画像に影ができることをシミュレートしています。図3にその例を示します。今回使用したデータセットは、トレーニング時にさらにランダムなシーンを生成するコードとともに、コミュニティに公開される予定です。

## 2.1. Rendering & Camera Model

BlenSorセンサーシミュレーションツールボックス[2]を用いて，ランダムに生成されたシーンのリアルなRGB-Dレンダリングを行いました．レイトレーシングを用いることで，Kinect センサーの実際のジオメトリを再現し，シーンへの IR パターンの投影と反射の観察を忠実にシミュレートしています．Kinectタイプのセンサーは反射があると故障してしまうため、レイトレーシングは1ホップに限定することで安全に行うことができます。さらに，Kinectが深度測定を行うために必要とする9x9の相関窓をシミュレートし[13]，視差測定にPerlinノイズを加えています．また，標準的なBlenderのパイプラインを使用してRGB画像をレンダリングしましたが，オブジェクトモデルにテクスチャがなく，照明モデルも単純化されているため，フォトリアリスティックではありませんでした．しかし，使用しているのは強度チャンネルのみであるため，このシンプルなRGBレンダリングで十分であることがわかりました．特に，実際のセンサー画像に適応するために伝達学習を使用していることを考えると，これで十分です．

## 2.2. Models

モデルは基準となるポーズに合わせなければならないので、単にインターネットからCADモデルを引っ張ってくることはできません。幸いなことに、Princeton ModelNet10データセット[16]には、バスタブ、ベッド、椅子、机、ドレッサー、モニター、ナイトスタンド、ソファ、テーブル、トイレという10種類のオブジェクトカテゴリについて、様々なポーズのモデルが用意されています。 用意されている標準的なトレーニングとテストの分割を使用しています。モデルはスケール正規化されていないので，クラスごとに妥当な値の範囲を選び，その範囲内に入るようにモデルをランダムにリスケールします。モデルは、合成した部屋の床または支持面のランダムな位置に、床に垂直な軸の周りにランダムな回転で挿入されます。

# 3. Network Architecture

我々はいくつかの異なるネットワーク構成をテストしたが，いずれも入力に少なくとも2つのKrizhevskyスタイル[9]（すなわちConv-ReLU-Pooling）の畳み込み層が含まれていた．最も成功したモデルは、図5に示すように、最近の「Inception」アーキテクチャに似た構成で、連続したNetwork-in-Network（NiN）層[11]を使用しています。分類には，2つの隠れ層を持つ別々の多層パーセプトロンを使用しています．さらに，クラスの出力をポーズ分類器と位置分類器の第2の隠れ層に接続しています．


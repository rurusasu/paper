# Aligning 3D Models to RGB-D Images of Cluttered Scenes

# Abst

本研究の目的は、RGB-Dのシーンにあるオブジェクトを、ライブラリにある対応する3Dモデルで表現することです。本研究では、まずシーン内の物体を検出してセグメント化し、次に畳み込みニューラルネットワーク（CNN）を用いて物体の姿勢を予測することで、この問題に取り組んでいます。このCNNは、合成物体のレンダリングを含む画像のピクセル表面法線を用いて学習されます。 この方法は、実データでテストしたところ、実データで学習した他のアルゴリズムよりも優れていました。次に、この粗いポーズ推定値と推測されるピクセルサポートを用いて、少数のプロ・プロトタイプモデルをデータに合わせ、最もフィットしたモデルをシーンに配置します。  この結果、現在の最新技術[34]と比較して、3D検出タスクの性能が48%相対的に向上し、しかも1桁速いことが確認されました。

# Intro

![fig1]()


シーンを真に理解するには、見えるものだけでなく、見えないものも含めて推論する必要があります。例えば、図1の画像を考えてみましょう。 椅子という物体を認識した後、その物体がどこまで奥行きがあるのか、別の視点からはどのように見えるのか、といったことをかなり理解しています。 このような理解をコンピュータビジョンシステムで実現するためには、椅子の3DCADモデルをレンダリングすることで、椅子のピクセルを「再配置」することが考えられます。 このように3DCADモデルとの対応を明示的に行うことで、オブジェクト検出、セマンティックセグメンテーション、インスタンスセグメンテーション、きめ細かなカテゴリー化、ポーズ推定といった従来のコンピュータビジョンのアルゴリズムでは得られなかった豊かな表現が可能になります。 これらのタスクは、軌道最適化、動作計画、把持推定など、ロボット工学の観点からは十分ではありません。我々の提案するシステムは、散らかった屋内シーンの単一のRGB-D画像から始まり、図1に示すような出力を生成します。我々のアプローチは、以下のことを可能にします。
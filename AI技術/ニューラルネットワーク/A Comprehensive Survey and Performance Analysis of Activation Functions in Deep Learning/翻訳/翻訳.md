# A Comprehensive Survey and Performance Analysis of Activation Functions in Deep Learning

# 備考
## 著者
Shiv Ram Dubey, Satish Kumar Singh, Bidyut Baran Chaudhuri
## 掲載
arXiv:2109.14545 [cs]

# Abst

概要-ニューラルネットワークは，数多くの問題を解決するために，近年，非常に大きな成長を遂げている。さまざまなタイプの問題に対処するために，さまざまなタイプのニューラルネットワークが導入されている。しかし，ニューラルネットワークの主な目的は，階層化された層を用いて，非線形に分離可能な入力データを，より線形に分離可能な抽象的特徴に変換することである。これらの層は、線形関数と非線形関数の組み合わせです。最もポピュラーで一般的な非線形層は、ロジスティックシグモイド、Tanh、ReLU、ELU、Swish、Mishなどの活性化関数（AF）です。この論文では、深層学習のためのニューラルネットワークにおけるAFについて、包括的な概要と調査を示しています。ロジスティックシグモイドやTanhベース、ReLUベース、ELUベース、Learningベースといった異なるクラスのAFをカバーしている。また、出力範囲、単調性、平滑性など、AFのいくつかの特徴も指摘されています。また、異なるネットワークを持つ18種類のAFについて、異なる種類のデータに対する性能比較を行った。AFの洞察は、研究者にとってはさらなる研究のために、実務者にとってはさまざまな選択肢の中から選ぶために役立つように提示されている。実験に使用したコードは、https:／／github.com／shivram1987／ActivationFunctions.Netで公開しています。

# 1. Intro

近年，深層学習は，顔分析[2]，[113]，予測評価[74]，感情分析[143]，[152]，ハイパースペクトル画像分析[144]，画像合成と意味操作[1]，デジタル画像の拡張[72]，画像検索[33]などの困難な問題を解決するために，大きな成長を見せている．データから抽象的な特徴を学習するための深層学習では，多層パーセプトロン（MLP）[30]，畳み込みニューラルネットワーク（CNN）[82]，[73]，リカレントニューラルネットワーク（RNN）[54]，Generative Adversarial Networks（GAN）[12]など，さまざまな種類のニューラルネットワークが開発されている．ニューラルネットワークの重要な側面には，重みの初期化 [104]，損失関数 [130]，正則化 [102]，オーバーフィッティングの制御 [18]，活性化関数 [136]，最適化 [35]などがあります．

活性化関数(AF)は，非線形変換により抽象的な特徴を学習することで，ニューラルネットワークにおいて非常に重要な役割を果たしている[36]．活性化関数の一般的な特性は，以下の通りである： a) ネットワークの学習収束性を向上させるために，最適化ランドスケープに非線形の曲率を加えること， b) モデルの計算量を広範囲に増加させないこと， c) 学習中の勾配の流れを妨げないこと， d) ネットワークのより良い学習を促進するために，データの分布を保持すること．近年、上記の特性を実現するために、深層学習のためのいくつかのAFが検討されている。本調査では、ニューラルネットワークにおけるAFの分野での発展を紹介する。異なるAFの洞察は、深層学習コミュニティに利益をもたらす推論とともに提示される。本調査の主な貢献は以下のように概説される。

1. 今回の調査では、広範囲のAFに対して詳細な分類を行っています。また，ロジスティックシグモイド／タン，整流ユニット，指数ユニット，適応型AFなど，非常に包括的なAFを収録している。
1. このサーベイでは、様々な視点から分析された最先端のAFが紹介されています。特に、深層学習のためのAFの進歩を取り上げています。
1. 本調査では、様々なタイプのデータに対するAFの適合性を示すために、簡単なハイライトと重要な議論でAFを要約する（表VI参照）。
1. 本調査は、その重要性を示すために、既存の調査／分析と比較している（表 VII 参照）。
1. 本論文では、異なるモダリティの4つのベンチマークデータセットにおいて、異なるタイプのネットワークを持つ18種類の最先端のAFを使用した場合の性能比較も示す（表VIII、IX、XI参照）。

AFの進化をセクションIIで説明している。ロジスティックシグモイドとタン、整流されたAF、指数関数的なAF、適応的なAF、そして雑多なAFの進歩を、それぞれ第III章、第IV章、第V章、第VI章、第VII章にまとめている。セクションVIIIでは，AFのいくつかの側面について議論する．セクションIXでは、包括的な性能分析を行う。セクションXでは，結論と推奨事項をまとめている．

---

![fig1](https://raw.githubusercontent.com/rurusasu/paper/master/AI%E6%8A%80%E8%A1%93/%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF/A%20Comprehensive%20Survey%20and%20Performance%20Analysis%20of%20Activation%20Functions%20in%20Deep%20Learning/%E7%94%BB%E5%83%8F/fig1.png)

図1: リニア，ロジスティックシグモイド，tanhの各AFをそれぞれ赤，青，緑で表した図。x軸とy軸はそれぞれ入力と出力を表す

---

# 2. 活性化関数の発展

線形関数とは，入力 $x$ に対して $c \times x$ を出力する，$c$ を定数とする単純なAFであると考えることができる．図1では，$c = 1$ の場合の線形AFを赤で示している．すなわち，同一性関数である．線形AFは、ネットワークに非線形性を付加しないことに注意してほしい。しかし，ニューラルネットワークには非線形性を導入する必要がある．そうしないと，ニューラルネットワークはいくつかの層を持っているにもかかわらず，入力の線形関数として出力を生成してしまいます．さらに、実際のデータは一般的に線形に分離できないため、非線形層はデータを特徴空間に非線形に投影するのに役立ち、さまざまな目的関数に使用することができます。このセクションでは、深層学習のためのAFの進化について概観します。異なる特性と特徴的なタイプの観点から、図2に分類を示す。

---

![fig2](https://raw.githubusercontent.com/rurusasu/paper/master/AI%E6%8A%80%E8%A1%93/%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF/A%20Comprehensive%20Survey%20and%20Performance%20Analysis%20of%20Activation%20Functions%20in%20Deep%20Learning/%E7%94%BB%E5%83%8F/fig2.png)

活性化関数の分類。

---

ロジスティックシグモイド／Tanhユニットベース活性化関数。ニューラルネットワークに非線形性を導入するために，初期にはロジスティックシグモイドAFやTanh AFが使われていました。ロジスティックシグモイドやタンの活性化関数を人工ニューロンに適用した動機は、双極性ニューロンの発火でした。ロジスティックシグモイドAFは非常にポピュラーで伝統的な非線形関数である。これは次のように与えられる

$$
Logistic Sigmoid(x) = 1 / (1 + \exp^{-x})
$$

このAFは，図1に青色で示したように，0と1の間の出力を潰してしまう．ロジスティックシグモイド関数の出力は、高い入力と低い入力で飽和してしまい、グラデーションが消えてしまい、学習プロセスが死んでしまいます。また、出力がゼロ中心にならないため、収束性が悪くなります。Tanh関数もニューラルネットワークのAFとして使われています。Tanh関数はロジスティックシグモイド関数に似ていますが、図1に緑色で示されているように、ゼロ中心の性質を持っています。Tanh関数は次のように書かれる。
$$
Tanh(x) = (\exp^x - \exp{-x}) / (\exp^x + \exp^{-x})
$$
Tanh関数も入力を潰しますが、その範囲は$[-1, 1]$です。ロジスティックシグモイド関数の欠点である消失勾配や計算量の多さは、Tanh関数にも当てはまります。ロジスティックシグモイド関数とTanh関数は，主に消失勾配に悩まされている．ロジスティックシグモイドとTanh AFに基づいていくつかの改良が提案されており，それらについてはセクションIIIで詳しく説明する．

Rectified Linear Unit Based Activation Functions: 前述のロジスティックシグモイドやTanhベースのAFでは，出力の飽和や複雑さの増大が大きな課題となっていた．Rectified Linear Unit (ReLU) [103]は、その単純さと性能の向上により、最先端の活性化関数となっている。ReLUはAlexNetモデル[82]でも使用されています。ReLUは，負の値を利用できない，非線形性が制限されている，出力が制限されているなどの欠点を解決するために，さまざまなバリエーションが研究されています（セクションIVで詳しく説明します）．

指数ユニットベースの活性化関数。ロジスティックシグモイドやTanhベースの活性化関数が直面する大きな問題は、大きな正負の入力に対して出力が飽和してしまうことです。同様に、ReLUベースの活性関数では、負の値が十分に利用されず、勾配が消えてしまうという問題がある。これらの制限に対処するために、指数関数ベースのAFが使われてきた。Exponential Linear Unit (ELU) [29]に基づくAFは、指数関数の助けを借りて負の値を利用する。ELUを用いたAFは、文献にも紹介されているが、その詳細はセクションVで紹介する。

学習／Adaptive Activation Functions: Sigmoid, Tanh, ReLU, ELU に基づく活性化関数の多くは手動で設計されており，データの複雑さを利用できない場合がある．最近のトレンドは，学習ベースの適応型活性関数である．例えば，Adaptive Piecewise Linear (APL) [3]やSwish [118]は，それぞれ2つと1つの学習可能なパラメータを持っている．最近では，セクション VI で説明するように，いくつかの学習ベースの AF が提案されている．

その他の活性化関数: 近年，セクションVIIで紹介するように，その他のAFも多く研究されている．これらの活性化関数には、ソフトプラスユニット、確率関数、多項式関数、カーネル関数などがある。

---

表 I: Diminishing gradients，Limited non-linearity，Optimization difficulty，Lack of adapttibility，Computational inefficiencyの観点から見たプライマリーAFの長所と短所。

![tab1](https://raw.githubusercontent.com/rurusasu/paper/master/AI%E6%8A%80%E8%A1%93/%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF/A%20Comprehensive%20Survey%20and%20Performance%20Analysis%20of%20Activation%20Functions%20in%20Deep%20Learning/%E7%94%BB%E5%83%8F/tab1.png)

---

表 I では、 勾配の減少、 非線形性の制限、 最適化の困難さ、 計算の非効率性、 適応性の欠如といった点で、 既存の AF の利点と欠点を示している。既存のAFにおけるこれらの限界は、本稿のさらなるセクションで調査したように、最近のAFの開発の原動力となっている。

# 3. ロジスティック・シグモイドとTanh・ベースのAFs

ロジスティックシグモイドやTanhといった従来のAFは、ニューラルネットワークの黎明期には非常に多く使用されていました。しかし、これらのAFは出力が飽和しているため、深層ネットワークを学習するにはハードルが高かった。これらのAFを改良する試みもいくつかなされている。表2は，ロジスティックシグモイドとTanhベースのAFを，その方程式と，範囲，パラメトリック，平滑性，有界性などの特性の観点から比較したものである．

---

表2: ロジスティックシグモイドとTanhベースの活性化関数の、等式、範囲、パラメトリック、単調、平滑性、有界性などの特性のまとめ。

![tab2]

---

Tanh の出力範囲の制限とゼログラデーションの問題を解決するために，[84]ではスケーリングされた双曲タンジェント (sTanh) が使用されている．パラメトリックシグモイド関数 (PSF) は，連続的で微分可能，かつ有界の関数として提案されており，$(1／(1＋\exp^{-x}))^m$ ($m$ はハイパーパラメータ) となる．また，生成された特徴量の対称性を維持するAFとして，シフトした対数シグモイドの和が検討されている[129]．Rectified Hyperbolic Secant (ReSech) AF は，微分可能で，対称性があり，有界である [107]．しかし，大きな正の入力と大きな負の入力の両方で飽和するため，消失勾配の問題がある．ロジスティックシグモイドやTanh AFの傾きが原点付近で一様であるため，深層ネットワークの学習が困難になる[149]．この制限を最小化するために，Scaled Sigmoid (sSigmoid) は $(4 \times Sigmoid(x) - 2)$ として開発され，Penalized Tanh (pTanh) は $x ≥ 0$ では $Tanh(x)$ ，$x < 0$ では $\frac{Tanh(x)}{4}$ として開発された． しかし，sSigmoid と pTanh の AF は消失勾配の問題にも悩まされる．自然言語処理(NLP)タスクでは，pTanh AFがより良い性能を発揮することが注目されている[41]．

勾配消失問題を解決するために，ノイジーAFが開発された[57]．ノイズを追加することで，飽和領域でもグラジエントが容易に流れるようになります．消失勾配問題は，Tanhに似ていますが，勾配をスケーリングしたHexpo関数[79]によって最小化されます．シグモイド関数の出力は，シグモイド重み付け線形単位(SiLU)AF[43]で入力と乗算される．同時に，シグモイドの消失勾配問題をシグモイド関数と線形関数の部分的な組み合わせで解決する改良型ロジスティックシグモイド(ISigmoid)AF[115]が提案されている．Linearly scaled hyperbolic tangent (LiSHT) AF は，消失勾配問題を解決するために Tanh を線形にスケーリングするものである [119]．LiSHT関数は対称的ですが，非結合かつ非負の出力しか含まないという欠点があります．Elliott AF [46] は，特性図の点でシグモイド関数に似ており，$\frac{0.5 \times x}{1 + |x|} + 0.5$ で定義される．Soft-Root-Sign (SRS) AF [159] は，$x／(\frac{x}{\alpha}+\exp^{\frac{x}{\beta}})$ と定義され， $\alpha$ と $\beta$ は学習可能なパラメータである．追加のパラメータを使用すると，SRS関数の複雑さが増す．Sigmoid/Tanh AFのほとんどの改良版は，消失勾配の問題を解決しようとしている．しかし、この問題はほとんどのAFに残っている。


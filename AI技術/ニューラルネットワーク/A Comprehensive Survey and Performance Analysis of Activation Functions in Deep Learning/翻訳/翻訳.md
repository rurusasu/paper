# A Comprehensive Survey and Performance Analysis of Activation Functions in Deep Learning

# 備考
## 著者
Shiv Ram Dubey, Satish Kumar Singh, Bidyut Baran Chaudhuri
## 掲載
arXiv:2109.14545 [cs]

# Abst

概要-ニューラルネットワークは，数多くの問題を解決するために，近年，非常に大きな成長を遂げている。さまざまなタイプの問題に対処するために，さまざまなタイプのニューラルネットワークが導入されている。しかし，ニューラルネットワークの主な目的は，階層化された層を用いて，非線形に分離可能な入力データを，より線形に分離可能な抽象的特徴に変換することである。これらの層は、線形関数と非線形関数の組み合わせです。最もポピュラーで一般的な非線形層は、ロジスティックシグモイド、Tanh、ReLU、ELU、Swish、Mishなどの活性化関数（AF）です。この論文では、深層学習のためのニューラルネットワークにおけるAFについて、包括的な概要と調査を示しています。ロジスティックシグモイドやTanhベース、ReLUベース、ELUベース、Learningベースといった異なるクラスのAFをカバーしている。また、出力範囲、単調性、平滑性など、AFのいくつかの特徴も指摘されています。また、異なるネットワークを持つ18種類のAFについて、異なる種類のデータに対する性能比較を行った。AFの洞察は、研究者にとってはさらなる研究のために、実務者にとってはさまざまな選択肢の中から選ぶために役立つように提示されている。実験に使用したコードは、https:／／github.com／shivram1987／ActivationFunctions.Netで公開しています。

# 1. Intro

近年，深層学習は，顔分析[2]，[113]，予測評価[74]，感情分析[143]，[152]，ハイパースペクトル画像分析[144]，画像合成と意味操作[1]，デジタル画像の拡張[72]，画像検索[33]などの困難な問題を解決するために，大きな成長を見せている．データから抽象的な特徴を学習するための深層学習では，多層パーセプトロン（MLP）[30]，畳み込みニューラルネットワーク（CNN）[82]，[73]，リカレントニューラルネットワーク（RNN）[54]，Generative Adversarial Networks（GAN）[12]など，さまざまな種類のニューラルネットワークが開発されている．ニューラルネットワークの重要な側面には，重みの初期化 [104]，損失関数 [130]，正則化 [102]，オーバーフィッティングの制御 [18]，活性化関数 [136]，最適化 [35]などがあります．

活性化関数(AF)は，非線形変換により抽象的な特徴を学習することで，ニューラルネットワークにおいて非常に重要な役割を果たしている[36]．活性化関数の一般的な特性は，以下の通りである： a) ネットワークの学習収束性を向上させるために，最適化ランドスケープに非線形の曲率を加えること， b) モデルの計算量を広範囲に増加させないこと， c) 学習中の勾配の流れを妨げないこと， d) ネットワークのより良い学習を促進するために，データの分布を保持すること．近年、上記の特性を実現するために、深層学習のためのいくつかのAFが検討されている。本調査では、ニューラルネットワークにおけるAFの分野での発展を紹介する。異なるAFの洞察は、深層学習コミュニティに利益をもたらす推論とともに提示される。本調査の主な貢献は以下のように概説される。

1. 今回の調査では、広範囲のAFに対して詳細な分類を行っています。また，ロジスティックシグモイド／タン，整流ユニット，指数ユニット，適応型AFなど，非常に包括的なAFを収録している。
1. このサーベイでは、様々な視点から分析された最先端のAFが紹介されています。特に、深層学習のためのAFの進歩を取り上げています。
1. 本調査では、様々なタイプのデータに対するAFの適合性を示すために、簡単なハイライトと重要な議論でAFを要約する（表VI参照）。
1. 本調査は、その重要性を示すために、既存の調査／分析と比較している（表 VII 参照）。
1. 本論文では、異なるモダリティの4つのベンチマークデータセットにおいて、異なるタイプのネットワークを持つ18種類の最先端のAFを使用した場合の性能比較も示す（表VIII、IX、XI参照）。

AFの進化をセクションIIで説明している。ロジスティックシグモイドとタン、整流されたAF、指数関数的なAF、適応的なAF、そして雑多なAFの進歩を、それぞれ第III章、第IV章、第V章、第VI章、第VII章にまとめている。セクションVIIIでは，AFのいくつかの側面について議論する．セクションIXでは、包括的な性能分析を行う。セクションXでは，結論と推奨事項をまとめている．

---

![fig1]

図1: リニア，ロジスティックシグモイド，tanhの各AFをそれぞれ赤，青，緑で表した図。x軸とy軸はそれぞれ入力と出力を表す

---

# 2. 活性化関数の発展

線形関数とは，入力 $x$ に対して $c \times x$ を出力する，$c$ を定数とする単純なAFであると考えることができる．図1では，$c = 1$ の場合の線形AFを赤で示している．すなわち，同一性関数である．線形AFは、ネットワークに非線形性を付加しないことに注意してほしい。しかし，ニューラルネットワークには非線形性を導入する必要がある．そうしないと，ニューラルネットワークはいくつかの層を持っているにもかかわらず，入力の線形関数として出力を生成してしまいます．さらに、実際のデータは一般的に線形に分離できないため、非線形層はデータを特徴空間に非線形に投影するのに役立ち、さまざまな目的関数に使用することができます。このセクションでは、深層学習のためのAFの進化について概観します。異なる特性と特徴的なタイプの観点から、図2に分類を示す。

---

![fig2](https://raw.githubusercontent.com/rurusasu/paper/master/AI%E6%8A%80%E8%A1%93/%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF/A%20Comprehensive%20Survey%20and%20Performance%20Analysis%20of%20Activation%20Functions%20in%20Deep%20Learning/%E7%94%BB%E5%83%8F/fig2.png)

活性化関数の分類。

---

ロジスティックシグモイド／Tanhユニットベース活性化関数。ニューラルネットワークに非線形性を導入するために，初期にはロジスティックシグモイドAFやTanh AFが使われていました。ロジスティックシグモイドやタンの活性化関数を人工ニューロンに適用した動機は、双極性ニューロンの発火でした。ロジスティックシグモイドAFは非常にポピュラーで伝統的な非線形関数である。これは次のように与えられる

$$
Logistic Sigmoid(x) = 1 / (1 + \exp^{-x})
$$

このAFは，図1に青色で示したように，0と1の間の出力を潰してしまう．ロジスティックシグモイド関数の出力は、高い入力と低い入力で飽和してしまい、グラデーションが消えてしまい、学習プロセスが死んでしまいます。また、出力がゼロ中心にならないため、収束性が悪くなります。Tanh関数もニューラルネットワークのAFとして使われています。Tanh関数はロジスティックシグモイド関数に似ていますが、図1に緑色で示されているように、ゼロ中心の性質を持っています。Tanh関数は次のように書かれる。
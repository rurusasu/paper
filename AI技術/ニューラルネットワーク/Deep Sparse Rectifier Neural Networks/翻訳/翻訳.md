# Deep Sparse Rectifier Neural Networks

# 備考
## 著者
Xavier Glorot, Antoine Bordes, Yoshua Bengio

## 掲載
``Deep Sparse Rectifier Neural Networks
," Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTAT), pp. 315--323, 2011.

# Abst

ロジスティック・シグモイド・ニューロンはハイパーボリック・タンジェント・ニューロンよりも生物学的に妥当であるが、後者は多層ニューラル・ネットワークの学習に適している。本論文では、整流ニューロンが生物学的ニューロンのさらに優れたモデルであることを示し、ゼロでの硬い非線形性と非微分性にもかかわらず、双曲接線ネットワークと同等以上の性能が得られ、真のゼロを持つ疎な表現が作成され、自然に疎なデータに驚くほど適していると思われる。深層整流器ネットワークは、ラベルのないデータを使った半教師付きのセットアップを利用できるにもかかわらず、ラベル付きの大規模データセットを使った純粋な教師付きタスクでは、教師なしの事前学習を必要とせずに最高の性能を発揮します。今回の結果は、純粋に教師ありきのニューラルネットワークを学習することの難しさを理解し、教師なしの事前学習を行った場合と行わなかった場合のニューラルネットワークの性能差を縮めるための新たなマイルストーンになると考えられます。


# 2. Background

## 2.1. 脳科学の観察

生物学的なニューロンのモデルでは、活性化関数は、シナプスに入力される信号から現在生じている全入力の関数として期待される発火率である(Dayan and Abott, 2001)。活性化関数は、強い興奮性の入力パターンの反対に対する応答が、それぞれ強い抑制性または興奮性のものである場合には、それぞれ反対称または対称と呼ばれ、この応答がゼロである場合には片側と呼ばれる。計算論的神経科学モデルと機械学習モデルとの間に考慮したい主なギャップは以下の通りです。

- 脳のエネルギー消費に関する研究によると、ニューロンはまばらに分散して情報をエンコードしており（Attwell and Laughlin, 2001）、同時に活動しているニューロンの割合は1〜4％と推定されている（Lennie, 2003）。これは、表現の豊かさと小さな活動電位のエネルギー消費との間のトレードオフに相当する。L1ペナルティのような正則化を追加しないと、通常のフィードフォワードニューラルネットはこの特性を持ちません。例えば、シグモイド活性化は$\frac{1}{2}$付近に定常状態の領域があるため、小さな重みで初期化すると、すべてのニューロンが飽和領域の半分で発火します。これは生物学的にはありえないことで、勾配ベースの最適化に支障をきたします(LeCun et al., 1998; Bengio and Glorot, 2010)。

- **生物学的モデルと機械学習モデルの間の重要な相違点は、非線形活性化関数にあります。** 一般的なニューロンの生物学的モデルである leaky integrate-and-fire (LIF) (Dayan and Abott, 2001)では、図1（左）に示すように、発火率と入力電流の間に以下の関係があります。

$$
f(I) = \left\{
  \begin{matrix}
    \left[\tau \log \left(\frac{E+RI-V_{\tau}}{E+RI-V_{th}} + t_{ref}\right)\right]^{-1} & if \ E+RI > V_{th} \\
    0 & if \ E + RI \leq V_{th}
  \end{matrix}
\right\}
$$

ここで，$t_{ref}$ は不応期 (2つの活動電位の間の最小時間)，$I$ は入力電流，$V_r$ は静止電位，$V_{th}$ は閾値電位 ($V_{th} ＞V_r$の場合)，$R$，$E$，$\tau$ は膜抵抗，電位，時定数である。深層学習やニューラルネットワークの文献でよく使われている活性化関数は、標準的なロジスティック・シグモイドと双曲タンジェント (図1右参照) で、これらは線形変換までは等価である。双曲タンジェントは0で定常状態となるため、最適化の観点からは好まれていますが (LeCun et al., 1998; Bengio and Glorot, 2010)、生体のニューロンにはない0付近での反対称性が強制されます。

---

![fig1](https://raw.githubusercontent.com/rurusasu/paper/master/AI%E6%8A%80%E8%A1%93/%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF/Deep%20Sparse%20Rectifier%20Neural%20Networks/%E7%94%BB%E5%83%8F/fig1.png)

図1: 左: 生物学的データに基づいた一般的な神経活性化関数。右: ニューラルネットワークの文献でよく使われている活性化関数: ロジスティック・シグモイドとハイパーボリック・タンジェント（tanh）。

---

## 2.2. スパース性の利点

スパース性は，計算論的神経科学や機械学習だけでなく，統計学や信号処理の分野でも注目されている概念です (Candes and Tao, 2005)．スパース性は，計算論的神経科学において，視覚系におけるスパース・コーディングの文脈で初めて導入されました (Olshausen and Field, 1997)．スパース性ペナルティは，オートエンコーダーの変形を利用した深層畳み込みネットワークの重要な要素であり (Ranzato et al., 2007, 2008; Mairal et al., 2009)，深層ビリーフネットワークの重要な要素でもある (Lee et al., 2008)．スパース性ペナルティは，計算論的神経科学 (Olshausen and Field, 1997; Doi et al., 2006) や機械学習モデル (Lee et al., 2007; Mairal et al., 2009)，特に深層アーキテクチャ (Lee et al., 2008; Ranzato et al., 2007, 2008) で用いられてきた．しかし，後者では，ニューロンの活性化や発火確率は小さいがゼロではないという結果になる．ここでは、整流性のある非線形性を用いることで、活性化の実質的なゼロが得られ、その結果、真に疎な表現が得られることを示します。このような表現は、次のような理由で計算上魅力的です。

- **情報の離散化。** 深層学習アルゴリズムの目的の1つは、データの変動を説明する要因を切り離すことである(Bengio, 2009)。密な表現は、入力のほとんどすべての変化が表現ベクトルのほとんどのエントリを変更するため、非常に絡み合っている。その代わり、表現が疎であり、かつ入力の小さな変化に頑健であれば、ゼロでない特徴のセットは、入力の小さな変化によってほぼ保存されることになる。

- **効率的な可変サイズの表現。** 異なる入力には異なる量の情報が含まれている可能性があり、コンピュータによる情報表現で一般的な可変サイズのデータ構造を用いた方がより便利に表現できます。アクティブなニューロンの数を変化させることで、与えられた入力に対する表現の効果的な次元と必要な精度をコントロールすることができます。

- **線形分離可能。** 疎な表現は、単純に情報が高次元空間に表現されているため、線形分離可能であるか、または非線形機械が少なく容易に分離できる可能性も高い。その上、これは元のデータ形式を反映していることもあります。例えば、テキスト関連のアプリケーションでは、元の生データはすでに非常にスパースです（セクション4.2参照）。

- **密な分散表現** 密な分散表現は、最も豊かな表現であり、純粋に局所的な表現よりも指数関数的に効率が高くなる可能性があります(Bengio, 2009)。疎な表現の効率はまだ指数関数的に大きく、指数の累乗はゼロではない特徴の数になります。疎な表現は、上記の基準において、良いトレードオフとなる可能性があります。

とはいえ、スパース性を強要しすぎると、モデルの有効容量が減少するため、同じ数のニューロンでも予測性能が低下する可能性があります。

# 3. 深層整流器ネットワーク
## 3.1. 整流器ニューロン

神経科学の文献 (Bush and Sejnowski, 1995; Douglas and al., 2003) によると、大脳皮質ニューロンが最大飽和領域にあることはほとんどなく、その活性化関数は整流器で近似できることが示唆されています。整流型活性化関数を含むニューラルネットワークに関するこれまでの研究のほとんどは、リカレントネットワークに関するものでした (Salinas and Abbott, 1996; Hahnloser, 1998)．

整流機能 $rectifier(x) = \max(0, x)$ は片側性であるため，符号の対称性1や逆対称性1を強制するものではない：代わりに，興奮性の入力パターンの逆に対する応答は0（応答なし）となる．しかし，パラメータを共有する2つの整流器ユニットを組み合わせることで，対称性や逆対称性を得ることができる．

**利点:** 整流器活性化関数は，ネットワークが疎な表現を容易に得ることを可能にする．例えば，重みを一様に初期化した後，隠れユニットの連続出力値の約50％が実数のゼロであり，この割合はスパース性を誘発する正則化によって容易に増加させることができる．生物学的に妥当であるだけでなく、スパース性は数学的にも利点があります（前節参照）。

---

![fig2](https://raw.githubusercontent.com/rurusasu/paper/master/AI%E6%8A%80%E8%A1%93/%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF/Deep%20Sparse%20Rectifier%20Neural%20Networks/%E7%94%BB%E5%83%8F/fig2.png)

図2: 左: 整流器ユニットのネットワークにおける活性化と勾配のスパースな伝搬。入力はアクティブなニューロンのサブセットを選択し、計算はこのサブセットで線形に行われる。右: 整流器とソフトプラスの活性化関数．2つ目の活性化関数は1つ目の活性化関数を滑らかにしたもの．

---

図2（左）に示すように、ネットワークの唯一の非線形性は、個々のニューロンがアクティブかどうかに関連する経路選択に由来する。ある入力に対して、あるニューロンのサブセットだけがアクティブになります。このサブセットのニューロンが選択されると、出力は入力の一次関数となる（ただし、十分に大きな変化があると、アクティブなニューロンのセットが離散的に変化することがある）。各ニューロンによって計算された関数、またはネットワークの入力に対してネットワークの出力によって計算された関数は、部分的に線形となります。このモデルは、パラメータを共有する指数関数的な数の線形モデルとして見ることができます(Nair and Hinton, 2010)。この線形性のおかげで、ニューロンの活性化経路に勾配がよく流れ（シグモイドやタン単位の活性化非線形性による勾配消失効果がない）、数学的検討が容易になります。また、活性化の指数関数を計算する必要がなく、スパース性を利用することができるため、計算量も少なくて済みます。

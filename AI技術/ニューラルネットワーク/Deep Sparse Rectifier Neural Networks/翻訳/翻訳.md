# Deep Sparse Rectifier Neural Networks

# 備考
## 著者
Xavier Glorot, Antoine Bordes, Yoshua Bengio

## 掲載
Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTAT), pp. 315--323, 2011.

# Abst

ロジスティック・シグモイド・ニューロンはハイパーボリック・タンジェント・ニューロンよりも生物学的に妥当であるが、後者は多層ニューラル・ネットワークの学習に適している。本論文では、整流ニューロンが生物学的ニューロンのさらに優れたモデルであることを示し、ゼロでの硬い非線形性と非微分性にもかかわらず、双曲接線ネットワークと同等以上の性能が得られ、真のゼロを持つ疎な表現が作成され、自然に疎なデータに驚くほど適していると思われる。深層整流器ネットワークは、ラベルのないデータを使った半教師付きのセットアップを利用できるにもかかわらず、ラベル付きの大規模データセットを使った純粋な教師付きタスクでは、教師なしの事前学習を必要とせずに最高の性能を発揮します。今回の結果は、純粋に教師ありきのニューラルネットワークを学習することの難しさを理解し、教師なしの事前学習を行った場合と行わなかった場合のニューラルネットワークの性能差を縮めるための新たなマイルストーンになると考えられます。


# 2. Background

## 2.1. 脳科学の観察

生物学的なニューロンのモデルでは、活性化関数は、シナプスに入力される信号から現在生じている全入力の関数として期待される発火率である(Dayan and Abott, 2001)。活性化関数は、強い興奮性の入力パターンの反対に対する応答が、それぞれ強い抑制性または興奮性のものである場合には、それぞれ反対称または対称と呼ばれ、この応答がゼロである場合には片側と呼ばれる。計算論的神経科学モデルと機械学習モデルとの間に考慮したい主なギャップは以下の通りです。

- 脳のエネルギー消費に関する研究によると、ニューロンはまばらに分散して情報をエンコードしており（Attwell and Laughlin, 2001）、同時に活動しているニューロンの割合は1〜4％と推定されている（Lennie, 2003）。これは、表現の豊かさと小さな活動電位のエネルギー消費との間のトレードオフに相当する。L1ペナルティのような正則化を追加しないと、通常のフィードフォワードニューラルネットはこの特性を持ちません。例えば、シグモイド活性化は$\frac{1}{2}$付近に定常状態の領域があるため、小さな重みで初期化すると、すべてのニューロンが飽和領域の半分で発火します。これは生物学的にはありえないことで、勾配ベースの最適化に支障をきたします(LeCun et al., 1998; Bengio and Glorot, 2010)。

- **生物学的モデルと機械学習モデルの間の重要な相違点は、非線形活性化関数にあります。** 一般的なニューロンの生物学的モデルである leaky integrate-and-fire (LIF) (Dayan and Abott, 2001)では、図1（左）に示すように、発火率と入力電流の間に以下の関係があります。


---

![fig1]()

---
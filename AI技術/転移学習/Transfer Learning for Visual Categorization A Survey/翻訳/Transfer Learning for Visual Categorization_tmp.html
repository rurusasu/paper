<!DOCTYPE html>
<html>
<head>
<title>Transfer Learning for Visual Categorization.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css"
    integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js"
    integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js"
    integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe" crossorigin="anonymous"></script>
  <script>
    document.addEventListener("DOMContentLoaded", () => {
      renderMathInElement(document.body, {
        delimiters: [
          { left: "$$", right: "$$", display: true },
          { left: "$", right: "$", display: false },
        ]
      });
    });
  </script>
<h1 id="transfer-learning-for-visual-categorization-a-survey">Transfer Learning for Visual Categorization: A Survey</h1>
<h1 id="%E5%82%99%E8%80%83">備考</h1>
<h1 id="abstract">Abstract</h1>
<p>通常の機械学習やデータマイニング技術では，将来のデータが同じ特徴空間内にあるか，あるいは訓練データと同じ分布を持っているという大前提の下で，将来の推論のための訓練データを学習する． しかし、人間がラベルを付けた訓練データの利用可能性は限られているため、将来のデータが同じ特徴空間内に留まる、あるいは将来のデータと同じ分布を持つ訓練データは、オーバーフィットの問題を回避するのに十分な量を保証することはできない。 実際のアプリケーションでは、ターゲット領域のデータとは別に、異なる領域の関連データを含めることで、ターゲットとなる未来のデータに関する事前知識の利用可能性を拡大することができる。 転移学習は、このような領域横断的な学習問題を解決するために、関連する領域のデータから有用な情報を抽出し、それを転移して対象タスクで利用することを目的としています。 近年では，視覚分類への応用が進んでおり，行動認識タスクでのビュー発散や画像分類タスクでの概念ドリフトなどの典型的な問題を効率的に解決することができるようになってきている．本論文では、物体認識、画像分類、人間の行動認識などの視覚的な分類に適用されている最新の転移学習アルゴリズムを調査する。</p>
<h1 id="introduction">Introduction</h1>
<p>過去数年の間に、コンピュータビジョンのコミュニティでは、ビデオ検索、監視、ロボット工学などの分野でかなりの量の応用が見られるようになってきました。通常の機械学習アプローチ[1]-[7]は、学習データとテストデータが同じ特徴空間内にあるか、同じ分布を共有しているという大前提の下で、有望な結果を得てきた。しかし、実社会での応用では、人間の手作業によるラベリングの高価さや環境の制約から、テストデータと同じ特徴空間や同じ分布に属する十分な学習データが必ずしも得られない場合がある。代表的な例としては、訓練のために各アクションクラスに対して1つのアクションテンプレートのみが提供されている[8]-[11]や、訓練サンプルが異なる視点からキャプチャされている[12]などがある。このような状況では、通常の機械学習技術は失敗する可能性が高い。 このことは、人間の視覚システムの能力を思い起こさせる。物体の巨大な幾何学的変動やクラス内変動を考えると、人間は一生のうちに何万もの視覚カテゴリを学習することができ、これは、人間が情報と知識の蓄積によってこのような能力を達成しているという仮説につながる[13]。世界には約1～3万個の物体クラスがあると推定されており[14]、子供は1日に4～5個の物体クラスを学ぶことができるとされている[13]。 子供が一日のうちに見ることができるオブジェクトの数には限りがあるため、対応する大量のオブジェクトデータから新しいオブジェクトクラスを学習することはできない。 したがって、以前に知られていたオブジェクトから得られた既存の知識が、新しいオブジェクトクラスとの関連性を介して新しい学習プロセスを支援すると考えられている。 例えば、スイカが何であるかを知らなかったとすると、新しい物体カテゴリであるスイカを記憶するためには、スイカの丸い形や緑の色などの以前の知識と一緒に、スイカの学習サンプルを1つだけ用意すればよい。転写学習は、与えられた領域で新たなタスクを実行する際に、他の関連領域の十分な量の予備知識を利用することで、人間の視覚システムを模倣したものである。転移学習では、学習データとテストデータの両方が2種類の領域に貢献することができます。</p>
<ol>
<li>目標ドメイン</li>
<li>ソースドメイン</li>
</ol>
<p>対象領域には、カテゴリ化システムのタスクであるテストインスタンスが含まれ、ソース領域には、対象領域データとは異なる分布下にあるトレーニングインスタンスが含まれます。多くの場合、転移学習タスクのターゲットドメインは1つだけであるが、ソースドメインは単一でも複数でもよい。 例えば、[15]では、異なるドメインのデータセットをまたいで行動認識を行い、背景がきれいで視点や規模の変化が限定的なKTHデータセット[16]をソースデータセットとし、現実的なシナリオからキャプチャしたMicrosoftの研究行動データセットとTRECVIDの監視データ[17]をターゲットデータセットとしている。[18]では，映像概念検出の課題に対して，異なるテレビ番組のチャンネルからソースデータセットとターゲットデータセットを選択している．</p>
<p>転移学習は、使用される学習データの一部/全部がテストデータとは異なる分布下にある特殊な学習パラダイムと考えることができる。視覚学習問題の観点から知識移転の意義を理解するために、文献（[19]-[21]を参照）では、移転プロセスに関する3つの一般的な問題を結論づけている。</p>
<ol>
<li>いつ移転するか</li>
<li>何を移転するか</li>
<li>どのように移転するか</li>
</ol>
<p>まず、「いつ移転するか」には、特定の学習課題に対して移転学習が必要かどうか、移転元のドメインデータと移転先のドメインデータが関連しているかどうかという問題が含まれている。 [22]-[24]のシナリオでは、学習サンプルが十分であり、目標領域の制約を受けながらも、他の領域を含めて原領域が余計なものになってしまうような場合に、優れた性能を発揮することが可能である。 ソースドメインとターゲットドメインのデータの異なるペア間には様々な発散レベルが存在し、発散に関係なくソースドメインからターゲットドメインに知識を強要すると、ある種の性能低下を引き起こしたり、さらに悪い場合にはターゲットドメインでの元のデータの整合性を壊したりすることになります。第二に、何を移転するかという答えは、3つの側面から結論づけられる。</p>
<ol>
<li>帰納的移転学習、すなわち、すべてのソースドメインのインスタンスとそれに対応するラベルを知識移転のために使用</li>
<li>インスタンス移転学習、すなわち、ソースドメインのインスタンスのみを使用</li>
<li>パラメータ移転学習、すなわち、ソースドメインのインスタンスとラベルに加えて、ソースドメインから事前に学習されたモデルのいくつかのパラメータを利用して、ターゲットドメインでの性能向上を支援</li>
</ol>
<p>最後に、どのように移行するかは、具体的な移行学習手法のすべてを含み、移行学習文献の中で最も研究されてきた部分でもある。例えば、[25]-[27]では、知識の移転を非負行列トリファクタリゼーションのフレームワークに基づいて行っており、[28]では、次元削減を介した移転学習を行っている。従来の機械学習アプローチと知識移転アプローチの基本的なフレームワークを図 1 に示す。従来の機械学習アプローチでは、テストインスタンスの車を予測するための訓練セットの理想的な選択は、車を含むことである。 しかし、知識転移の場合には、学習セットは車ではなく、車の車輪に似ている車輪、車の車輪と車輪の知識を共有する自転車、あるいは、車とは無関係に見えるが、実際には車の画像の局所的な部分と特定のエッジや幾何学的なレイアウトを共有するラップトップや鳥など、いくつかの関連するカテゴリを含むことができる。</p>
<p>ビッグデータの時代が到来した今、転移学習は、より関連性の高いデータを用いて目的の問題を解決するために、より多くのメリットを提供することができる。 このように，今後の研究では，より多くの応用が期待されている． 本調査では、視覚的な分類課題に対する転移学習の手法を包括的に概観することで、読者の皆様が、視覚的な分類課題に転移学習がどのように適用できるのか、あるいは適切な転移学習手法を用いて課題を解決するために、本調査での分析や議論を活用できる可能性があることを示したいと思います。視覚的分類タスクは、学習プロセスで使用できる視覚的特性、例えば、物体の部分の外観や形状、物体の局所的な対称性、構造などに起因するいくつかのユニークな特性を持っている。これらのユニークな特性はすべて転移学習アルゴリズムを設計する際に採用することができ、我々の研究は前者がデータマイニングタスクに関連した分類、回帰、クラスタリング問題に焦点を当て、後者が強化学習に焦点を当て、正しくラベル付けされた例ではなく、限られた環境フィードバックのみで問題を解決する[19]や[29]の研究とは異なるものとなっている。</p>
<p>本調査の残りの部分は以下のように構成されている。第II節では概要を説明する。 第III節と第IV節では、特徴表現と分類器を用いて知識の伝達を実行する2つの伝達学習カテゴリについて詳細に議論し、「何をどのように伝達するか」「どのように伝達するか」という問題を解決する。第V節では、複数のソースドメインからのモデル選択方法、すなわち、いつ移転するかについて議論する。第VI節では、述べられた移転学習法の評価、分析、考察を行う。最後に、第VII節で結論を述べる。</p>
<h1 id="2-overview">2. OverView</h1>
<h2 id="a-developing-interests-on-transfer-learning">A. Developing Interests on Transfer Learning</h2>
<p>転移学習（クロスドメイン学習、ドメイントランスファー、ドメインアダプテーション）は、前世紀にその概念が提唱されて以来、特定の機械学習手法として研究されてきた歴史があります。近年、インターネット上での情報（音声、画像、動画など）の爆発的な普及に伴い、精度、データ規模、計算効率の面で対象タスクへの要求が高まってきており、パターン認識や機械学習のあらゆる研究分野から転移学習アプローチへの関心が高まってきている。 通常の機械学習技術が限界に達したとき、転移学習は新しい流れを開き、これまでの学習方法や分類や回帰タスクの扱い方を根本的に変えてしまう可能性があります。 この流れに沿って、いくつかのワークショップやチュートリアルが開催されてきた（例えば、機械学習やデータマイニングの分野では、NIPS 1995 postconference workshopが開催され、強化学習の分野では、[29]で移転学習の調査が行われている）。 本調査では、行動認識、物体認識、画像分類を含む視覚分類への転移学習技術の応用に焦点を当てている。</p>
<p>$D^T=D^T_l \cup D^T_u$を対象領域データとし、部分的にラベル付けされている部分を $D^T_l$、ラベル付けされていない部分を $D^T_u$ とする。 対象領域データの他に、半ラベル化または完全ラベル化された補助データの集合をソース領域データと見なし、単一ソースの場合は $D^s = {(x_i,y_i)}^a_{i=1}$ 、複数ソースの場合は $D^s_k = {(x^k_i, y^k_i)}^{N^a_k}_{i=1}$ の表現 $D^s_1, D^s_2, ..., D^s_M$ とする。 ここで、$x_i \in \mathbb{R}^d$ は、$i$ 番目の特徴ベクトルであり、$d$ はデータ次元、$y_i$ は $i$ 番目のサンプルのクラスラベルを表す。</p>
<p>先行提案によれば、知識伝達に関する共通の問題点は2つある。 第一に、補助サンプルは一般的に適応時に相互依存性を考慮せずに扱われるため、適応されたデータが任意に分散し、補助データの単一データサンプルを超えた構造情報が損なわれる可能性がある。 第二に、適応の間、ノイズ、特に補助領域からの可能性のある外れ値は、盲目的にターゲット領域に強制される[30]。</p>
<p>補助領域から対象領域へ知識を伝達する際には、対象領域データと各ソース領域データとの間の分布類似度を知ることが重要である。 これまでのところ、2つのドメインの分布類似度を測定するための最も一般的な基準は、最大平均不一致（MMD）と呼ばれるノンパラメトリック距離測定法である。MMDは[31]で提案されており、再生カーネルヒルベルト空間におけるデータ分布を比較するものである。</p>

</body>
</html>

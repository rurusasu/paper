# How transferable are features in deep neural networks?

# 備考

## 著者

Jason Yosinski, Jeff Clune, Yoshua Bengio, Hod Lipson

## 掲載

``How transferable are features in deep neural networks?.'' Procs. Advances in neural information processing systems 27 (NIPS 2014), pp. 3320–3328, 2014.

# Abstract

自然画像上で訓練された多くのディープニューラルネットワークには、共通して不思議な現象が見られます：第1層では、ガボールフィルタやカラーブロブに似た特徴を学習します。 このような第1層の特徴は、特定のデータセットやタスクに特化したものではなく、多くのデータセットやタスクに適用できるという点で一般的なもののように見えます。このような特徴は、最終的にはネットワークの最終層で一般的なものから特定のものへと移行しなければならないが、この移行についてはこれまで広く研究されていなかった。この論文では、深層畳み込みニューラルネットワークの各層のニューロンの一般性と特異性を実験的に定量化し、いくつかの驚くべき結果を報告する。伝達可能性は、2つの異なる問題によってネガティブな影響を受ける。 (1)目標タスクの性能を犠牲にして、高次の層のニューロンが元のタスクに特化してしまうこと、(2)共同適応したニューロン間のネットワーク分割に関連した最適化の難しさ、である。本研究では、ImageNet上で学習したネットワークの例を用いて、ネットワークの下部、中部、上部のいずれから特徴を伝達するかによって、これら2つの問題のいずれかが支配的になることを実証した。 また、ベースタスクとターゲットタスクの距離が長くなるほど特徴の伝達性は低下するが、遠くのタスクからでもランダムな特徴を使うよりは、特徴を伝達する方が優れていることを示した。 最後の驚くべき結果は、ほとんど何層からでも転送された特徴を用いてネットワークを初期化することで、ターゲットデータセットを微調整した後でも、一般化を促進することができるということである。